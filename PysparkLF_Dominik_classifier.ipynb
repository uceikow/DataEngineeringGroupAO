{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some pictures and our coursework title here! Anyone has some ideas plz update here :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "import pyspark\n",
    "\n",
    "number_cores = int(os.environ['NUM_CPUS'])\n",
    "memory_gb = int(os.environ['AVAILABLE_MEMORY_MB']) // 1024\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setMaster('local[{}]'.format(number_cores))\n",
    "        .set('spark.driver.memory', '{}g'.format(memory_gb))\n",
    ")\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[4] appName=pyspark-shell>\n"
     ]
    }
   ],
   "source": [
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "indian = sqlContext.read.csv(\"/project/Project/DataEngineeringGroupAO/Recipe_dataset/data_indian.csv\", header = True)\n",
    "italian = sqlContext.read.csv(\"/project/Project/DataEngineeringGroupAO/Recipe_dataset/data_italian.csv\", header = True)\n",
    "mexican = sqlContext.read.csv(\"/project/Project/DataEngineeringGroupAO/Recipe_dataset/data_mexican.csv\", header = True)\n",
    "new_recipe = sqlContext.read.csv(\"/project/Project/DataEngineeringGroupAO/Recipe_dataset/final_scrape_not_cleaned.csv\", header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indian:  480  italian:  400  mexican:  620  new_recipe:  2340\n"
     ]
    }
   ],
   "source": [
    "# See the size of the dataset\n",
    "print('indian: ', indian.count(),' italian: ', italian.count(), ' mexican: ', mexican.count(),\n",
    "     ' new_recipe: ', new_recipe.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first three datasets, we do not have labels for them. We could use them as training set but three of them combined together is not enough. In this case, we decided to combine them together first, and then do splitting, and then remove the labels of training set. So, we have to first label the unlabelled dataset manually, which is shown as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label the data\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "indian = indian.withColumn(\"label\",lit(\"indian\"))\n",
    "italian = italian.withColumn(\"label\",lit(\"italian\"))\n",
    "mexican = mexican.withColumn(\"label\",lit(\"mexican\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+\n",
      "|               Title|         Description| label|\n",
      "+--------------------+--------------------+------+\n",
      "|  Indian Peanut Stew|This is an easy, ...|indian|\n",
      "|        Roomali Roti|There is no leave...|indian|\n",
      "|Spicy Sweet Potat...|It's important to...|indian|\n",
      "|        Chicken Saag|The classic India...|indian|\n",
      "|Paleo Slow Cooker...|Boneless pork loi...|indian|\n",
      "|Bombay Chicken an...|Chicken parts are...|indian|\n",
      "|Indian Carrots, P...|Potatoes, peas an...|indian|\n",
      "|Wendy's Indian Bu...|This recipe resem...|indian|\n",
      "|    Indian Chickpeas|Garbanzo beans, o...|indian|\n",
      "|Dal Makhani (Indi...|These richly spic...|indian|\n",
      "|               Raita|Chopped tomatoes ...|indian|\n",
      "|Yogurt-Marinated ...|A yogurt-based ma...|indian|\n",
      "|Indian-Spiced Roa...|Spicy roasted chi...|indian|\n",
      "|Cauliflower and T...|Pressed tofu cube...|indian|\n",
      "|Channa Masala (Ch...|This fantastic In...|indian|\n",
      "|Bengali Chicken C...|Thy this deliciou...|indian|\n",
      "|  Indian Sweet Bread|A crisp and sweet...|indian|\n",
      "| Rosy's Palak Paneer|An Indian friend ...|indian|\n",
      "|Roti Bread from I...|This version of t...|indian|\n",
      "|Indian Vegetable ...|Basmati rice enha...|indian|\n",
      "+--------------------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combine all datasets into one\n",
    "\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def unionAll(dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "dfs = [indian, italian, mexican, new_recipe]\n",
    "recipe = unionAll(dfs)\n",
    "recipe.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As these two scraping are from the same website, but using different method, there might be some duplicates in the combined datasets. We try to remove the duplicates before data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3840\n"
     ]
    }
   ],
   "source": [
    "# Size before removing duplicates\n",
    "print(recipe.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "recipe = recipe.dropDuplicates([\"Title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3793\n"
     ]
    }
   ],
   "source": [
    "# Size after dropping duplicates\n",
    "print(recipe.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we come to the data cleaning part! Our dataset is all about **text**, and it needs to be transformed a bit for better analysis.\n",
    "\n",
    "- Lowercase\n",
    "- Remove punctuations and digits. \n",
    "\n",
    "In NLTK, we have packages which helps remove punctuations and digits, but in our case, we try to use **regular expression** to remove unwanted signs in our dataset.\n",
    "- Remove stopwords\n",
    "- Stemming the text \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all packages needed for data cleaning\n",
    "\n",
    "from pyspark.sql.functions import udf, regexp_replace, lower, col\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from pyspark.sql.types import IntegerType, ArrayType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|               Title|         Description|  label|\n",
      "+--------------------+--------------------+-------+\n",
      "|amanda & graham's...|restaurant in our...|italian|\n",
      "|avocado and mango...|a wonderfully fru...|mexican|\n",
      "|batman's best cae...|fast, easy, cream...|italian|\n",
      "|beef chilli with ...|this is a hearty ...|mexican|\n",
      "|chicken enchilada...|chicken, sour cre...|mexican|\n",
      "|chorizo breakfast...|a yummy breakfast...|mexican|\n",
      "|coconut-coriander...|simple preparatio...| indian|\n",
      "|coriander chicken...|this chicken dish...|mexican|\n",
      "|           curd rice|curd rice, or yog...| indian|\n",
      "|daz's tomato chee...|this is my favour...|italian|\n",
      "|easy chicken tikk...|a quick and easy ...| indian|\n",
      "|four seasons ench...|nice and spicy us...|mexican|\n",
      "|homemade four che...|fresh pasta fille...|italian|\n",
      "|homemade paneer c...|paneer is a white...| indian|\n",
      "|      indian khichri|khichri is a very...| indian|\n",
      "|               kheer|this is a popular...| indian|\n",
      "| madras potato curry|this dish makes a...| indian|\n",
      "|meatball sub cass...|start with frozen...|italian|\n",
      "|quick and easy pi...|a quick, chewy pi...|italian|\n",
      "| refried black beans|these low fat ref...|mexican|\n",
      "+--------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lowercase\n",
    "\n",
    "recipe = recipe.select(*[lower(col(col_name)).name(col_name) for col_name in recipe.columns])\n",
    "recipe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|               title|                 des|  label|\n",
      "+--------------------+--------------------+-------+\n",
      "|amanda  grahams m...|restaurant in our...|italian|\n",
      "|avocado and mango...|a wonderfully fru...|mexican|\n",
      "|batmans best caes...|fast easy creamy ...|italian|\n",
      "|beef chilli with ...|this is a hearty ...|mexican|\n",
      "|chicken enchilada...|chicken sour crea...|mexican|\n",
      "|chorizo breakfast...|a yummy breakfast...|mexican|\n",
      "|coconutcoriander ...|simple preparatio...| indian|\n",
      "|coriander chicken...|this chicken dish...|mexican|\n",
      "|           curd rice|curd rice or yogh...| indian|\n",
      "|dazs tomato chees...|this is my favour...|italian|\n",
      "|easy chicken tikk...|a quick and easy ...| indian|\n",
      "|four seasons ench...|nice and spicy us...|mexican|\n",
      "|homemade four che...|fresh pasta fille...|italian|\n",
      "|homemade paneer c...|paneer is a white...| indian|\n",
      "|      indian khichri|khichri is a very...| indian|\n",
      "|               kheer|this is a popular...| indian|\n",
      "| madras potato curry|this dish makes a...| indian|\n",
      "|meatball sub cass...|start with frozen...|italian|\n",
      "|quick and easy pi...|a quick chewy piz...|italian|\n",
      "| refried black beans|these low fat ref...|mexican|\n",
      "+--------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove punctuation and digits\n",
    "\n",
    "recipe_clean = recipe.select(regexp_replace('Title', \"[^a-zA-Z\\\\s]\", \"\").alias('title'), \n",
    "    (regexp_replace('Description', \"[^a-zA-Z\\\\s]\", \"\").alias('des')),'label')\n",
    "recipe_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|               title|           des_clean|  label|\n",
      "+--------------------+--------------------+-------+\n",
      "|amanda  grahams m...|[restaurant, livi...|italian|\n",
      "|avocado and mango...|[wonderfully, fru...|mexican|\n",
      "|batmans best caes...|[fast, easy, crea...|italian|\n",
      "|beef chilli with ...|[hearty, deliciou...|mexican|\n",
      "|chicken enchilada...|[chicken, sour, c...|mexican|\n",
      "|chorizo breakfast...|[yummy, breakfast...|mexican|\n",
      "|coconutcoriander ...|[simple, preparat...| indian|\n",
      "|coriander chicken...|[chicken, dish, d...|mexican|\n",
      "|           curd rice|[curd, rice, yogh...| indian|\n",
      "|dazs tomato chees...|[favourite, start...|italian|\n",
      "|easy chicken tikk...|[quick, easy, cur...| indian|\n",
      "|four seasons ench...|[nice, spicy, use...|mexican|\n",
      "|homemade four che...|[fresh, pasta, fi...|italian|\n",
      "|homemade paneer c...|[paneer, white, c...| indian|\n",
      "|      indian khichri|[khichri, popular...| indian|\n",
      "|               kheer|[popular, pakista...| indian|\n",
      "| madras potato curry|[dish, makes, col...| indian|\n",
      "|meatball sub cass...|[start, frozen, m...|italian|\n",
      "|quick and easy pi...|[quick, chewy, pi...|italian|\n",
      "| refried black beans|[low, fat, refrie...|mexican|\n",
      "+--------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove Stopwords\n",
    "\n",
    "# Tokenize text\n",
    "tokenizer = Tokenizer(inputCol = \"des\", outputCol = \"des_token\")\n",
    "recipe = tokenizer.transform(recipe_clean).select('title','des','des_token','label')\n",
    "# tokenized.select(\"Description\", \"Des_words\")\\\n",
    "    #.withColumn(\"tokens\", countTokens(col(\"Des_words\"))).show(truncate=False)\n",
    "\n",
    "# Remove stopwords\n",
    "remover = StopWordsRemover(inputCol = 'des_token', outputCol = 'des_clean')\n",
    "recipe_no_stopw = remover.transform(recipe).select('title','des_clean', 'label')\n",
    "recipe_no_stopw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|               title|         des_stemmed|  label|\n",
      "+--------------------+--------------------+-------+\n",
      "|amanda  grahams m...|[restaur, live, r...|italian|\n",
      "|avocado and mango...|[wonder, fruiti, ...|mexican|\n",
      "|batmans best caes...|[fast, easi, crea...|italian|\n",
      "|beef chilli with ...|[hearti, delici, ...|mexican|\n",
      "|chicken enchilada...|[chicken, sour, c...|mexican|\n",
      "|chorizo breakfast...|[yummi, breakfast...|mexican|\n",
      "|coconutcoriander ...|[simpl, prepar, i...| indian|\n",
      "|coriander chicken...|[chicken, dish, d...|mexican|\n",
      "|           curd rice|[curd, rice, yogh...| indian|\n",
      "|dazs tomato chees...|[favourit, starte...|italian|\n",
      "|easy chicken tikk...|[quick, easi, cur...| indian|\n",
      "|four seasons ench...|[nice, spici, use...|mexican|\n",
      "|homemade four che...|[fresh, pasta, fi...|italian|\n",
      "|homemade paneer c...|[paneer, white, c...| indian|\n",
      "|      indian khichri|[khichri, popular...| indian|\n",
      "|               kheer|[popular, pakista...| indian|\n",
      "| madras potato curry|[dish, make, colo...| indian|\n",
      "|meatball sub cass...|[start, frozen, m...|italian|\n",
      "|quick and easy pi...|[quick, chewi, pi...|italian|\n",
      "| refried black beans|[low, fat, refri,...|mexican|\n",
      "+--------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stem text\n",
    "stemmer = SnowballStemmer(language = 'english')\n",
    "stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
    "recipe = recipe_no_stopw.withColumn(\"des_stemmed\", stemmer_udf(\"des_clean\")).select('title', 'des_stemmed','label')\n",
    "recipe.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splitting & Pattern Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting\n",
    "\n",
    "maxim's wording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out different recipes\n",
    "# Create temp table\n",
    "recipe.createOrReplaceTempView('recipes')\n",
    "\n",
    "recipe_ind = sqlContext.sql(\"SELECT * FROM recipes WHERE label == 'indian'\")\n",
    "recipe_ita = sqlContext.sql(\"SELECT * FROM recipes WHERE label == 'italian'\")\n",
    "recipe_mex = sqlContext.sql(\"SELECT * FROM recipes WHERE label == 'mexican'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We asssume that we do not know labels for the majority of data points, \n",
    "# hence further explore only test split\n",
    "recipe_ind_tr, recipe_ind_ts, recipe_ind_d,recipe_ind_v = recipe_ind.randomSplit([0.6,0.2,0.1,0.1],seed = 11)\n",
    "recipe_ita_tr, recipe_ita_ts, recipe_ita_d, recipe_ita_v = recipe_ita.randomSplit([0.6,0.2,0.1,0.1],seed = 11)\n",
    "recipe_mex_tr, recipe_mex_ts, recipe_mex_d, recipe_mex_v = recipe_mex.randomSplit([0.6,0.2,0.1,0.1],seed = 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern Exploration\n",
    "\n",
    "Before writing the label functions, we want to get our head around how our dataset look like and see if we can get some ideas from it for writing label functions.\n",
    "\n",
    "Since we want to label the type of the recipe. We made a word frequency dataframe, which displays top 15 words in each type of the recipe to help us identify the keywords connected to each cuise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create frequency list of description\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "top_n = 20\n",
    "\n",
    "ind_counts = recipe_ind_d.select(f.explode('des_stemmed').alias('col')).groupBy('col').count()\n",
    "ind_des_freq = ind_counts.orderBy(ind_counts[\"count\"].desc()).limit(top_n)\n",
    "\n",
    "ita_counts = recipe_ita_d.select(f.explode('des_stemmed').alias('col')).groupBy('col').count()\n",
    "ita_des_freq = ita_counts.orderBy(ita_counts[\"count\"].desc()).limit(top_n)\n",
    "\n",
    "mex_counts = recipe_mex_d.select(f.explode('des_stemmed').alias('col')).groupBy('col').count()\n",
    "mex_des_freq = mex_counts.orderBy(mex_counts[\"count\"].desc()).limit(top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-------+-----+--------+-----+\n",
      "|    col|count|    col|count|     col|count|\n",
      "+-------+-----+-------+-----+--------+-----+\n",
      "|  curri|   56|italian|   49| chicken|   40|\n",
      "| indian|   46|  pasta|   41|    serv|   27|\n",
      "|   rice|   39|   make|   35|    bean|   26|\n",
      "|   serv|   35|  recip|   33|tortilla|   25|\n",
      "|chicken|   31| tomato|   32| mexican|   23|\n",
      "|  recip|   31|  chees|   32|  tomato|   21|\n",
      "|   make|   30|   sauc|   30|   fresh|   21|\n",
      "|   dish|   27|  fresh|   29|   salsa|   20|\n",
      "|   easi|   24|   dish|   27|    dish|   18|\n",
      "|flavour|   23|   serv|   25|    make|   18|\n",
      "|  spice|   20|    use|   24|   chees|   18|\n",
      "|   cook|   19|   easi|   22|   recip|   18|\n",
      "|  spici|   18| delici|   22|  chilli|   17|\n",
      "| delici|   18|       |   22|   green|   17|\n",
      "|   made|   18|  bread|   20|    cook|   16|\n",
      "|    use|   17|   made|   19|    corn|   16|\n",
      "|   love|   15|   love|   17|   great|   15|\n",
      "|   meal|   15| garlic|   17|   spici|   15|\n",
      "| potato|   15|chicken|   16|     use|   15|\n",
      "|  cumin|   14|   tast|   15|    rice|   15|\n",
      "+-------+-----+-------+-----+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View then in one dataframe\n",
    "from pyspark.sql.functions import monotonically_increasing_id \n",
    "\n",
    "df1 = ind_des_freq.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "df2 = ita_des_freq.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "df3 = mex_des_freq.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "\n",
    "des_freq = df1.join(df2,(\"row_id\")).join(df3,(\"row_id\")).drop(\"row_id\")\n",
    "des_freq.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maxim continue with your wording for data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "# train split full / remove lists\n",
    "df_tr = recipe_ind_tr.union(recipe_ita_tr)\n",
    "df_tr = df_tr.union(recipe_mex_tr).withColumn(\"des_stemmed\", concat_ws(\" \", \"des_stemmed\"))\n",
    "\n",
    "# test split \n",
    "df_ts = recipe_ind_ts.union(recipe_ita_ts)\n",
    "df_ts = df_ts.union(recipe_mex_ts).withColumn(\"des_stemmed\", concat_ws(\" \", \"des_stemmed\"))\n",
    "\n",
    "# dev split - labelled\n",
    "df_d = recipe_ind_d.union(recipe_ita_d)\n",
    "df_d = df_d.union(recipe_mex_d).withColumn(\"des_stemmed\", concat_ws(\" \", \"des_stemmed\"))\n",
    "\n",
    "# val split - labelled\n",
    "df_v = recipe_ind_d.union(recipe_ita_v)\n",
    "df_v = df_v.union(recipe_mex_v).withColumn(\"des_stemmed\", concat_ws(\" \", \"des_stemmed\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop train split for labelling\n",
    "# df_tr.drop('label').first()\n",
    "df_tr = df_tr.drop('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'indian':0, 'italian':1, 'mexican':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+\n",
      "|               title|         des_stemmed|num_label|\n",
      "+--------------------+--------------------+---------+\n",
      "|homemade paneer c...|paneer white chee...|        0|\n",
      "|slow cooker balti...|noth better someo...|        0|\n",
      "|fresh coconut chu...|chutney tradit se...|        0|\n",
      "|kashmiri chicken ...|sweet banana exot...|        0|\n",
      "|chicken and cauli...|mild creami curri...|        0|\n",
      "|creamy chicken korma|coconut almond gi...|        0|\n",
      "|gujarati carrot a...|peanut add wonder...|        0|\n",
      "|  kheer rice pudding|flavour indiansty...|        0|\n",
      "|sherry chicken curry|outsid restaur on...|        0|\n",
      "|curried beef with...|medley winter veg...|        0|\n",
      "|authentic chicken...|tender chicken co...|        0|\n",
      "|chicken masala curry|fragrant homemad ...|        0|\n",
      "|          okra curry|slice okra round ...|        0|\n",
      "|      gujarati kadhi|tri indian yogurt...|        0|\n",
      "|    masala spicy tea|masala tea also k...|        0|\n",
      "|vegetarian palak ...|indian dal classi...|        0|\n",
      "|cucumber peanut s...|fresh delici cucu...|        0|\n",
      "|        khara pongal|classic south ind...|        0|\n",
      "|bombay chicken wings|chicken wing ligh...|        0|\n",
      "|black pepper goat...|black peppercorn ...|        0|\n",
      "+--------------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import create_map\n",
    "from itertools import chain\n",
    "\n",
    "mapping_func = create_map([lit(x) for x in chain(*mapping.items())])\n",
    "\n",
    "df_v = df_v.withColumn(\"num_label\", mapping_func.getItem(col(\"label\")))\n",
    "# df_dv = df_dv.select('title','des_clean', 'num_label')\n",
    "df_v = df_v.select('title','des_stemmed', 'num_label')\n",
    "df_v.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_label for test split for accuracy - INCLUDE LABEL TO GET REAL GOLD:)\n",
    "df_ts = df_ts.withColumn(\"num_label\", mapping_func.getItem(col(\"label\")))\n",
    "df_ts = df_ts.select('title','des_stemmed', 'num_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column with actual values \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "Y_v = df_v.select('num_label').rdd.flatMap(lambda x: x).collect()\n",
    "Y_v = np.array(Y_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = df_ts.select('num_label').rdd.flatMap(lambda x: x).collect()\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For clarity, we define constants to represent the class labels and abstaining.\n",
    "ABSTAIN = -1\n",
    "INDIAN = 0\n",
    "ITALIAN = 1\n",
    "MEXICAN = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labelling Functions Introduction\n",
    "\n",
    "Once the pattern exploration has been finished, it was possible to write Labelling Functions. Hereby, due to the fact that there were no structural differences (e.g. there were no visible patterns for one cuisine) between the names and description of any cuisine, the Labelling Functions mainly search for aforementioned keywords or their combinations (obtained through careful exploration of the scraped datasets) that may be relevent to just one cuisine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snorkel in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (0.9.3)\n",
      "Collecting pandas<0.26.0,>=0.25.0\n",
      "  Using cached pandas-0.25.3-cp36-cp36m-manylinux1_x86_64.whl (10.4 MB)\n",
      "Requirement already satisfied: munkres==1.1.2 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from snorkel) (1.1.2)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.2.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from snorkel) (1.4.1)\n",
      "Requirement already satisfied: torch<1.2.0,>=1.1.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from snorkel) (1.1.0)\n",
      "Collecting scikit-learn<0.22.0,>=0.20.2\n",
      "  Using cached scikit_learn-0.21.3-cp36-cp36m-manylinux1_x86_64.whl (6.7 MB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.16.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from snorkel) (1.18.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.33.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from snorkel) (4.43.0)\n",
      "Requirement already satisfied: networkx<2.4,>=2.2 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from snorkel) (2.3)\n",
      "Requirement already satisfied: tensorboardX<2.0,>=1.6 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from snorkel) (1.9)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from pandas<0.26.0,>=0.25.0->snorkel) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from pandas<0.26.0,>=0.25.0->snorkel) (2019.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from scikit-learn<0.22.0,>=0.20.2->snorkel) (0.14.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from networkx<2.4,>=2.2->snorkel) (4.4.1)\n",
      "Requirement already satisfied: six in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from tensorboardX<2.0,>=1.6->snorkel) (1.14.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from tensorboardX<2.0,>=1.6->snorkel) (3.11.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorboardX<2.0,>=1.6->snorkel) (45.2.0.post20200210)\n",
      "\u001b[31mERROR: faculty-models 0.1.1 has requirement mlflow-faculty>=0.4.3, but you'll have mlflow-faculty 0.4.2 which is incompatible.\u001b[0m\n",
      "Installing collected packages: pandas, scikit-learn\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.0.1\n",
      "    Uninstalling pandas-1.0.1:\n",
      "      Successfully uninstalled pandas-1.0.1\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.22.2.post1\n",
      "    Uninstalling scikit-learn-0.22.2.post1:\n",
      "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
      "Successfully installed pandas-0.25.3 scikit-learn-0.22.1\n"
     ]
    }
   ],
   "source": [
    "! pip install snorkel\n",
    "from snorkel.labeling.apply.spark import SparkLFApplier\n",
    "from snorkel.labeling import LFAnalysis\n",
    "from snorkel.labeling import labeling_function\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indian LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling function that checks whether identified indian keywords are in the title column of the RDD\n",
    "ind_keywords = ['curri','masala','paneer','chutney','simmered','cumin','yogurt',\n",
    "                'coconut','lentil','veget','chickpea','korma','dal','tandoori','tikka']\n",
    "\n",
    "@labeling_function()\n",
    "def indian_title_keywords(x):\n",
    "        if any(word in x.title for word in ind_keywords):\n",
    "            return INDIAN\n",
    "        else:\n",
    "            return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling function that checks whether identified indian keywords are in the description column of the RDD\n",
    "ind_keywords_des = ['naan','curri','vegetarian','tradit','veget']\n",
    "@labeling_function()\n",
    "def indian_des_keywords(x):\n",
    "        if any(word in x.des_stemmed for word in ind_keywords_des):\n",
    "            return INDIAN\n",
    "        else:\n",
    "            return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling function that checks whether combination of curry + different kinds of meat is in the description column of the RDD\n",
    "@labeling_function()\n",
    "def currymeat(x):\n",
    "    return INDIAN if re.search(r\"(?=.*curry)(?=.*(chicken|lamb|beef|prawn))\", x.des_stemmed, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling function that checks whether combination of quick or easy + different kinds of ingredients is in the description column of the RDD\n",
    "@labeling_function()\n",
    "def cook_food(x):\n",
    "    return INDIAN if re.search(r\"(?=.*(quick|easy))(?=.*(rice|sauc|potato|tomato|bread))\", x.des_stemmed, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling function that checks whether combination of sweet + spicy is in the description column of the RDD\n",
    "\n",
    "@labeling_function()\n",
    "def sweet_spicy(x):\n",
    "    return INDIAN if re.search(r\"(?=.*sweet)(?=.*(spici))\", x.des_stemmed, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling function that checks whether combination of slow + cook is in the description column of the RDD\n",
    "\n",
    "@labeling_function()\n",
    "def slow_cook(x):\n",
    "    return INDIAN if re.search(r\"(?=.*slow)(?=.*(cook))\", x.des_stemmed, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Italian LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling function that checks whether identified italian keywords from are in the title column of the RDD\n",
    "ita_keywords = ['pasta','spaghetti','risotto','sausag','mushroom','lasagn','lasagna','bolognes'\n",
    "                ,'pizza','parmesan','tiramisu','ricotta','bruschetta','bake']\n",
    "\n",
    "@labeling_function()\n",
    "def italian_title_keywords(x):\n",
    "        if any(word in x.title for word in ita_keywords):\n",
    "            return ITALIAN\n",
    "        else:\n",
    "            return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling function that checks whether identified indian keywords are in the description column of the RDD\n",
    "ita_keywords_des = ['garlic','classic','oliv','parmesan','pasta','rich','mozzarella','lasagna','pesto','dente']\n",
    "\n",
    "@labeling_function()\n",
    "def italian_des_keywords(x):\n",
    "        if any(word in x.des_stemmed for word in ita_keywords_des):\n",
    "            return ITALIAN\n",
    "        else:\n",
    "            return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling function that checks whether combination of pasta + different kinds of ingredients is in the description column of the RDD\n",
    "@labeling_function()\n",
    "def pasta_with(x):\n",
    "    return ITALIAN if re.search(r\"(?=.*pasta)(?=.*(chicken|lamb|beef|pesto|creami|cream|shrimp|chees))\", x.des_stemmed, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling function that checks whether combination of tomato +  sun-dried or sundried is in the description column of the RDD\n",
    "@labeling_function()\n",
    "def sundried_tomatoes(x):\n",
    "    return ITALIAN if re.search(r\"(?=.*tomato)(?=.*(sun-dri|sundri))\", x.des_stemmed, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling function that checks whether any of the identified regions is in the description column of the RDD\n",
    "ita_regions = ['tuscan','sicilian', 'romano', 'romaine', 'mediterranean','meditterranean' ]\n",
    "\n",
    "@labeling_function()\n",
    "def ita_regions(x):\n",
    "        if any(word in x.des_stemmed for word in ita_keywords):\n",
    "            return ITALIAN\n",
    "        else:\n",
    "            return ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mexican LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling function that checks whether identified mexican keywords from are in the title column of the RDD\n",
    "mex_keywords = ['enchilada','taco','bean','salsa','soup','fahita','pork','guacamol',\n",
    "                'de','quesadilla','casserol','black','dip','burrito','corn']\n",
    "\n",
    "@labeling_function()\n",
    "def mexican_title_keywords(x):\n",
    "        if any(word in x.title for word in mex_keywords):\n",
    "            return MEXICAN\n",
    "        else:\n",
    "            return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling function that checks whether identified mexican keywords are in the description column of the RDD\n",
    "mex_keywords_des = ['tortilla','corn','bean','salsa','chilli','spici','chip','simmer','taco','season','soup']\n",
    "\n",
    "@labeling_function()\n",
    "def mexican_des_keywords(x):\n",
    "        if any(word in x.des_stemmed for word in mex_keywords_des):\n",
    "            return MEXICAN\n",
    "        else:\n",
    "            return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling function that checks whether combination of tortilla + corn/salsa/bean is in the description column of the RDD\n",
    "\n",
    "@labeling_function()\n",
    "def tortilla_corn(x):\n",
    "    return MEXICAN if re.search(r\"(?=.*tortilla)(?=.*(corn|salsa|bean))\", x.des_stemmed, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling function that checks whether combination of salsa + chill/sauce/soup/fill is in the description column of the RDD\n",
    "@labeling_function()\n",
    "def salsa_food(x):\n",
    "    return MEXICAN if re.search(r\"(?=.*(salsa))(?=.*(chilli|sauc|sauce|soup|fill))\", x.des_stemmed, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling function that checks whether combination of chip + chicken/shrimp/chilli/pepper/sauce/fill is in the description column of the RDD\n",
    "@labeling_function()\n",
    "def chip(x):\n",
    "    return MEXICAN if re.search(r\"(?=.*(chip))(?=.*(chicken|shrimp|chilli|pepper|sauc|sauce||fill))\", x.des_stemmed, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling function that checks whether combination of canned + chilli/soup/sauce is in the description column of the RDD\n",
    "@labeling_function()\n",
    "def canned_food(x):\n",
    "    return MEXICAN if re.search(r\"(?=.*(cann))(?=.*(chilli|soup|sauc|sauce))\", x.des_stemmed, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate performance on training and dev sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting PySpark DataFrames to RDD\n",
    "\n",
    "df_tr_rdd = df_tr.rdd\n",
    "df_v_rdd = df_v.rdd\n",
    "\n",
    "# Pooling all LFs into one list that will be passed to SparkLFApplier\n",
    "lfs = [indian_title_keywords, \n",
    "       indian_des_keywords, \n",
    "       currymeat, \n",
    "       cook_food, \n",
    "       sweet_spicy, \n",
    "       slow_cook, \n",
    "       italian_title_keywords, \n",
    "       italian_des_keywords, \n",
    "       pasta_with, \n",
    "       sundried_tomatoes, \n",
    "       ita_regions,\n",
    "       mexican_title_keywords, \n",
    "       mexican_des_keywords, \n",
    "       tortilla_corn, \n",
    "       salsa_food, \n",
    "       chip, \n",
    "       canned_food]\n",
    "\n",
    "# Using SparkLFApplier\n",
    "applier = SparkLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df_tr_rdd)\n",
    "L_val = applier.apply(df_v_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>indian_title_keywords</th>\n",
       "      <td>0</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.149441</td>\n",
       "      <td>0.110962</td>\n",
       "      <td>0.067562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indian_des_keywords</th>\n",
       "      <td>1</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.228635</td>\n",
       "      <td>0.157047</td>\n",
       "      <td>0.109620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>currymeat</th>\n",
       "      <td>2</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>0.000895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cook_food</th>\n",
       "      <td>3</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.033557</td>\n",
       "      <td>0.030425</td>\n",
       "      <td>0.024609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sweet_spicy</th>\n",
       "      <td>4</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.006264</td>\n",
       "      <td>0.006264</td>\n",
       "      <td>0.006264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slow_cook</th>\n",
       "      <td>5</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.021924</td>\n",
       "      <td>0.017897</td>\n",
       "      <td>0.013423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>italian_title_keywords</th>\n",
       "      <td>6</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.187472</td>\n",
       "      <td>0.166443</td>\n",
       "      <td>0.074720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>italian_des_keywords</th>\n",
       "      <td>7</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.230872</td>\n",
       "      <td>0.199553</td>\n",
       "      <td>0.114989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pasta_with</th>\n",
       "      <td>8</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.040268</td>\n",
       "      <td>0.040268</td>\n",
       "      <td>0.017897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sundried_tomatoes</th>\n",
       "      <td>9</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.004474</td>\n",
       "      <td>0.003579</td>\n",
       "      <td>0.000447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ita_regions</th>\n",
       "      <td>10</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.227740</td>\n",
       "      <td>0.212081</td>\n",
       "      <td>0.103356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mexican_title_keywords</th>\n",
       "      <td>11</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.255034</td>\n",
       "      <td>0.207159</td>\n",
       "      <td>0.099329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mexican_des_keywords</th>\n",
       "      <td>12</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.342729</td>\n",
       "      <td>0.289933</td>\n",
       "      <td>0.170917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tortilla_corn</th>\n",
       "      <td>13</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.035794</td>\n",
       "      <td>0.035794</td>\n",
       "      <td>0.010291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salsa_food</th>\n",
       "      <td>14</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.012528</td>\n",
       "      <td>0.012528</td>\n",
       "      <td>0.006264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chip</th>\n",
       "      <td>15</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.026846</td>\n",
       "      <td>0.026846</td>\n",
       "      <td>0.007159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>canned_food</th>\n",
       "      <td>16</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.004027</td>\n",
       "      <td>0.004027</td>\n",
       "      <td>0.003579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         j Polarity  Coverage  Overlaps  Conflicts\n",
       "indian_title_keywords    0      [0]  0.149441  0.110962   0.067562\n",
       "indian_des_keywords      1      [0]  0.228635  0.157047   0.109620\n",
       "currymeat                2      [0]  0.000895  0.000895   0.000895\n",
       "cook_food                3      [0]  0.033557  0.030425   0.024609\n",
       "sweet_spicy              4      [0]  0.006264  0.006264   0.006264\n",
       "slow_cook                5      [0]  0.021924  0.017897   0.013423\n",
       "italian_title_keywords   6      [1]  0.187472  0.166443   0.074720\n",
       "italian_des_keywords     7      [1]  0.230872  0.199553   0.114989\n",
       "pasta_with               8      [1]  0.040268  0.040268   0.017897\n",
       "sundried_tomatoes        9      [1]  0.004474  0.003579   0.000447\n",
       "ita_regions             10      [1]  0.227740  0.212081   0.103356\n",
       "mexican_title_keywords  11      [2]  0.255034  0.207159   0.099329\n",
       "mexican_des_keywords    12      [2]  0.342729  0.289933   0.170917\n",
       "tortilla_corn           13      [2]  0.035794  0.035794   0.010291\n",
       "salsa_food              14      [2]  0.012528  0.012528   0.006264\n",
       "chip                    15      [2]  0.026846  0.026846   0.007159\n",
       "canned_food             16      [2]  0.004027  0.004027   0.003579"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking Statistics of Labelling Functions\n",
    "from snorkel.labeling import LFAnalysis\n",
    "\n",
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Incorrect</th>\n",
       "      <th>Emp. Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>indian_title_keywords</th>\n",
       "      <td>0</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.178667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.090667</td>\n",
       "      <td>61</td>\n",
       "      <td>6</td>\n",
       "      <td>0.910448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indian_des_keywords</th>\n",
       "      <td>1</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.197333</td>\n",
       "      <td>0.144000</td>\n",
       "      <td>74</td>\n",
       "      <td>26</td>\n",
       "      <td>0.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>currymeat</th>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cook_food</th>\n",
       "      <td>3</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.010667</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sweet_spicy</th>\n",
       "      <td>4</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slow_cook</th>\n",
       "      <td>5</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.029333</td>\n",
       "      <td>0.021333</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>italian_title_keywords</th>\n",
       "      <td>6</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.141333</td>\n",
       "      <td>0.128000</td>\n",
       "      <td>0.053333</td>\n",
       "      <td>45</td>\n",
       "      <td>8</td>\n",
       "      <td>0.849057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>italian_des_keywords</th>\n",
       "      <td>7</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.221333</td>\n",
       "      <td>0.189333</td>\n",
       "      <td>0.125333</td>\n",
       "      <td>58</td>\n",
       "      <td>25</td>\n",
       "      <td>0.698795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pasta_with</th>\n",
       "      <td>8</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.034667</td>\n",
       "      <td>0.034667</td>\n",
       "      <td>0.021333</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sundried_tomatoes</th>\n",
       "      <td>9</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>0.002667</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ita_regions</th>\n",
       "      <td>10</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.202667</td>\n",
       "      <td>0.194667</td>\n",
       "      <td>0.109333</td>\n",
       "      <td>56</td>\n",
       "      <td>20</td>\n",
       "      <td>0.736842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mexican_title_keywords</th>\n",
       "      <td>11</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.269333</td>\n",
       "      <td>0.210667</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>66</td>\n",
       "      <td>35</td>\n",
       "      <td>0.653465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mexican_des_keywords</th>\n",
       "      <td>12</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.352000</td>\n",
       "      <td>0.296000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>61</td>\n",
       "      <td>71</td>\n",
       "      <td>0.462121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tortilla_corn</th>\n",
       "      <td>13</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salsa_food</th>\n",
       "      <td>14</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.010667</td>\n",
       "      <td>0.010667</td>\n",
       "      <td>0.002667</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chip</th>\n",
       "      <td>15</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>canned_food</th>\n",
       "      <td>16</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         j Polarity  Coverage  Overlaps  Conflicts  Correct  \\\n",
       "indian_title_keywords    0      [0]  0.178667  0.133333   0.090667       61   \n",
       "indian_des_keywords      1      [0]  0.266667  0.197333   0.144000       74   \n",
       "currymeat                2       []  0.000000  0.000000   0.000000        0   \n",
       "cook_food                3      [0]  0.016000  0.013333   0.010667        3   \n",
       "sweet_spicy              4      [0]  0.005333  0.005333   0.005333        2   \n",
       "slow_cook                5      [0]  0.040000  0.029333   0.021333        7   \n",
       "italian_title_keywords   6      [1]  0.141333  0.128000   0.053333       45   \n",
       "italian_des_keywords     7      [1]  0.221333  0.189333   0.125333       58   \n",
       "pasta_with               8      [1]  0.034667  0.034667   0.021333       12   \n",
       "sundried_tomatoes        9      [1]  0.005333  0.005333   0.002667        2   \n",
       "ita_regions             10      [1]  0.202667  0.194667   0.109333       56   \n",
       "mexican_title_keywords  11      [2]  0.269333  0.210667   0.120000       66   \n",
       "mexican_des_keywords    12      [2]  0.352000  0.296000   0.200000       61   \n",
       "tortilla_corn           13      [2]  0.016000  0.016000   0.008000        6   \n",
       "salsa_food              14      [2]  0.010667  0.010667   0.002667        4   \n",
       "chip                    15      [2]  0.016000  0.016000   0.008000        4   \n",
       "canned_food             16      [2]  0.008000  0.008000   0.008000        1   \n",
       "\n",
       "                        Incorrect  Emp. Acc.  \n",
       "indian_title_keywords           6   0.910448  \n",
       "indian_des_keywords            26   0.740000  \n",
       "currymeat                       0   0.000000  \n",
       "cook_food                       3   0.500000  \n",
       "sweet_spicy                     0   1.000000  \n",
       "slow_cook                       8   0.466667  \n",
       "italian_title_keywords          8   0.849057  \n",
       "italian_des_keywords           25   0.698795  \n",
       "pasta_with                      1   0.923077  \n",
       "sundried_tomatoes               0   1.000000  \n",
       "ita_regions                    20   0.736842  \n",
       "mexican_title_keywords         35   0.653465  \n",
       "mexican_des_keywords           71   0.462121  \n",
       "tortilla_corn                   0   1.000000  \n",
       "salsa_food                      0   1.000000  \n",
       "chip                            2   0.666667  \n",
       "canned_food                     2   0.333333  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting summary on VALIDATION set\n",
    "\n",
    "LFAnalysis(L_val, lfs=lfs).lf_summary(Y_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment regarding the performance of LFs**\n",
    "\n",
    "From the tables above it can be seen that the LFs vary in their coverage, which describes what proportion of the dataset a label covers. \n",
    "\n",
    "Indeed, the label with highest coverage is 'mexican_des_keywords' (34.27%) while the lowest are 'currymeat', 'sweet_spicy', 'sundried_tomatoes', 'canned_food' (all nearly 0). It can be argued that this disproportion ranges due to the specificity of the latter LFs. Secondly, 12 out of 14 LFs have conflicts and overlaps. This should be also visible which will follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bucket or something here, some explanations for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our labeling functions vary in coverage, accuracy, and how much they overlap/conflict with one another. We can view a histogram of how many LF labels the data points in our dev set have to get an idea of our total coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAaHklEQVR4nO3de5gddZ3n8fcn4eIFQZA4jgmQgAGFUcFporPMiCKXuGDC44CGQTfOssuC4I2VNawzOBt3ZyOuzE3kMhBFB4gIKJkliDwSYBZF0qCCCQZCiNAGpcdwUxTo5LN/VDVzclLdXU26+hw6n9fz9NOnLr863+6k+9NVv6rfT7aJiIhoN6nTBURERHdKQERERKUEREREVEpAREREpQRERERU2q7TBYyV3Xff3dOnT+90GRERLyp33nnnv9qeUrWt0YCQNBv4O2AycLHtRW3bTwFOAzYCvwZOtr2q3HYWcFK57aO2bxjuvaZPn05vb+/YfxEREROYpJ8Nta2xS0ySJgPnAe8G9gdOkLR/226X236j7QOBc4Bzy7b7A/OAA4DZwJfK40VExDhpsg9iFrDG9lrbzwJLgLmtO9h+smXx5cDgU3tzgSW2n7H9ILCmPF5ERIyTJi8xTQUeblnuA97avpOk04AzgB2Aw1ra3t7WdmozZUZERJUmzyBUsW6LcT1sn2d7H+BTwF+Mpq2kkyX1Surt7+/fqmIjImJzTQZEH7BHy/I0YP0w+y8Bjh1NW9sX2e6x3TNlSmUnfEREvEBNBsQKYKakGZJ2oOh0Xtq6g6SZLYtHA/eXr5cC8yTtKGkGMBO4o8FaIyKiTWN9ELYHJJ0O3EBxm+ti2yslLQR6bS8FTpd0OPAc8Bgwv2y7UtKVwCpgADjN9samao2IiC1pogz33dPT4zwHERExOpLutN1TtS1DbURERKUJM9TGRDV9wXVjdqx1i44es2NFxMSXM4iIiKiUgIiIiEq5xNSAsbwsFBHRKTmDiIiISgmIiIiolICIiIhKCYiIiKiUgIiIiEoJiIiIqJSAiIiISgmIiIiolICIiIhKCYiIiKiUgIiIiEoJiIiIqJSAiIiISgmIiIiolICIiIhKCYiIiKiUgIiIiEoJiIiIqJSAiIiISgmIiIiolICIiIhKCYiIiKjUaEBImi1ptaQ1khZUbD9D0ipJd0v6rqS9WrZtlPSj8mNpk3VGRMSWtmvqwJImA+cBRwB9wApJS22vatnth0CP7aclnQqcA7y/3PZb2wc2VV9ERAyvyTOIWcAa22ttPwssAea27mB7ue2ny8XbgWkN1hMREaPQZEBMBR5uWe4r1w3lJOD6luWXSOqVdLukY6saSDq53Ke3v79/6yuOiIjnNXaJCVDFOlfuKH0A6AEObVm9p+31kvYGbpJ0j+0HNjuYfRFwEUBPT0/lsSMi4oVp8gyiD9ijZXkasL59J0mHA58G5th+ZnC97fXl57XAzcBBDdYaERFtmgyIFcBMSTMk7QDMAza7G0nSQcCFFOHwaMv6XSXtWL7eHTgEaO3cjoiIhjV2icn2gKTTgRuAycBi2yslLQR6bS8FPg/sBHxDEsBDtucAbwAulLSJIsQWtd39FBERDWuyDwLby4BlbevObnl9+BDtvge8scnaIiJieHmSOiIiKiUgIiKiUgIiIiIqJSAiIqJSAiIiIiolICIiolICIiIiKiUgIiKiUgIiIiIqJSAiIqJSAiIiIiolICIiolICIiIiKiUgIiKiUgIiIiIqJSAiIqJSAiIiIiqNGBCDc0OPtC4iIiaWOmcQ36+5LiIiJpAh56SW9BpgKvBSSQcBKjftDLxsHGqLiIgOGjIggKOADwHTgHNb1j8J/PcGa4qIiC4wZEDYvhS4VNKf2r56HGuKiIguUKcP4jZJl0i6HkDS/pJOariuiIjosDoB8WXgBuC15fJ9wMcbqygiIrpCnYDY3faVwCYA2wPAxkarioiIjqsTEL+R9CrAAJLeBjzRaFUREdFxdQLiDGApsI+k24CvAh+pc3BJsyWtlrRG0oKK7WdIWiXpbknflbRXy7b5ku4vP+bX/HoiImKMDHebKwC275J0KLAfxbMQq20/N1I7SZOB84AjgD5ghaSltle17PZDoMf205JOBc4B3i9pN+AzQA/FmcudZdvHRvn1RUTEC1RnqI3jgZfaXgkcC3xd0ltqHHsWsMb2WtvPAkuAua072F5u++ly8XaKZy6geAbjRtsbylC4EZhd6yuKiIgxUecS01/afkrSH1P84r4UOL9Gu6nAwy3LfeW6oZwEXP8C20ZExBirExCDdywdDZxv+1pghxrtVLHOlTtKH6C4nPT50bSVdLKkXkm9/f39NUqKiIi66gTEzyVdCLwPWFaO5FqnXR+wR8vyNGB9+06SDgc+Dcyx/cxo2tq+yHaP7Z4pU6bUKCkiIuqq84v+fRQPys22/TiwG3BmjXYrgJmSZkjaAZhHcTfU88pBAC+kCIdHWzbdABwpaVdJuwJHlusiImKc1LmL6WngGkmvlrRnufqnNdoNSDqd4hf7ZGCx7ZWSFgK9tpdSXFLaCfiGJICHbM+xvUHSZylCBmCh7Q2j/uoiIuIFGzEgJM0BvkAx1MajwJ4UAXHASG1tLwOWta07u+X14cO0XQwsHuk9IiKiGXUuMX0WeBtwn+0ZwOHAbY1WFRERHVcnIJ6z/StgkqRJtpcDBzZcV0REdNiIl5iAxyXtBNwKXCbpUWCg2bIiIqLT6pxBzAWeBj4BfBt4ADimyaIiIqLz6gTE2bY32R6wfantvwc+1XRhERHRWXUC4oiKde8e60IiIqK7DNkHUY6u+mFgb0l3t2x6BbmLKSJiwhuuk/pyisHz/jfQOpfDU3loLSJi4hsyIGw/QTFz3AkAkl4NvATYSdJOth8anxIjIqIT6swH8R5J9wMPArcA6/i3YbkjImKCqtNJ/T/Z/Enqd5E+iIiICS9PUkdERKU8SR0REZXqPkn9WzZ/kvo9TRYVERGdV2c+iN+0LF7aYC0REdFFhntQ7imGmEMawPbOjVQUERFdYbjnIF4BUM4A9wvga4CAEymepo6IiAmsTh/EUba/ZPsp20/aPh/406YLi4iIzqoTEBslnShpsqRJkk4ENjZdWEREdFadgPgz4H3AL8uP48t1ERExgdW5i2kdxa2uERGxDalzBhEREdugOk9SxwQxfcF1Y3asdYuOHrNjRUR3GvIMQtLHys+HjF85ERHRLYa7xPTn5ed/GI9CIiKiuwx3ieleSeuAKW1Tjgqw7Tc1Wtk4G8vLLxERE8FwT1KfIOk1wA3AnPErKSIiusGwdzHZ/oXtNwOPUAyv8Qpgve2f1Tm4pNmSVktaI2lBxfa3S7pL0oCk49q2bZT0o/Jjaf0vKSIixsKIdzFJOhT4KsVUowL2kDTf9q0jtJsMnAccAfQBKyQttb2qZbeHgA8Bn6w4xG9tZ2KiiIgOqXOb67nAkbZXA0jaF7gC+MMR2s0C1theW7ZbQvHA3fMBUT6Eh6RNo648IiIaVedBue0HwwHA9n3A9jXaTQUeblnuK9fV9RJJvZJul3Rs1Q6STi736e3v7x/FoSMiYiR1ziB6JV1CMdw3FMN931mjnSrWDTm/RIU9ba+XtDdwk6R7bD+w2cHsi4CLAHp6ekZz7IiIGEGdM4hTgZXAR4GPUVwiOqVGuz5gj5blacD6uoXZXl9+XgvcDBxUt21ERGy9OoP1PUPRD3HuKI+9ApgpaQbwc2AeNUeBlbQr8LTtZyTtDhwCnDPK94+IiK3Q2GB9tgeA0ymeo7gXuNL2SkkLJc0BkHSwpD6KIcQvlLSybP4GiktbPwaWA4va7n6KiIiGNTpYn+1lwLK2dWe3vF5Bcempvd33gDc2WVtERAwvw31HRESlOg/K7QucCezVur/twxqsKyIiOqzOJaZvABcA/0jmoo6I2GbUCYgB2+c3XklERHSVOn0Q/yzpw5J+X9Jugx+NVxYRER1V5wxifvn5zJZ1BvYe+3IiIqJb1HlQbsZ4FBIREd2lzl1M21MMt/H2ctXNwIW2n2uwroiI6LA6l5jOpxi99Uvl8gfLdf+pqaIiIqLz6gTEweWscoNuKofAiIiICazOXUwbJe0zuFAOv53nISIiJrg6ZxBnAsslraWY42Ev4M8brSoiIjquzl1M35U0E9iPIiB+Wg4BHhERE9iQASHpMNs3SXpv26Z9JGH7moZri4iIDhruDOJQ4CbgPRXbDCQgIiImsCEDwvZnypcLbT/Yuq2cJS4iIiawOncxXV2x7qqxLiQiIrrLcH0QrwcOAHZp64fYGXhJ04VFRERnDdcHsR9wDPBKNu+HeAr4z00WFRERnTdcH8S1wLWS/sj298expoiI6AJ1+iBOkfTKwQVJu0pa3GBNERHRBeoExJtsPz64YPsx4KDmSoqIiG5QJyAmSdp1cKGcTa7OEB0REfEiVucX/ReA70kavLX1eOB/NVdSRER0gzpjMX1V0p3AOynGYnqv7VWNVxYRER1V61KR7ZWS+imff5C0p+2HGq0sIiI6asQ+CElzJN0PPAjcAqwDrm+4roiI6LA6ndSfBd4G3Gd7BvAu4LY6B5c0W9JqSWskLajY/nZJd0kakHRc27b5ku4vP+bXeb+IiBg7dQLiOdu/oribaZLt5cCBIzWSNBk4D3g3sD9wgqT923Z7CPgQcHlb292AzwBvBWYBn2m9kyoiIppXpw/icUk7AbcCl0l6FBio0W4WsMb2WgBJS4C5wPMd3LbXlds2tbU9CrjR9oZy+43AbOCKGu8bERFjoM4ZxFzgaeATwLeBB6ieI6LdVODhluW+cl0dtdpKOllSr6Te/v7+moeOiIg6hg2I8jLRtbY32R6wfantvy8vOY1EFetcs65abW1fZLvHds+UKVNqHjoiIuoYNiBsbwSelrTLCzh2H7BHy/I0YP04tI2IiDFQpw/id8A9ZT/AbwZX2v7oCO1WADPL2ed+DswD/qxmXTcAf93SMX0kcFbNthERMQbqBMR15ceo2B6QdDrFL/vJwOLygbuFQK/tpZIOBr4J7Aq8R9L/sH2A7Q2SPksRMlBMe7phtDVERMQLN9yMcnvafsj2pS/04LaXAcva1p3d8noFxeWjqraLgQwrHhHRIcP1QXxr8IWkqnmpIyJiAhsuIFrvJNq76UIiIqK7DBcQHuJ1RERsA4brpH6zpCcpziReWr6mXLbtnRuvLiIiOmbIgLA9eTwLiYiI7lJnqI2IiNgGJSAiIqJSAiIiIiolICIiolICIiIiKiUgIiKiUgIiIiIqJSAiIqJSAiIiIiolICIiolKdCYMitjB9wajnkBrSukVHj9mxImLs5AwiIiIqJSAiIqJSAiIiIiolICIiolICIiIiKiUgIiKiUgIiIiIqJSAiIqJSAiIiIiolICIiolKjASFptqTVktZIWlCxfUdJXy+3/0DS9HL9dEm/lfSj8uOCJuuMiIgtNTYWk6TJwHnAEUAfsELSUturWnY7CXjM9uskzQM+B7y/3PaA7QObqi8iIobX5BnELGCN7bW2nwWWAHPb9pkLXFq+vgp4lyQ1WFNERNTUZEBMBR5uWe4r11XuY3sAeAJ4VblthqQfSrpF0p9UvYGkkyX1Surt7+8f2+ojIrZxTQZE1ZmAa+7zCLCn7YOAM4DLJe28xY72RbZ7bPdMmTJlqwuOiIh/02RA9AF7tCxPA9YPtY+k7YBdgA22n7H9KwDbdwIPAPs2WGtERLRpMiBWADMlzZC0AzAPWNq2z1Jgfvn6OOAm25Y0pezkRtLewExgbYO1RkREm8buYrI9IOl04AZgMrDY9kpJC4Fe20uBS4CvSVoDbKAIEYC3AwslDQAbgVNsb2iq1oiI2FKjU47aXgYsa1t3dsvr3wHHV7S7Gri6ydoiImJ4eZI6IiIqJSAiIqJSAiIiIiolICIiolICIiIiKiUgIiKiUgIiIiIqJSAiIqJSAiIiIiolICIiolICIiIiKiUgIiKiUqOD9UXUMX3BdWN2rHWLjh6zY0Vs63IGERERlRIQERFRKQERERGVEhAREVEpAREREZUSEBERUSkBERERlRIQERFRKQERERGVEhAREVEpAREREZUSEBERUSmD9cWEkoH/IsZOziAiIqJSowEhabak1ZLWSFpQsX1HSV8vt/9A0vSWbWeV61dLOqrJOiMiYkuNBYSkycB5wLuB/YETJO3ftttJwGO2Xwf8DfC5su3+wDzgAGA28KXyeBERMU6a7IOYBayxvRZA0hJgLrCqZZ+5wF+Vr68CvihJ5foltp8BHpS0pjze9xusN2Iz6c+IbV2TATEVeLhluQ9461D72B6Q9ATwqnL97W1tp7a/gaSTgZPLxV9LWr0V9e4O/OtWtG9K6hqdrqxLn+vOuujS7xepa7S2pq69htrQZECoYp1r7lOnLbYvAi4afWlbktRru2csjjWWUtfopK7RSV2js63V1WQndR+wR8vyNGD9UPtI2g7YBdhQs21ERDSoyYBYAcyUNEPSDhSdzkvb9lkKzC9fHwfcZNvl+nnlXU4zgJnAHQ3WGhERbRq7xFT2KZwO3ABMBhbbXilpIdBreylwCfC1shN6A0WIUO53JUWH9gBwmu2NTdVaGpNLVQ1IXaOTukYndY3ONlWXij/YIyIiNpcnqSMiolICIiIiKm3zATHScCCdIGkPScsl3StppaSPdbqmVpImS/qhpP/b6VoGSXqlpKsk/bT8vv1Rp2sCkPSJ8t/wJ5KukPSSDtayWNKjkn7Ssm43STdKur/8vGuX1PX58t/ybknflPTKbqirZdsnJVnS7t1Sl6SPlL/LVko6Zyzea5sOiJrDgXTCAPBfbb8BeBtwWpfUNehjwL2dLqLN3wHftv164M10QX2SpgIfBXps/wHFzRrzOljSVyiGrmm1APiu7ZnAd8vl8fYVtqzrRuAPbL8JuA84a7yLorouJO0BHAE8NN4Flb5CW12S3kkxAsWbbB8A/J+xeKNtOiBoGQ7E9rPA4HAgHWX7Edt3la+fovhlt8WT5J0gaRpwNHBxp2sZJGln4O0Ud8Vh+1nbj3e2qudtB7y0fM7nZXTweR7bt1LcLdhqLnBp+fpS4NhxLYrqumx/x/ZAuXg7xbNQHa+r9DfAf6Pi4d3xMERdpwKLyuGJsP3oWLzXth4QVcOBdMUv4kHlCLcHAT/obCXP+1uKH45NnS6kxd5AP/Dl8tLXxZJe3umibP+c4i+5h4BHgCdsf6ezVW3h92w/AsUfJsCrO1xPlf8IXN/pIgAkzQF+bvvHna6lzb7An5SjYt8i6eCxOOi2HhC1hvToFEk7AVcDH7f9ZBfUcwzwqO07O11Lm+2AtwDn2z4I+A2duVSymfJ6/lxgBvBa4OWSPtDZql5cJH2a4pLrZV1Qy8uATwNnd7qWCtsBu1Jckj4TuLIc+HSrbOsB0bVDekjaniIcLrN9TafrKR0CzJG0juJy3GGS/qmzJQHFv2Of7cGzrKsoAqPTDgcetN1v+zngGuDfdbimdr+U9PsA5ecxuTQxFiTNB44BTnR3PLC1D0XY/7j8GZgG3CXpNR2tqtAHXOPCHRRn+Fvdgb6tB0Sd4UDGXZn8lwD32j630/UMsn2W7Wm2p1N8r26y3fG/iG3/AnhY0n7lqnex+bDynfIQ8DZJLyv/Td9FF3Set2kd7mY+cG0Ha3mepNnAp4A5tp/udD0Atu+x/Wrb08ufgT7gLeX/v077FnAYgKR9gR0Yg1Fnt+mAKDvBBocDuRe40vbKzlYFFH+pf5DiL/QflR//vtNFdbmPAJdJuhs4EPjrDtdDeUZzFXAXcA/Fz1vHhmqQdAXFnCr7SeqTdBKwCDhC0v0Ud+Ys6pK6vgi8Arix/P9/QZfU1XFD1LUY2Lu89XUJMH8szroy1EZERFTaps8gIiJiaAmIiIiolICIiIhKCYiIiKiUgIiIiEoJiHhRK0fU/ELL8icl/dUYHfsrko4bi2ON8D7HlyPQLm9bP71qJNG2fd4x2lF1Jd0sacwnuI+JJwERL3bPAO/txLDLwylHCq7rJODDtt/ZVD0RL0QCIl7sBigePvtE+4b2MwBJvy4/v6Mc0OxKSfdJWiTpREl3SLpH0j4thzlc0r+U+x1Ttp9czlewopyv4L+0HHe5pMspHoxrr+eE8vg/kfS5ct3ZwB8DF0j6/FBfZHk28S+S7io/Wofs2FnFnAmrJF0gaVLZ5khJ3y/3/0Y5tlfrMSeX36OflHVt8T2Mbdt2nS4gYgycB9yt0U2S8mbgDRTDJq8FLrY9S8XkTB8BPl7uNx04lGIcnuWSXgf8B4qRWQ+WtCNwm6TBUVpnUcxj8GDrm0l6LfA54A+Bx4DvSDrW9kJJhwGftN07TL2PAkfY/p2kmcAVwOBlolkU85n8DPg2xRnVzcBfAIfb/o2kTwFnAAtbjnkgMLWcqwJ1YFKe6G4JiHjRs/2kpK9STM7z25rNVgwOcy3pAWDwF/w9QOulnittbwLul7QWeD1wJPCmlrOTXYCZwLPAHe3hUDoYuNl2f/mel1HMYfGtmvVuD3xR0oHARorhnQfdYXttedwrKM5IfkcRGreVg3ruQDE8Q6u1FMMz/ANwXcv3IAJIQMTE8bcUYx59uWXdAOVl1HKwvB1atj3T8npTy/ImNv+5aB+LxhTDxH/E9g2tGyS9g2Ko8SpbO/TyJ4BfUpz5TKIIgJFqvNH2CUMd0PZjkt4MHAWcBryPYu6FCCB9EDFB2N4AXEnR4TtoHcUlHSjmZdj+BRz6eEmTyn6JvYHVFIM7nqpiSHYk7auRJyj6AXCopN3LDuwTgFtGUccuwCPl2cwHKaYvHTRLxYjEk4D3A/+PYha2Q8pLYpQjyraedVB27E+yfTXwl3THEOnRRXIGERPJFyhG5x30j8C1ku6gmG95qL/uh7Oa4hf57wGnlH0AF1P0TdxVnpn0M8JUnbYfkXQWsJzir/tltkcztPaXgKslHV8eo/Vr+T7FKKxvBG4Fvml7k6QPAVeU/SRQ9Enc19JuKsUsfIN/KHZi3ufoYhnNNSIiKuUSU0REVEpAREREpQRERERUSkBERESlBERERFRKQERERKUEREREVPr/lATM5+AsJUoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_label_frequency(L):\n",
    "    plt.hist((L != ABSTAIN).sum(axis=1), density=True, bins=range(L.shape[1]))\n",
    "    plt.xlabel(\"Number of labels\")\n",
    "    plt.ylabel(\"Fraction of dataset\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_label_frequency(L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_train = label_model.predict_proba(L=L_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that around half of our train dataset data points have 2 or fewer labels from LFs. Fortunately, the signal we do have can be used to train a classifier over the comment text directly, allowing it to generalize beyond what we've specified via our LFs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compared against Y_test - actual values of L\n",
    "L_test = applier.apply(df_ts.rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we use LabelModel presented by spark which (as documentation) claims to produce probablity-aware labels to train the further classification model. However, comparison between Label Model and Majority Vote is also provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy:   70.5%\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling import MajorityLabelVoter\n",
    "\n",
    "majority_model = MajorityLabelVoter(cardinality = 3)\n",
    "preds_train = majority_model.predict(L = L_train)\n",
    "\n",
    "majority_acc = majority_model.score(L = L_test, Y = Y_test, tie_break_policy = \"random\")[\"accuracy\"]\n",
    "print(f\"{'Majority Vote Accuracy:':<25} {majority_acc * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Model Accuracy:     70.6%\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling import LabelModel\n",
    "\n",
    "label_model = LabelModel(cardinality = 3, verbose = True)\n",
    "label_model.fit(L_train, n_epochs = 100, log_freq = 100, seed = 11)\n",
    "\n",
    "label_model_acc = label_model.score(L = L_test, Y = Y_test, tie_break_policy = \"random\")[\"accuracy\"]\n",
    "print(f\"{'Label Model Accuracy:':<25} {label_model_acc * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above Label Model performs better in labelling test split. (NOTE from Max - n_epochs affect Label model). Cardinality = number of labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a classifier with Scikit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are training our classifier with Scikit-Learn, following successful classification examples from the Snorkle documentation. We tried using  MLlib which supports two linear methods for classification: linear Support Vector Machines (SVMs) and logistic regression. However, as Linear SVMs supports only binary classification, logistic regression would be the only suitable option for a multiclass classification problems like ours. We ran into issues with our selected MLlib Multiclass classifier (\"LogisticRegressionWithLBFGS\") not accepting our input, even after data conversions. Thus, we switch to Scikit and also focus on a linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import models \n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Spark train dataframe to Pandas dataframe to proceed\n",
    "pandas_df_tr = df_tr.select(\"*\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Spark test dataframe to Pandas dataframe to proceed\n",
    "pandas_df_ts = df_ts.select(\"*\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>des_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coconutcoriander chutney</td>\n",
       "      <td>simpl prepar incred flavour take minut prepar ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>curd rice</td>\n",
       "      <td>curd rice yoghurt rice indian favourit easi cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>indian khichri</td>\n",
       "      <td>khichri popular easi dish northern region indi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kheer</td>\n",
       "      <td>popular pakistani rice pud flavour cardamon ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>madras potato curry</td>\n",
       "      <td>dish make colour main dinner parti potato caul...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      title                                        des_stemmed\n",
       "0  coconutcoriander chutney  simpl prepar incred flavour take minut prepar ...\n",
       "1                 curd rice  curd rice yoghurt rice indian favourit easi cu...\n",
       "2            indian khichri  khichri popular easi dish northern region indi...\n",
       "3                     kheer  popular pakistani rice pud flavour cardamon ro...\n",
       "4       madras potato curry  dish make colour main dinner parti potato caul..."
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check dataframe before proceeding\n",
    "pandas_df_tr.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide to merge the two columns to unify the following work, namely filtering out unlabeled data points, featurization and classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Dataframe Changes:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the two existing columns into one\n",
    "pandas_df_tr['title_description_merged'] = pandas_df_tr[['title', 'des_stemmed']].agg(' '.join, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_description_merged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>smoky eggplant and yogurt salad pure eggplant ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2137</th>\n",
       "      <td>pico de gallo fresh tomato salsa made red onio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>chicken and ricotta lasagne great chicken take...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>chicken wrapped in parma ham with pizzaiola sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>spaghetti aglio e olio say box spaghetti toss ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               title_description_merged\n",
       "509   smoky eggplant and yogurt salad pure eggplant ...\n",
       "2137  pico de gallo fresh tomato salsa made red onio...\n",
       "855   chicken and ricotta lasagne great chicken take...\n",
       "846   chicken wrapped in parma ham with pizzaiola sa...\n",
       "817   spaghetti aglio e olio say box spaghetti toss ..."
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop old columns\n",
    "pandas_df_tr.drop(columns = ['title', 'des_stemmed'], inplace = True)\n",
    "pandas_df_tr.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Dataframe Changes:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the two existing columns into one\n",
    "pandas_df_ts['title_description_merged'] = pandas_df_ts[['title', 'des_stemmed']].agg(' '.join, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_label</th>\n",
       "      <th>title_description_merged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>1</td>\n",
       "      <td>baked sweet peaches stuffed with figs and pine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0</td>\n",
       "      <td>carrot soup indienne i carrot featur creami cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>2</td>\n",
       "      <td>cheesy nacho chilli mexican style dish tasti c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0</td>\n",
       "      <td>beet and yogurt salad dish consist slice beet ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>1</td>\n",
       "      <td>tomato and basil salad simpl salad perfect gri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     num_label                           title_description_merged\n",
       "402          1  baked sweet peaches stuffed with figs and pine...\n",
       "145          0  carrot soup indienne i carrot featur creami cu...\n",
       "550          2  cheesy nacho chilli mexican style dish tasti c...\n",
       "131          0  beet and yogurt salad dish consist slice beet ...\n",
       "283          1  tomato and basil salad simpl salad perfect gri..."
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop a selection of old columns (not the label)\n",
    "pandas_df_ts.drop(columns = ['title', 'des_stemmed'], inplace = True)\n",
    "pandas_df_ts.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out unlabeled data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve performance because of missing labels in some LFs, we filter them out using a module from the Snorkel library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the library and filtering out unlabeled data points\n",
    "\n",
    "from snorkel.labeling import filter_unlabeled_dataframe\n",
    "\n",
    "df_train_filtered, probs_train_filtered = filter_unlabeled_dataframe(\n",
    "    X = pandas_df_tr, y = probs_train, L = L_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suggested in the Snorkel Tutorial, we implement \"bag of n-grams\" for simplicity and performance reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range = (1, 5))\n",
    "X_train = vectorizer.fit_transform(df_train_filtered.title_description_merged.tolist())\n",
    "X_test = vectorizer.transform(pandas_df_ts.title_description_merged.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics by class\n",
    "labels = data.map(lambda lp: lp.label).distinct().collect()\n",
    "for label in sorted(labels):\n",
    "    print(\"Class %s precision = %s\" % (label, metrics.precision(label)))\n",
    "    print(\"Class %s recall = %s\" % (label, metrics.recall(label)))\n",
    "    print(\"Class %s F1 Measure = %s\" % (label, metrics.fMeasure(label, beta=1.0)))\n",
    "\n",
    "# Weighted stats\n",
    "print(\"Weighted recall = %s\" % metrics.weightedRecall)\n",
    "print(\"Weighted precision = %s\" % metrics.weightedPrecision)\n",
    "print(\"Weighted F(1) Score = %s\" % metrics.weightedFMeasure())\n",
    "print(\"Weighted F(0.5) Score = %s\" % metrics.weightedFMeasure(beta=0.5))\n",
    "print(\"Weighted false positive rate = %s\" % metrics.weightedFalsePositiveRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.utils import probs_to_preds\n",
    "\n",
    "preds_train_filtered = probs_to_preds(probs=probs_train_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1000.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "sklearn_model = LogisticRegression(C = 1e3, solver = \"liblinear\")\n",
    "sklearn_model.fit(X = X_train, y = preds_train_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 80.9%\n"
     ]
    }
   ],
   "source": [
    "# print accuracy of model\n",
    "print(f\"Test Accuracy: {sklearn_model.score(X = X_test, y = Y_test) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.67      0.79       276\n",
      "           1       0.84      0.86      0.85       240\n",
      "           2       0.69      0.92      0.79       229\n",
      "\n",
      "    accuracy                           0.81       745\n",
      "   macro avg       0.83      0.82      0.81       745\n",
      "weighted avg       0.84      0.81      0.81       745\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print further model results (scores)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(Y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learn that the model yields a weighted average precision score of 81 (same as the f-score). The numbers show that on the hold-out set, the model correctly produces the following results as shows in the confusion matrix: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Produce confusion matrix with seaborn library\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(Y_test, predictions)\n",
    "cmn = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis]\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "target_names = [\"indian\", \"italian\", \"mexican\"]\n",
    "sns.heatmap(cmn, annot = True, fmt = '.2f', xticklabels = target_names, yticklabels = target_names)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix shows us in detail the solid performance of the classifier. About 67% of indian dishes were correctly classified, respectively 86% for italian and  overall best and impressively  92% correctly for mexixan. The largest error rate can be observed for 23% indian dishes being classified as mexican, indicating a culinary similiarity between those two cuisines. Conversely, the italian and indian cuisine are the two most distinct cuisines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After building and running the classifier we see a steady increease in test accuracy to ca. \n",
    "81%. As suggested by the documentation, this is because the generalization of the classifier, going fruther than the labeling functions which we built before. With the confusion matrix we were able to understand how the cuisines are different from each other  and which implications this has if a data scientist and a Product Team wanted to autonomously label user generated content."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
