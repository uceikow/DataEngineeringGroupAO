{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "import pyspark\n",
    "\n",
    "number_cores = int(os.environ['NUM_CPUS'])\n",
    "memory_gb = int(os.environ['AVAILABLE_MEMORY_MB']) // 1024\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setMaster('local[{}]'.format(number_cores))\n",
    "        .set('spark.driver.memory', '{}g'.format(memory_gb))\n",
    ")\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[1] appName=pyspark-shell>\n"
     ]
    }
   ],
   "source": [
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOAD DATASET FROM GITHUB (TBD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "indian = sqlContext.read.csv(\"/project/Project/DataEngineeringGroupAO/Recipe_dataset/data_indian.csv\",header=True)\n",
    "italian = sqlContext.read.csv(\"/project/Project/DataEngineeringGroupAO/Recipe_dataset/data_italian.csv\",header=True)\n",
    "mexican = sqlContext.read.csv(\"/project/Project/DataEngineeringGroupAO/Recipe_dataset/data_mexican.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label the data\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "indian = indian.withColumn(\"label\",lit(\"indian\"))\n",
    "italian = italian.withColumn(\"label\",lit(\"italian\"))\n",
    "mexican = mexican.withColumn(\"label\",lit(\"mexican\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+\n",
      "|               Title|         Description| label|\n",
      "+--------------------+--------------------+------+\n",
      "|  Indian Peanut Stew|This is an easy, ...|indian|\n",
      "|        Roomali Roti|There is no leave...|indian|\n",
      "|Spicy Sweet Potat...|It's important to...|indian|\n",
      "|        Chicken Saag|The classic India...|indian|\n",
      "|Paleo Slow Cooker...|Boneless pork loi...|indian|\n",
      "|Bombay Chicken an...|Chicken parts are...|indian|\n",
      "|Indian Carrots, P...|Potatoes, peas an...|indian|\n",
      "|Wendy's Indian Bu...|This recipe resem...|indian|\n",
      "|    Indian Chickpeas|Garbanzo beans, o...|indian|\n",
      "|Dal Makhani (Indi...|These richly spic...|indian|\n",
      "|               Raita|Chopped tomatoes ...|indian|\n",
      "|Yogurt-Marinated ...|A yogurt-based ma...|indian|\n",
      "|Indian-Spiced Roa...|Spicy roasted chi...|indian|\n",
      "|Cauliflower and T...|Pressed tofu cube...|indian|\n",
      "|Channa Masala (Ch...|This fantastic In...|indian|\n",
      "|Bengali Chicken C...|Thy this deliciou...|indian|\n",
      "|  Indian Sweet Bread|A crisp and sweet...|indian|\n",
      "| Rosy's Palak Paneer|An Indian friend ...|indian|\n",
      "|Roti Bread from I...|This version of t...|indian|\n",
      "|Indian Vegetable ...|Basmati rice enha...|indian|\n",
      "+--------------------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combine 3 dataset into one\n",
    "\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def unionAll(dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "dfs = [indian, italian, mexican]\n",
    "recipe = unionAll(dfs)\n",
    "recipe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipe.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert it to RDD\n",
    "recipe_rdd = recipe.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Title='Indian Peanut Stew', Description='This is an easy, authentic dish from South Asia that appeals to a wide range of tastes. The…', label='indian'),\n",
       " Row(Title='Roomali Roti', Description='There is no leavening in this simple, tender Indian flatbread of bread flour, oil, salt and…', label='indian'),\n",
       " Row(Title='Spicy Sweet Potato Salad', Description=\"It's important to use good mayonnaise in this recipe, and to let the cooked potatoes chill…\", label='indian'),\n",
       " Row(Title='Chicken Saag', Description='The classic Indian chicken and spinach dish gets richness from sour cream.', label='indian'),\n",
       " Row(Title='Paleo Slow Cooker Pork Loin', Description='Boneless pork loin slowly cooks in a curried fruit sauce until tender and delicious.', label='indian')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipe_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all packages needed for data cleaning\n",
    "\n",
    "from pyspark.sql.functions import udf, regexp_replace, lower, col\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+\n",
      "|               Title|         Description| label|\n",
      "+--------------------+--------------------+------+\n",
      "|  indian peanut stew|this is an easy, ...|indian|\n",
      "|        roomali roti|there is no leave...|indian|\n",
      "|spicy sweet potat...|it's important to...|indian|\n",
      "|        chicken saag|the classic india...|indian|\n",
      "|paleo slow cooker...|boneless pork loi...|indian|\n",
      "|bombay chicken an...|chicken parts are...|indian|\n",
      "|indian carrots, p...|potatoes, peas an...|indian|\n",
      "|wendy's indian bu...|this recipe resem...|indian|\n",
      "|    indian chickpeas|garbanzo beans, o...|indian|\n",
      "|dal makhani (indi...|these richly spic...|indian|\n",
      "|               raita|chopped tomatoes ...|indian|\n",
      "|yogurt-marinated ...|a yogurt-based ma...|indian|\n",
      "|indian-spiced roa...|spicy roasted chi...|indian|\n",
      "|cauliflower and t...|pressed tofu cube...|indian|\n",
      "|channa masala (ch...|this fantastic in...|indian|\n",
      "|bengali chicken c...|thy this deliciou...|indian|\n",
      "|  indian sweet bread|a crisp and sweet...|indian|\n",
      "| rosy's palak paneer|an indian friend ...|indian|\n",
      "|roti bread from i...|this version of t...|indian|\n",
      "|indian vegetable ...|basmati rice enha...|indian|\n",
      "+--------------------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lowercase\n",
    "\n",
    "recipe = recipe.select(*[lower(col(col_name)).name(col_name) for col_name in recipe.columns])\n",
    "recipe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation and digits\n",
    "\n",
    "recipe_clean = recipe.select(regexp_replace('Title', \"[^a-zA-Z\\\\s]\", \"\").alias('title'), \n",
    "    (regexp_replace('Description', \"[^a-zA-Z\\\\s]\", \"\").alias('des')),'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+\n",
      "|               title|                 des| label|\n",
      "+--------------------+--------------------+------+\n",
      "|  indian peanut stew|this is an easy a...|indian|\n",
      "|        roomali roti|there is no leave...|indian|\n",
      "|spicy sweet potat...|its important to ...|indian|\n",
      "|        chicken saag|the classic india...|indian|\n",
      "|paleo slow cooker...|boneless pork loi...|indian|\n",
      "|bombay chicken an...|chicken parts are...|indian|\n",
      "|indian carrots pe...|potatoes peas and...|indian|\n",
      "|wendys indian but...|this recipe resem...|indian|\n",
      "|    indian chickpeas|garbanzo beans on...|indian|\n",
      "|dal makhani india...|these richly spic...|indian|\n",
      "|               raita|chopped tomatoes ...|indian|\n",
      "|yogurtmarinated s...|a yogurtbased mar...|indian|\n",
      "|indianspiced roas...|spicy roasted chi...|indian|\n",
      "|cauliflower and t...|pressed tofu cube...|indian|\n",
      "|channa masala chi...|this fantastic in...|indian|\n",
      "|bengali chicken c...|thy this deliciou...|indian|\n",
      "|  indian sweet bread|a crisp and sweet...|indian|\n",
      "|  rosys palak paneer|an indian friend ...|indian|\n",
      "|roti bread from i...|this version of t...|indian|\n",
      "|indian vegetable ...|basmati rice enha...|indian|\n",
      "+--------------------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recipe_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+\n",
      "|               title|           des_clean| label|\n",
      "+--------------------+--------------------+------+\n",
      "|  indian peanut stew|[easy, authentic,...|indian|\n",
      "|        roomali roti|[leavening, simpl...|indian|\n",
      "|spicy sweet potat...|[important, use, ...|indian|\n",
      "|        chicken saag|[classic, indian,...|indian|\n",
      "|paleo slow cooker...|[boneless, pork, ...|indian|\n",
      "|bombay chicken an...|[chicken, parts, ...|indian|\n",
      "|indian carrots pe...|[potatoes, peas, ...|indian|\n",
      "|wendys indian but...|[recipe, resemble...|indian|\n",
      "|    indian chickpeas|[garbanzo, beans,...|indian|\n",
      "|dal makhani india...|[richly, spiced, ...|indian|\n",
      "|               raita|[chopped, tomatoe...|indian|\n",
      "|yogurtmarinated s...|[yogurtbased, mar...|indian|\n",
      "|indianspiced roas...|[spicy, roasted, ...|indian|\n",
      "|cauliflower and t...|[pressed, tofu, c...|indian|\n",
      "|channa masala chi...|[fantastic, india...|indian|\n",
      "|bengali chicken c...|[thy, delicious, ...|indian|\n",
      "|  indian sweet bread|[crisp, sweet, fl...|indian|\n",
      "|  rosys palak paneer|[indian, friend, ...|indian|\n",
      "|roti bread from i...|[version, delicat...|indian|\n",
      "|indian vegetable ...|[basmati, rice, e...|indian|\n",
      "+--------------------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove Stopwords\n",
    "\n",
    "# Tokenize text\n",
    "tokenizer = Tokenizer(inputCol=\"des\", outputCol=\"des_token\")\n",
    "recipe = tokenizer.transform(recipe_clean).select('title','des','des_token','label')\n",
    "# tokenized.select(\"Description\", \"Des_words\")\\\n",
    "    #.withColumn(\"tokens\", countTokens(col(\"Des_words\"))).show(truncate=False)\n",
    "\n",
    "# Remove stopwords\n",
    "remover = StopWordsRemover(inputCol='des_token', outputCol='des_clean')\n",
    "recipe_no_stopw = remover.transform(recipe).select('title','des_clean', 'label')\n",
    "recipe_no_lists = recipe_no_stopw\n",
    "recipe_no_stopw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe = recipe_no_stopw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out different recipes\n",
    "# Create temp table\n",
    "recipe.createOrReplaceTempView('recipes')\n",
    "\n",
    "recipe_ind = sqlContext.sql(\"SELECT * FROM recipes WHERE label == 'indian'\")\n",
    "recipe_ita = sqlContext.sql(\"SELECT * FROM recipes WHERE label == 'italian'\")\n",
    "recipe_mex = sqlContext.sql(\"SELECT * FROM recipes WHERE label == 'mexican'\")\n",
    "# print((recipe_ind.count(), len(recipe_ind.columns)))\n",
    "# print((recipe_ita.count(), len(recipe_ita.columns)))\n",
    "# print((recipe_mex.count(), len(recipe_mex.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We asssume that we do not know labels for the majority of data points, \n",
    "# hence further explore only test split\n",
    "recipe_ind_tr, recipe_ind_ts, recipe_ind_d,recipe_ind_v  = recipe_ind.randomSplit([0.6,0.2,0.1,0.1],seed = 11)\n",
    "recipe_ita_tr, recipe_ita_ts, recipe_ita_d, recipe_ita_v = recipe_ita.randomSplit([0.6,0.2,0.1,0.1],seed = 11)\n",
    "recipe_mex_tr, recipe_mex_ts, recipe_mex_d, recipe_mex_v = recipe_mex.randomSplit([0.6,0.2,0.1,0.1],seed = 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We left aside dev/val split as 10% of each of the datasets and 20% as test split to calculate accuracies of LFs. Bigger test set of \"gold\" is valuable to get more matches of LFs and see overall performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create frequency list\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "top_n = 15\n",
    "\n",
    "ind_counts = recipe_ind_d.select(f.explode('des_clean').alias('col')).groupBy('col').count()\n",
    "ind_des_freq = ind_counts.orderBy(ind_counts[\"count\"].desc()).limit(top_n)\n",
    "\n",
    "ita_counts = recipe_ita_d.select(f.explode('des_clean').alias('col')).groupBy('col').count()\n",
    "ita_des_freq = ita_counts.orderBy(ita_counts[\"count\"].desc()).limit(top_n)\n",
    "\n",
    "mex_counts = recipe_mex_d.select(f.explode('des_clean').alias('col')).groupBy('col').count()\n",
    "mex_des_freq = mex_counts.orderBy(mex_counts[\"count\"].desc()).limit(top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+--------+-----+---------+-----+\n",
      "|      col|count|     col|count|      col|count|\n",
      "+---------+-----+--------+-----+---------+-----+\n",
      "|    curry|   11| italian|   12|  chicken|   18|\n",
      "|     dish|   10|  cheese|    8|  mexican|   13|\n",
      "|   indian|    9|   sauce|    7|    salsa|   10|\n",
      "|   yogurt|    8|  garlic|    7|     corn|    8|\n",
      "|    cumin|    8|   pasta|    7|   recipe|    7|\n",
      "|     rice|    8|   fresh|    5|   cheese|    7|\n",
      "|   spices|    7|  tomato|    5|    beans|    7|\n",
      "|  coconut|    6|tomatoes|    5|    green|    6|\n",
      "|  chicken|    6| lasagna|    5|tortillas|    6|\n",
      "|   spiced|    5|   great|    5|   cooked|    6|\n",
      "|   flavor|    5|   bread|    5|    great|    6|\n",
      "|delicious|    4| chicken|    5|    onion|    6|\n",
      "|   recipe|    4| sausage|    5|     like|    6|\n",
      "|   tossed|    4| ricotta|    5| tomatoes|    5|\n",
      "| simmered|    4|    dish|    4|    spicy|    5|\n",
      "+---------+-----+--------+-----+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View then in one dataframe\n",
    "from pyspark.sql.functions import monotonically_increasing_id \n",
    "\n",
    "df1 = ind_des_freq.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "df2 = ita_des_freq.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "df3 = mex_des_freq.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "\n",
    "des_freq = df1.join(df2,(\"row_id\")).join(df3,(\"row_id\")).drop(\"row_id\")\n",
    "des_freq.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "# concat_ws to convert lists to strings for SparkLFApplier compatability\n",
    "# train split - unlabelled\n",
    "df_tr = recipe_ind_tr.union(recipe_ita_tr)\n",
    "df_tr = df_tr.union(recipe_mex_tr).withColumn(\"des_clean\", concat_ws(\" \", \"des_clean\"))\n",
    "\n",
    "# test split - labelled\n",
    "df_ts = recipe_ind_ts.union(recipe_ita_ts)\n",
    "df_ts = df_ts.union(recipe_mex_ts).withColumn(\"des_clean\", concat_ws(\" \", \"des_clean\"))\n",
    "\n",
    "# dev split - labelled\n",
    "df_d = recipe_ind_d.union(recipe_ita_d)\n",
    "df_d = df_d.union(recipe_mex_d).withColumn(\"des_clean\", concat_ws(\" \", \"des_clean\"))\n",
    "\n",
    "# val split - labelled\n",
    "df_v = recipe_ind_d.union(recipe_ita_v)\n",
    "df_v = df_v.union(recipe_mex_v).withColumn(\"des_clean\", concat_ws(\" \", \"des_clean\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(title='ada adai', des_clean='try crepelike items indianstyle breakfast made lentils rice')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop train label split for further labelling\n",
    "df_tr.drop('label').first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'indian':0, 'italian':1, 'mexican':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+\n",
      "|               title|           des_clean|num_label|\n",
      "+--------------------+--------------------+---------+\n",
      "|           aloo gobi|traditional india...|        0|\n",
      "|andreas dal for i...|india dal means n...|        0|\n",
      "|black pepper goat...|black peppercorns...|        0|\n",
      "|bombay chicken wings|chicken wings lig...|        0|\n",
      "|butternut squash ...|butternut squash ...|        0|\n",
      "|chaat  dahi batat...|fragrant aromatic...|        0|\n",
      "|chicken roti from...|fragrant homemade...|        0|\n",
      "|chickpea coconut ...|garbanzo beans ge...|        0|\n",
      "|    cilantro chutney|green chile peppe...|        0|\n",
      "|creamy cashew chi...|marinating curry ...|        0|\n",
      "|cucumber peanut s...|fresh delicious c...|        0|\n",
      "|     easy curry rice|rice side dish re...|        0|\n",
      "|four seasons chic...|hot sweet sour fr...|        0|\n",
      "|fruited tofu curr...|vegetarian tofu s...|        0|\n",
      "|  goan pork vindaloo|traditional goan ...|        0|\n",
      "|gobi masala cauli...|small florets sti...|        0|\n",
      "|indian barbeque c...|marinate chicken ...|        0|\n",
      "|indian chole aloo...|delicious vegan i...|        0|\n",
      "|indian curry pota...|potato cubes frie...|        0|\n",
      "| indian sabji recipe|cabbage potatoes ...|        0|\n",
      "+--------------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get numerical value for corresponding cusine to validate LFs - clear memory to \n",
    "from pyspark.sql.functions import col, create_map, lit\n",
    "from itertools import chain\n",
    "\n",
    "mapping_func = create_map([lit(x) for x in chain(*mapping.items())])\n",
    "\n",
    "df_v = df_v.withColumn(\"num_label\", mapping_func.getItem(col(\"label\")))\n",
    "# df_dv = df_dv.select('title','des_clean', 'num_label')\n",
    "df_v = df_v.select('title','des_clean', 'num_label')\n",
    "df_v.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_label for test split for accuracy - INCLUDE LABEL TO GET REAL GOLD:)\n",
    "df_ts = df_ts.withColumn(\"num_label\", mapping_func.getItem(col(\"label\")))\n",
    "df_ts = df_ts.select('title','des_clean', 'num_label')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print((df_v.count(), len(df_v.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# create column with actual values for validation and convert to array since lf_summary() works with arrays\n",
    "Y_v = df_v.select('num_label').rdd.flatMap(lambda x: x).collect()\n",
    "Y_v = np.array(Y_v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = df_ts.select('num_label').rdd.flatMap(lambda x: x).collect()\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For clarity, we define constants to represent the class labels and abstaining.\n",
    "ABSTAIN = -1\n",
    "INDIAN = 0\n",
    "ITALIAN = 1\n",
    "MEXICAN = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting snorkel\n",
      "  Using cached snorkel-0.9.3-py3-none-any.whl (139 kB)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.2.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from snorkel) (1.4.1)\n",
      "Collecting torch<1.2.0,>=1.1.0\n",
      "Killed\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'snorkel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-b52b8bfce712>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' pip install snorkel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msnorkel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabeling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkLFApplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msnorkel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabeling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLFAnalysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msnorkel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabeling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlabeling_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'snorkel'"
     ]
    }
   ],
   "source": [
    "! pip install snorkel\n",
    "from snorkel.labeling.apply.spark import SparkLFApplier\n",
    "from snorkel.labeling import LFAnalysis\n",
    "from snorkel.labeling import labeling_function\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_keywords = ['curry','indian','masala','paneer','chutney','curried',\n",
    "                'simmered','cumin','yogurt','coconut']\n",
    "\n",
    "@labeling_function()\n",
    "def indian_keywords(x):\n",
    "        if any(word in x.title for word in ind_keywords):\n",
    "            return INDIAN\n",
    "        else:\n",
    "            return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Combo curry + meat\n",
    "@labeling_function()\n",
    "def currymeat(x):\n",
    "    return INDIAN if re.search(r\"(?=.*curry)(?=.*(chicken|lamb|beef))\", x.des_clean, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cooking process + food name\n",
    "@labeling_function()\n",
    "def cook_food(x):\n",
    "    return INDIAN if re.search(r\"(?=.*(quick|easy))(?=.*(rice|sauce|potatoes))\", x.des_clean, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Combo Sweet + Spicy\n",
    "\n",
    "@labeling_function()\n",
    "def sweet_spicy(x):\n",
    "    return INDIAN if re.search(r\"(?=.*sweet)(?=.*(spicy))\", x.des_clean, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Combo Slow + Cook\n",
    "\n",
    "@labeling_function()\n",
    "def slow_cook(x):\n",
    "    return INDIAN if re.search(r\"(?=.*slow)(?=.*(cook))\", x.des_clean, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr_rdd = df_tr.rdd\n",
    "df_v_rdd = df_v.rdd\n",
    "\n",
    "lfs = [indian_keywords, currymeat, cook_food, sweet_spicy, slow_cook]\n",
    "\n",
    "applier = SparkLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df_tr_rdd)\n",
    "L_dev = applier.apply(df_v_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import LFAnalysis\n",
    "\n",
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFAnalysis(L_dev, lfs=lfs).lf_summary(Y_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Explanation needed:</b> EXPLAIN coverage,ovarlaps,conflicts and etc.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compared against Y_test - actual values of L\n",
    "L_test = applier.apply(df_ts.rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we use LabelModel presented by spark which (as documentation) claims to produce probablity-aware labels to train the further classification model. However, comparison between Label Model and Majority Vote is also provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import LabelModel\n",
    "\n",
    "label_model = LabelModel(cardinality=3, verbose=True)\n",
    "label_model.fit(L_train, n_epochs=500, log_freq=100, seed=11)\n",
    "\n",
    "label_model_acc = label_model.score(L=L_test, Y=Y_test, tie_break_policy=\"random\")[\"accuracy\"]\n",
    "print(f\"{'Label Model Accuracy:':<25} {label_model_acc * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import MajorityLabelVoter\n",
    "\n",
    "majority_model = MajorityLabelVoter()\n",
    "preds_train = majority_model.predict(L=L_train)\n",
    "\n",
    "majority_acc = majority_model.score(L=L_test, Y=Y_test, tie_break_policy=\"random\")[\n",
    "    \"accuracy\"\n",
    "]\n",
    "print(f\"{'Majority Vote Accuracy:':<25} {majority_acc * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above Label Model performs better in labelling test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
