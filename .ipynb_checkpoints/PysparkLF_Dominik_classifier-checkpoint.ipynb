{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some pictures and our coursework title here! Anyone has some ideas plz update here :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "import pyspark\n",
    "\n",
    "number_cores = int(os.environ['NUM_CPUS'])\n",
    "memory_gb = int(os.environ['AVAILABLE_MEMORY_MB']) // 1024\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setMaster('local[{}]'.format(number_cores))\n",
    "        .set('spark.driver.memory', '{}g'.format(memory_gb))\n",
    ")\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[4] appName=pyspark-shell>\n"
     ]
    }
   ],
   "source": [
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "indian = sqlContext.read.csv(\"/project/Project/DataEngineeringGroupAO/Recipe_dataset/data_indian.csv\", header = True)\n",
    "italian = sqlContext.read.csv(\"/project/Project/DataEngineeringGroupAO/Recipe_dataset/data_italian.csv\", header = True)\n",
    "mexican = sqlContext.read.csv(\"/project/Project/DataEngineeringGroupAO/Recipe_dataset/data_mexican.csv\", header = True)\n",
    "new_recipe = sqlContext.read.csv(\"/project/Project/DataEngineeringGroupAO/Recipe_dataset/final_scrape_not_cleaned.csv\", header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indian:  480  italian:  400  mexican:  620  new_recipe:  2340\n"
     ]
    }
   ],
   "source": [
    "# See the size of the dataset\n",
    "print('indian: ', indian.count(),' italian: ', italian.count(), ' mexican: ', mexican.count(),\n",
    "     ' new_recipe: ', new_recipe.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first three datasets, we do not have labels for them. We could use them as training set but three of them combined together is not enough. In this case, we decided to combine them together first, and then do splitting, and then remove the labels of training set. So, we have to first label the unlabelled dataset manually, which is shown as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label the data\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "indian = indian.withColumn(\"label\",lit(\"indian\"))\n",
    "italian = italian.withColumn(\"label\",lit(\"italian\"))\n",
    "mexican = mexican.withColumn(\"label\",lit(\"mexican\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+\n",
      "|               Title|         Description| label|\n",
      "+--------------------+--------------------+------+\n",
      "|  Indian Peanut Stew|This is an easy, ...|indian|\n",
      "|        Roomali Roti|There is no leave...|indian|\n",
      "|Spicy Sweet Potat...|It's important to...|indian|\n",
      "|        Chicken Saag|The classic India...|indian|\n",
      "|Paleo Slow Cooker...|Boneless pork loi...|indian|\n",
      "|Bombay Chicken an...|Chicken parts are...|indian|\n",
      "|Indian Carrots, P...|Potatoes, peas an...|indian|\n",
      "|Wendy's Indian Bu...|This recipe resem...|indian|\n",
      "|    Indian Chickpeas|Garbanzo beans, o...|indian|\n",
      "|Dal Makhani (Indi...|These richly spic...|indian|\n",
      "|               Raita|Chopped tomatoes ...|indian|\n",
      "|Yogurt-Marinated ...|A yogurt-based ma...|indian|\n",
      "|Indian-Spiced Roa...|Spicy roasted chi...|indian|\n",
      "|Cauliflower and T...|Pressed tofu cube...|indian|\n",
      "|Channa Masala (Ch...|This fantastic In...|indian|\n",
      "|Bengali Chicken C...|Thy this deliciou...|indian|\n",
      "|  Indian Sweet Bread|A crisp and sweet...|indian|\n",
      "| Rosy's Palak Paneer|An Indian friend ...|indian|\n",
      "|Roti Bread from I...|This version of t...|indian|\n",
      "|Indian Vegetable ...|Basmati rice enha...|indian|\n",
      "+--------------------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combine all datasets into one\n",
    "\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def unionAll(dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "dfs = [indian, italian, mexican, new_recipe]\n",
    "recipe = unionAll(dfs)\n",
    "recipe.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As these two scraping are from the same website, but using different method, there might be some duplicates in the combined datasets. We try to remove the duplicates before data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3840\n"
     ]
    }
   ],
   "source": [
    "# Size before removing duplicates\n",
    "print(recipe.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "recipe = recipe.dropDuplicates([\"Title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3793\n"
     ]
    }
   ],
   "source": [
    "# Size after dropping duplicates\n",
    "print(recipe.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we come to the data cleaning part! Our dataset is all about **text**, and it needs to be transformed a bit for better analysis.\n",
    "\n",
    "- Lowercase\n",
    "- Remove punctuations and digits. \n",
    "\n",
    "In NLTK, we have packages which helps remove punctuations and digits, but in our case, we try to use **regular expression** to remove unwanted signs in our dataset.\n",
    "- Remove stopwords\n",
    "- Stemming the text \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all packages needed for data cleaning\n",
    "\n",
    "from pyspark.sql.functions import udf, regexp_replace, lower, col\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from pyspark.sql.types import IntegerType, ArrayType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|               Title|         Description|  label|\n",
      "+--------------------+--------------------+-------+\n",
      "|amanda & graham's...|restaurant in our...|italian|\n",
      "|avocado and mango...|a wonderfully fru...|mexican|\n",
      "|batman's best cae...|fast, easy, cream...|italian|\n",
      "|beef chilli with ...|this is a hearty ...|mexican|\n",
      "|chicken enchilada...|chicken, sour cre...|mexican|\n",
      "|chorizo breakfast...|a yummy breakfast...|mexican|\n",
      "|coconut-coriander...|simple preparatio...| indian|\n",
      "|coriander chicken...|this chicken dish...|mexican|\n",
      "|           curd rice|curd rice, or yog...| indian|\n",
      "|daz's tomato chee...|this is my favour...|italian|\n",
      "|easy chicken tikk...|a quick and easy ...| indian|\n",
      "|four seasons ench...|nice and spicy us...|mexican|\n",
      "|homemade four che...|fresh pasta fille...|italian|\n",
      "|homemade paneer c...|paneer is a white...| indian|\n",
      "|      indian khichri|khichri is a very...| indian|\n",
      "|               kheer|this is a popular...| indian|\n",
      "| madras potato curry|this dish makes a...| indian|\n",
      "|meatball sub cass...|start with frozen...|italian|\n",
      "|quick and easy pi...|a quick, chewy pi...|italian|\n",
      "| refried black beans|these low fat ref...|mexican|\n",
      "+--------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lowercase\n",
    "\n",
    "recipe = recipe.select(*[lower(col(col_name)).name(col_name) for col_name in recipe.columns])\n",
    "recipe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|               title|                 des|  label|\n",
      "+--------------------+--------------------+-------+\n",
      "|amanda  grahams m...|restaurant in our...|italian|\n",
      "|avocado and mango...|a wonderfully fru...|mexican|\n",
      "|batmans best caes...|fast easy creamy ...|italian|\n",
      "|beef chilli with ...|this is a hearty ...|mexican|\n",
      "|chicken enchilada...|chicken sour crea...|mexican|\n",
      "|chorizo breakfast...|a yummy breakfast...|mexican|\n",
      "|coconutcoriander ...|simple preparatio...| indian|\n",
      "|coriander chicken...|this chicken dish...|mexican|\n",
      "|           curd rice|curd rice or yogh...| indian|\n",
      "|dazs tomato chees...|this is my favour...|italian|\n",
      "|easy chicken tikk...|a quick and easy ...| indian|\n",
      "|four seasons ench...|nice and spicy us...|mexican|\n",
      "|homemade four che...|fresh pasta fille...|italian|\n",
      "|homemade paneer c...|paneer is a white...| indian|\n",
      "|      indian khichri|khichri is a very...| indian|\n",
      "|               kheer|this is a popular...| indian|\n",
      "| madras potato curry|this dish makes a...| indian|\n",
      "|meatball sub cass...|start with frozen...|italian|\n",
      "|quick and easy pi...|a quick chewy piz...|italian|\n",
      "| refried black beans|these low fat ref...|mexican|\n",
      "+--------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove punctuation and digits\n",
    "\n",
    "recipe_clean = recipe.select(regexp_replace('Title', \"[^a-zA-Z\\\\s]\", \"\").alias('title'), \n",
    "    (regexp_replace('Description', \"[^a-zA-Z\\\\s]\", \"\").alias('des')),'label')\n",
    "recipe_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|               title|           des_clean|  label|\n",
      "+--------------------+--------------------+-------+\n",
      "|amanda  grahams m...|[restaurant, livi...|italian|\n",
      "|avocado and mango...|[wonderfully, fru...|mexican|\n",
      "|batmans best caes...|[fast, easy, crea...|italian|\n",
      "|beef chilli with ...|[hearty, deliciou...|mexican|\n",
      "|chicken enchilada...|[chicken, sour, c...|mexican|\n",
      "|chorizo breakfast...|[yummy, breakfast...|mexican|\n",
      "|coconutcoriander ...|[simple, preparat...| indian|\n",
      "|coriander chicken...|[chicken, dish, d...|mexican|\n",
      "|           curd rice|[curd, rice, yogh...| indian|\n",
      "|dazs tomato chees...|[favourite, start...|italian|\n",
      "|easy chicken tikk...|[quick, easy, cur...| indian|\n",
      "|four seasons ench...|[nice, spicy, use...|mexican|\n",
      "|homemade four che...|[fresh, pasta, fi...|italian|\n",
      "|homemade paneer c...|[paneer, white, c...| indian|\n",
      "|      indian khichri|[khichri, popular...| indian|\n",
      "|               kheer|[popular, pakista...| indian|\n",
      "| madras potato curry|[dish, makes, col...| indian|\n",
      "|meatball sub cass...|[start, frozen, m...|italian|\n",
      "|quick and easy pi...|[quick, chewy, pi...|italian|\n",
      "| refried black beans|[low, fat, refrie...|mexican|\n",
      "+--------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove Stopwords\n",
    "\n",
    "# Tokenize text\n",
    "tokenizer = Tokenizer(inputCol = \"des\", outputCol = \"des_token\")\n",
    "recipe = tokenizer.transform(recipe_clean).select('title','des','des_token','label')\n",
    "# tokenized.select(\"Description\", \"Des_words\")\\\n",
    "    #.withColumn(\"tokens\", countTokens(col(\"Des_words\"))).show(truncate=False)\n",
    "\n",
    "# Remove stopwords\n",
    "remover = StopWordsRemover(inputCol = 'des_token', outputCol = 'des_clean')\n",
    "recipe_no_stopw = remover.transform(recipe).select('title','des_clean', 'label')\n",
    "recipe_no_stopw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|               title|         des_stemmed|  label|\n",
      "+--------------------+--------------------+-------+\n",
      "|amanda  grahams m...|[restaur, live, r...|italian|\n",
      "|avocado and mango...|[wonder, fruiti, ...|mexican|\n",
      "|batmans best caes...|[fast, easi, crea...|italian|\n",
      "|beef chilli with ...|[hearti, delici, ...|mexican|\n",
      "|chicken enchilada...|[chicken, sour, c...|mexican|\n",
      "|chorizo breakfast...|[yummi, breakfast...|mexican|\n",
      "|coconutcoriander ...|[simpl, prepar, i...| indian|\n",
      "|coriander chicken...|[chicken, dish, d...|mexican|\n",
      "|           curd rice|[curd, rice, yogh...| indian|\n",
      "|dazs tomato chees...|[favourit, starte...|italian|\n",
      "|easy chicken tikk...|[quick, easi, cur...| indian|\n",
      "|four seasons ench...|[nice, spici, use...|mexican|\n",
      "|homemade four che...|[fresh, pasta, fi...|italian|\n",
      "|homemade paneer c...|[paneer, white, c...| indian|\n",
      "|      indian khichri|[khichri, popular...| indian|\n",
      "|               kheer|[popular, pakista...| indian|\n",
      "| madras potato curry|[dish, make, colo...| indian|\n",
      "|meatball sub cass...|[start, frozen, m...|italian|\n",
      "|quick and easy pi...|[quick, chewi, pi...|italian|\n",
      "| refried black beans|[low, fat, refri,...|mexican|\n",
      "+--------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stem text\n",
    "stemmer = SnowballStemmer(language = 'english')\n",
    "stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
    "recipe = recipe_no_stopw.withColumn(\"des_stemmed\", stemmer_udf(\"des_clean\")).select('title', 'des_stemmed','label')\n",
    "recipe.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splitting & Pattern Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting\n",
    "\n",
    "maxim's wording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out different recipes\n",
    "# Create temp table\n",
    "recipe.createOrReplaceTempView('recipes')\n",
    "\n",
    "recipe_ind = sqlContext.sql(\"SELECT * FROM recipes WHERE label == 'indian'\")\n",
    "recipe_ita = sqlContext.sql(\"SELECT * FROM recipes WHERE label == 'italian'\")\n",
    "recipe_mex = sqlContext.sql(\"SELECT * FROM recipes WHERE label == 'mexican'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We asssume that we do not know labels for the majority of data points, \n",
    "# hence further explore only test split\n",
    "recipe_ind_tr, recipe_ind_ts, recipe_ind_d,recipe_ind_v = recipe_ind.randomSplit([0.6,0.2,0.1,0.1],seed = 11)\n",
    "recipe_ita_tr, recipe_ita_ts, recipe_ita_d, recipe_ita_v = recipe_ita.randomSplit([0.6,0.2,0.1,0.1],seed = 11)\n",
    "recipe_mex_tr, recipe_mex_ts, recipe_mex_d, recipe_mex_v = recipe_mex.randomSplit([0.6,0.2,0.1,0.1],seed = 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern Exploration\n",
    "\n",
    "Before writing the label functions, we want to get our head around how our dataset look like and see if we can get some ideas from it for writing label functions.\n",
    "\n",
    "Since we want to label the type of the recipe. We made a word frequency dataframe, which displays top 15 words in each type of the recipe to help us identify the keywords connected to each cuise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create frequency list of description\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "top_n = 20\n",
    "\n",
    "ind_counts = recipe_ind_d.select(f.explode('des_stemmed').alias('col')).groupBy('col').count()\n",
    "ind_des_freq = ind_counts.orderBy(ind_counts[\"count\"].desc()).limit(top_n)\n",
    "\n",
    "ita_counts = recipe_ita_d.select(f.explode('des_stemmed').alias('col')).groupBy('col').count()\n",
    "ita_des_freq = ita_counts.orderBy(ita_counts[\"count\"].desc()).limit(top_n)\n",
    "\n",
    "mex_counts = recipe_mex_d.select(f.explode('des_stemmed').alias('col')).groupBy('col').count()\n",
    "mex_des_freq = mex_counts.orderBy(mex_counts[\"count\"].desc()).limit(top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-------+-----+--------+-----+\n",
      "|    col|count|    col|count|     col|count|\n",
      "+-------+-----+-------+-----+--------+-----+\n",
      "|  curri|   56|italian|   49| chicken|   40|\n",
      "| indian|   46|  pasta|   41|    serv|   27|\n",
      "|   rice|   39|   make|   35|    bean|   26|\n",
      "|   serv|   35|  recip|   33|tortilla|   25|\n",
      "|chicken|   31| tomato|   32| mexican|   23|\n",
      "|  recip|   31|  chees|   32|  tomato|   21|\n",
      "|   make|   30|   sauc|   30|   fresh|   21|\n",
      "|   dish|   27|  fresh|   29|   salsa|   20|\n",
      "|   easi|   24|   dish|   27|    dish|   18|\n",
      "|flavour|   23|   serv|   25|    make|   18|\n",
      "|  spice|   20|    use|   24|   chees|   18|\n",
      "|   cook|   19|   easi|   22|   recip|   18|\n",
      "|  spici|   18| delici|   22|  chilli|   17|\n",
      "| delici|   18|       |   22|   green|   17|\n",
      "|   made|   18|  bread|   20|    cook|   16|\n",
      "|    use|   17|   made|   19|    corn|   16|\n",
      "|   love|   15|   love|   17|   great|   15|\n",
      "|   meal|   15| garlic|   17|   spici|   15|\n",
      "| potato|   15|chicken|   16|     use|   15|\n",
      "|  cumin|   14|   tast|   15|    rice|   15|\n",
      "+-------+-----+-------+-----+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View then in one dataframe\n",
    "from pyspark.sql.functions import monotonically_increasing_id \n",
    "\n",
    "df1 = ind_des_freq.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "df2 = ita_des_freq.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "df3 = mex_des_freq.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "\n",
    "des_freq = df1.join(df2,(\"row_id\")).join(df3,(\"row_id\")).drop(\"row_id\")\n",
    "des_freq.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maxim continue with your wording for data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "# train split full / remove lists\n",
    "df_tr = recipe_ind_tr.union(recipe_ita_tr)\n",
    "df_tr = df_tr.union(recipe_mex_tr).withColumn(\"des_stemmed\", concat_ws(\" \", \"des_stemmed\"))\n",
    "\n",
    "# test split \n",
    "df_ts = recipe_ind_ts.union(recipe_ita_ts)\n",
    "df_ts = df_ts.union(recipe_mex_ts).withColumn(\"des_stemmed\", concat_ws(\" \", \"des_stemmed\"))\n",
    "\n",
    "# dev split - labelled\n",
    "df_d = recipe_ind_d.union(recipe_ita_d)\n",
    "df_d = df_d.union(recipe_mex_d).withColumn(\"des_stemmed\", concat_ws(\" \", \"des_stemmed\"))\n",
    "\n",
    "# val split - labelled\n",
    "df_v = recipe_ind_d.union(recipe_ita_v)\n",
    "df_v = df_v.union(recipe_mex_v).withColumn(\"des_stemmed\", concat_ws(\" \", \"des_stemmed\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop train split for labelling\n",
    "# df_tr.drop('label').first()\n",
    "df_tr = df_tr.drop('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'indian':0, 'italian':1, 'mexican':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+\n",
      "|               title|         des_stemmed|num_label|\n",
      "+--------------------+--------------------+---------+\n",
      "|homemade paneer c...|paneer white chee...|        0|\n",
      "|slow cooker balti...|noth better someo...|        0|\n",
      "|fresh coconut chu...|chutney tradit se...|        0|\n",
      "|kashmiri chicken ...|sweet banana exot...|        0|\n",
      "|chicken and cauli...|mild creami curri...|        0|\n",
      "|creamy chicken korma|coconut almond gi...|        0|\n",
      "|gujarati carrot a...|peanut add wonder...|        0|\n",
      "|  kheer rice pudding|flavour indiansty...|        0|\n",
      "|sherry chicken curry|outsid restaur on...|        0|\n",
      "|curried beef with...|medley winter veg...|        0|\n",
      "|authentic chicken...|tender chicken co...|        0|\n",
      "|chicken masala curry|fragrant homemad ...|        0|\n",
      "|          okra curry|slice okra round ...|        0|\n",
      "|      gujarati kadhi|tri indian yogurt...|        0|\n",
      "|    masala spicy tea|masala tea also k...|        0|\n",
      "|vegetarian palak ...|indian dal classi...|        0|\n",
      "|cucumber peanut s...|fresh delici cucu...|        0|\n",
      "|        khara pongal|classic south ind...|        0|\n",
      "|bombay chicken wings|chicken wing ligh...|        0|\n",
      "|black pepper goat...|black peppercorn ...|        0|\n",
      "+--------------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import create_map\n",
    "from itertools import chain\n",
    "\n",
    "mapping_func = create_map([lit(x) for x in chain(*mapping.items())])\n",
    "\n",
    "df_v = df_v.withColumn(\"num_label\", mapping_func.getItem(col(\"label\")))\n",
    "# df_dv = df_dv.select('title','des_clean', 'num_label')\n",
    "df_v = df_v.select('title','des_stemmed', 'num_label')\n",
    "df_v.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_label for test split for accuracy - INCLUDE LABEL TO GET REAL GOLD:)\n",
    "df_ts = df_ts.withColumn(\"num_label\", mapping_func.getItem(col(\"label\")))\n",
    "df_ts = df_ts.select('title','des_stemmed', 'num_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column with actual values \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "Y_v = df_v.select('num_label').rdd.flatMap(lambda x: x).collect()\n",
    "Y_v = np.array(Y_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = df_ts.select('num_label').rdd.flatMap(lambda x: x).collect()\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For clarity, we define constants to represent the class labels and abstaining.\n",
    "ABSTAIN = -1\n",
    "INDIAN = 0\n",
    "ITALIAN = 1\n",
    "MEXICAN = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labelling Functions Introduction\n",
    "\n",
    "Once the pattern exploration has been finished, it was possible to write Labelling Functions. Hereby, due to the fact that there were no structural differences (e.g. there were no visible patterns for one cuisine) between the names and description of any cuisine, the Labelling Functions mainly search for aforementioned keywords or their combinations (obtained through careful exploration of the scraped datasets) that may be relevent to just one cuisine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snorkel in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (0.9.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.33.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from snorkel) (4.43.0)\n",
      "Requirement already satisfied: tensorboardX<2.0,>=1.6 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from snorkel) (1.9)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.2.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from snorkel) (1.4.1)\n",
      "Requirement already satisfied: torch<1.2.0,>=1.1.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from snorkel) (1.1.0)\n",
      "Requirement already satisfied: munkres==1.1.2 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from snorkel) (1.1.2)\n",
      "Requirement already satisfied: networkx<2.4,>=2.2 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from snorkel) (2.3)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.16.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from snorkel) (1.18.1)\n",
      "Collecting scikit-learn<0.22.0,>=0.20.2\n",
      "  Downloading scikit_learn-0.21.3-cp36-cp36m-manylinux1_x86_64.whl (6.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.7 MB 16.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas<0.26.0,>=0.25.0\n",
      "  Using cached pandas-0.25.3-cp36-cp36m-manylinux1_x86_64.whl (10.4 MB)\n",
      "Requirement already satisfied: six in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from tensorboardX<2.0,>=1.6->snorkel) (1.14.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from tensorboardX<2.0,>=1.6->snorkel) (3.11.3)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from networkx<2.4,>=2.2->snorkel) (4.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from scikit-learn<0.22.0,>=0.20.2->snorkel) (0.14.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from pandas<0.26.0,>=0.25.0->snorkel) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from pandas<0.26.0,>=0.25.0->snorkel) (2019.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorboardX<2.0,>=1.6->snorkel) (45.2.0.post20200210)\n",
      "\u001b[31mERROR: faculty-models 0.1.1 has requirement mlflow-faculty>=0.4.3, but you'll have mlflow-faculty 0.4.2 which is incompatible.\u001b[0m\n",
      "Installing collected packages: scikit-learn, pandas\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.22.2.post1\n",
      "    Uninstalling scikit-learn-0.22.2.post1:\n",
      "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.0.1\n",
      "    Uninstalling pandas-1.0.1:\n",
      "      Successfully uninstalled pandas-1.0.1\n",
      "Successfully installed pandas-0.25.3 scikit-learn-0.22.1\n"
     ]
    }
   ],
   "source": [
    "! pip install snorkel\n",
    "from snorkel.labeling.apply.spark import SparkLFApplier\n",
    "from snorkel.labeling import LFAnalysis\n",
    "from snorkel.labeling import labeling_function\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indian LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling function that checks whether identified indian keywords are in the title column of the RDD\n",
    "ind_keywords = ['curri','masala','paneer','chutney','simmered','cumin','yogurt',\n",
    "                'coconut','lentil','veget','chickpea','korma','dal','tandoori','tikka']\n",
    "\n",
    "@labeling_function()\n",
    "def indian_title_keywords(x):\n",
    "        if any(word in x.title for word in ind_keywords):\n",
    "            return INDIAN\n",
    "        else:\n",
    "            return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling function that checks whether identified indian keywords are in the description column of the RDD\n",
    "ind_keywords_des = ['naan','curri','vegetarian','tradit','veget']\n",
    "@labeling_function()\n",
    "def indian_des_keywords(x):\n",
    "        if any(word in x.des_stemmed for word in ind_keywords_des):\n",
    "            return INDIAN\n",
    "        else:\n",
    "            return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling function that checks whether combination of curry + different kinds of meat is in the description column of the RDD\n",
    "@labeling_function()\n",
    "def currymeat(x):\n",
    "    return INDIAN if re.search(r\"(?=.*curry)(?=.*(chicken|lamb|beef|prawn))\", x.des_stemmed, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling function that checks whether combination of quick or easy + different kinds of ingredients is in the description column of the RDD\n",
    "@labeling_function()\n",
    "def cook_food(x):\n",
    "    return INDIAN if re.search(r\"(?=.*(quick|easy))(?=.*(rice|sauc|potato|tomato|bread))\", x.des_stemmed, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling function that checks whether combination of sweet + spicy is in the description column of the RDD\n",
    "\n",
    "@labeling_function()\n",
    "def sweet_spicy(x):\n",
    "    return INDIAN if re.search(r\"(?=.*sweet)(?=.*(spici))\", x.des_stemmed, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling function that checks whether combination of slow + cook is in the description column of the RDD\n",
    "\n",
    "@labeling_function()\n",
    "def slow_cook(x):\n",
    "    return INDIAN if re.search(r\"(?=.*slow)(?=.*(cook))\", x.des_stemmed, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Italian LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling function that checks whether identified italian keywords from are in the title column of the RDD\n",
    "ita_keywords = ['pasta','spaghetti','risotto','sausag','mushroom','lasagn','lasagna','bolognes'\n",
    "                ,'pizza','parmesan','tiramisu','ricotta','bruschetta','bake']\n",
    "\n",
    "@labeling_function()\n",
    "def italian_title_keywords(x):\n",
    "        if any(word in x.title for word in ita_keywords):\n",
    "            return ITALIAN\n",
    "        else:\n",
    "            return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling function that checks whether identified indian keywords are in the description column of the RDD\n",
    "ita_keywords_des = ['garlic','classic','oliv','parmesan','pasta','rich','mozzarella','lasagna','pesto','dente']\n",
    "\n",
    "@labeling_function()\n",
    "def italian_des_keywords(x):\n",
    "        if any(word in x.des_stemmed for word in ita_keywords_des):\n",
    "            return ITALIAN\n",
    "        else:\n",
    "            return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling function that checks whether combination of pasta + different kinds of ingredients is in the description column of the RDD\n",
    "@labeling_function()\n",
    "def pasta_with(x):\n",
    "    return ITALIAN if re.search(r\"(?=.*pasta)(?=.*(chicken|lamb|beef|pesto|creami|cream|shrimp|chees))\", x.des_stemmed, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling function that checks whether combination of tomato +  sun-dried or sundried is in the description column of the RDD\n",
    "@labeling_function()\n",
    "def sundried_tomatoes(x):\n",
    "    return ITALIAN if re.search(r\"(?=.*tomato)(?=.*(sun-dri|sundri))\", x.des_stemmed, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling function that checks whether any of the identified regions is in the description column of the RDD\n",
    "ita_regions = ['tuscan','sicilian', 'romano', 'romaine', 'mediterranean','meditterranean' ]\n",
    "\n",
    "@labeling_function()\n",
    "def ita_regions(x):\n",
    "        if any(word in x.des_stemmed for word in ita_keywords):\n",
    "            return ITALIAN\n",
    "        else:\n",
    "            return ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mexican LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling function that checks whether identified mexican keywords from are in the title column of the RDD\n",
    "mex_keywords = ['enchilada','taco','bean','salsa','soup','fahita','pork','guacamol',\n",
    "                'de','quesadilla','casserol','black','dip','burrito','corn']\n",
    "\n",
    "@labeling_function()\n",
    "def mexican_title_keywords(x):\n",
    "        if any(word in x.title for word in mex_keywords):\n",
    "            return MEXICAN\n",
    "        else:\n",
    "            return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling function that checks whether identified mexican keywords are in the description column of the RDD\n",
    "mex_keywords_des = ['tortilla','corn','bean','salsa','chilli','spici','chip','simmer','taco','season','soup']\n",
    "\n",
    "@labeling_function()\n",
    "def mexican_des_keywords(x):\n",
    "        if any(word in x.des_stemmed for word in mex_keywords_des):\n",
    "            return MEXICAN\n",
    "        else:\n",
    "            return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling function that checks whether combination of tortilla + corn/salsa/bean is in the description column of the RDD\n",
    "\n",
    "@labeling_function()\n",
    "def tortilla_corn(x):\n",
    "    return MEXICAN if re.search(r\"(?=.*tortilla)(?=.*(corn|salsa|bean))\", x.des_stemmed, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling function that checks whether combination of salsa + chill/sauce/soup/fill is in the description column of the RDD\n",
    "@labeling_function()\n",
    "def salsa_food(x):\n",
    "    return MEXICAN if re.search(r\"(?=.*(salsa))(?=.*(chilli|sauc|sauce|soup|fill))\", x.des_stemmed, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling function that checks whether combination of chip + chicken/shrimp/chilli/pepper/sauce/fill is in the description column of the RDD\n",
    "@labeling_function()\n",
    "def chip(x):\n",
    "    return MEXICAN if re.search(r\"(?=.*(chip))(?=.*(chicken|shrimp|chilli|pepper|sauc|sauce||fill))\", x.des_stemmed, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling function that checks whether combination of canned + chilli/soup/sauce is in the description column of the RDD\n",
    "@labeling_function()\n",
    "def canned_food(x):\n",
    "    return MEXICAN if re.search(r\"(?=.*(cann))(?=.*(chilli|soup|sauc|sauce))\", x.des_stemmed, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate performance on training and dev sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting PySpark DataFrames to RDD\n",
    "\n",
    "df_tr_rdd = df_tr.rdd\n",
    "df_v_rdd = df_v.rdd\n",
    "\n",
    "# Pooling all LFs into one list that will be passed to SparkLFApplier\n",
    "lfs = [indian_title_keywords, \n",
    "       indian_des_keywords, \n",
    "       currymeat, \n",
    "       cook_food, \n",
    "       sweet_spicy, \n",
    "       slow_cook, \n",
    "       italian_title_keywords, \n",
    "       italian_des_keywords, \n",
    "       pasta_with, \n",
    "       sundried_tomatoes, \n",
    "       ita_regions,\n",
    "       mexican_title_keywords, \n",
    "       mexican_des_keywords, \n",
    "       tortilla_corn, \n",
    "       salsa_food, \n",
    "       chip, \n",
    "       canned_food]\n",
    "\n",
    "# Using SparkLFApplier\n",
    "applier = SparkLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df_tr_rdd)\n",
    "L_val = applier.apply(df_v_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>indian_title_keywords</th>\n",
       "      <td>0</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.149441</td>\n",
       "      <td>0.110962</td>\n",
       "      <td>0.067562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indian_des_keywords</th>\n",
       "      <td>1</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.228635</td>\n",
       "      <td>0.157047</td>\n",
       "      <td>0.109620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>currymeat</th>\n",
       "      <td>2</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>0.000895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cook_food</th>\n",
       "      <td>3</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.033557</td>\n",
       "      <td>0.030425</td>\n",
       "      <td>0.024609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sweet_spicy</th>\n",
       "      <td>4</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.006264</td>\n",
       "      <td>0.006264</td>\n",
       "      <td>0.006264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slow_cook</th>\n",
       "      <td>5</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.021924</td>\n",
       "      <td>0.017897</td>\n",
       "      <td>0.013423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>italian_title_keywords</th>\n",
       "      <td>6</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.187472</td>\n",
       "      <td>0.166443</td>\n",
       "      <td>0.074720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>italian_des_keywords</th>\n",
       "      <td>7</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.230872</td>\n",
       "      <td>0.199553</td>\n",
       "      <td>0.114989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pasta_with</th>\n",
       "      <td>8</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.040268</td>\n",
       "      <td>0.040268</td>\n",
       "      <td>0.017897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sundried_tomatoes</th>\n",
       "      <td>9</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.004474</td>\n",
       "      <td>0.003579</td>\n",
       "      <td>0.000447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ita_regions</th>\n",
       "      <td>10</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.227740</td>\n",
       "      <td>0.212081</td>\n",
       "      <td>0.103356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mexican_title_keywords</th>\n",
       "      <td>11</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.255034</td>\n",
       "      <td>0.207159</td>\n",
       "      <td>0.099329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mexican_des_keywords</th>\n",
       "      <td>12</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.342729</td>\n",
       "      <td>0.289933</td>\n",
       "      <td>0.170917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tortilla_corn</th>\n",
       "      <td>13</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.035794</td>\n",
       "      <td>0.035794</td>\n",
       "      <td>0.010291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salsa_food</th>\n",
       "      <td>14</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.012528</td>\n",
       "      <td>0.012528</td>\n",
       "      <td>0.006264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chip</th>\n",
       "      <td>15</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.026846</td>\n",
       "      <td>0.026846</td>\n",
       "      <td>0.007159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>canned_food</th>\n",
       "      <td>16</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.004027</td>\n",
       "      <td>0.004027</td>\n",
       "      <td>0.003579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         j Polarity  Coverage  Overlaps  Conflicts\n",
       "indian_title_keywords    0      [0]  0.149441  0.110962   0.067562\n",
       "indian_des_keywords      1      [0]  0.228635  0.157047   0.109620\n",
       "currymeat                2      [0]  0.000895  0.000895   0.000895\n",
       "cook_food                3      [0]  0.033557  0.030425   0.024609\n",
       "sweet_spicy              4      [0]  0.006264  0.006264   0.006264\n",
       "slow_cook                5      [0]  0.021924  0.017897   0.013423\n",
       "italian_title_keywords   6      [1]  0.187472  0.166443   0.074720\n",
       "italian_des_keywords     7      [1]  0.230872  0.199553   0.114989\n",
       "pasta_with               8      [1]  0.040268  0.040268   0.017897\n",
       "sundried_tomatoes        9      [1]  0.004474  0.003579   0.000447\n",
       "ita_regions             10      [1]  0.227740  0.212081   0.103356\n",
       "mexican_title_keywords  11      [2]  0.255034  0.207159   0.099329\n",
       "mexican_des_keywords    12      [2]  0.342729  0.289933   0.170917\n",
       "tortilla_corn           13      [2]  0.035794  0.035794   0.010291\n",
       "salsa_food              14      [2]  0.012528  0.012528   0.006264\n",
       "chip                    15      [2]  0.026846  0.026846   0.007159\n",
       "canned_food             16      [2]  0.004027  0.004027   0.003579"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking Statistics of Labelling Functions\n",
    "from snorkel.labeling import LFAnalysis\n",
    "\n",
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Incorrect</th>\n",
       "      <th>Emp. Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>indian_title_keywords</th>\n",
       "      <td>0</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.178667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.090667</td>\n",
       "      <td>61</td>\n",
       "      <td>6</td>\n",
       "      <td>0.910448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indian_des_keywords</th>\n",
       "      <td>1</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.197333</td>\n",
       "      <td>0.144000</td>\n",
       "      <td>74</td>\n",
       "      <td>26</td>\n",
       "      <td>0.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>currymeat</th>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cook_food</th>\n",
       "      <td>3</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.010667</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sweet_spicy</th>\n",
       "      <td>4</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slow_cook</th>\n",
       "      <td>5</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.029333</td>\n",
       "      <td>0.021333</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>italian_title_keywords</th>\n",
       "      <td>6</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.141333</td>\n",
       "      <td>0.128000</td>\n",
       "      <td>0.053333</td>\n",
       "      <td>45</td>\n",
       "      <td>8</td>\n",
       "      <td>0.849057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>italian_des_keywords</th>\n",
       "      <td>7</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.221333</td>\n",
       "      <td>0.189333</td>\n",
       "      <td>0.125333</td>\n",
       "      <td>58</td>\n",
       "      <td>25</td>\n",
       "      <td>0.698795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pasta_with</th>\n",
       "      <td>8</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.034667</td>\n",
       "      <td>0.034667</td>\n",
       "      <td>0.021333</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sundried_tomatoes</th>\n",
       "      <td>9</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>0.002667</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ita_regions</th>\n",
       "      <td>10</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.202667</td>\n",
       "      <td>0.194667</td>\n",
       "      <td>0.109333</td>\n",
       "      <td>56</td>\n",
       "      <td>20</td>\n",
       "      <td>0.736842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mexican_title_keywords</th>\n",
       "      <td>11</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.269333</td>\n",
       "      <td>0.210667</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>66</td>\n",
       "      <td>35</td>\n",
       "      <td>0.653465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mexican_des_keywords</th>\n",
       "      <td>12</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.352000</td>\n",
       "      <td>0.296000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>61</td>\n",
       "      <td>71</td>\n",
       "      <td>0.462121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tortilla_corn</th>\n",
       "      <td>13</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salsa_food</th>\n",
       "      <td>14</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.010667</td>\n",
       "      <td>0.010667</td>\n",
       "      <td>0.002667</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chip</th>\n",
       "      <td>15</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>canned_food</th>\n",
       "      <td>16</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         j Polarity  Coverage  Overlaps  Conflicts  Correct  \\\n",
       "indian_title_keywords    0      [0]  0.178667  0.133333   0.090667       61   \n",
       "indian_des_keywords      1      [0]  0.266667  0.197333   0.144000       74   \n",
       "currymeat                2       []  0.000000  0.000000   0.000000        0   \n",
       "cook_food                3      [0]  0.016000  0.013333   0.010667        3   \n",
       "sweet_spicy              4      [0]  0.005333  0.005333   0.005333        2   \n",
       "slow_cook                5      [0]  0.040000  0.029333   0.021333        7   \n",
       "italian_title_keywords   6      [1]  0.141333  0.128000   0.053333       45   \n",
       "italian_des_keywords     7      [1]  0.221333  0.189333   0.125333       58   \n",
       "pasta_with               8      [1]  0.034667  0.034667   0.021333       12   \n",
       "sundried_tomatoes        9      [1]  0.005333  0.005333   0.002667        2   \n",
       "ita_regions             10      [1]  0.202667  0.194667   0.109333       56   \n",
       "mexican_title_keywords  11      [2]  0.269333  0.210667   0.120000       66   \n",
       "mexican_des_keywords    12      [2]  0.352000  0.296000   0.200000       61   \n",
       "tortilla_corn           13      [2]  0.016000  0.016000   0.008000        6   \n",
       "salsa_food              14      [2]  0.010667  0.010667   0.002667        4   \n",
       "chip                    15      [2]  0.016000  0.016000   0.008000        4   \n",
       "canned_food             16      [2]  0.008000  0.008000   0.008000        1   \n",
       "\n",
       "                        Incorrect  Emp. Acc.  \n",
       "indian_title_keywords           6   0.910448  \n",
       "indian_des_keywords            26   0.740000  \n",
       "currymeat                       0   0.000000  \n",
       "cook_food                       3   0.500000  \n",
       "sweet_spicy                     0   1.000000  \n",
       "slow_cook                       8   0.466667  \n",
       "italian_title_keywords          8   0.849057  \n",
       "italian_des_keywords           25   0.698795  \n",
       "pasta_with                      1   0.923077  \n",
       "sundried_tomatoes               0   1.000000  \n",
       "ita_regions                    20   0.736842  \n",
       "mexican_title_keywords         35   0.653465  \n",
       "mexican_des_keywords           71   0.462121  \n",
       "tortilla_corn                   0   1.000000  \n",
       "salsa_food                      0   1.000000  \n",
       "chip                            2   0.666667  \n",
       "canned_food                     2   0.333333  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting summary on VALIDATION set\n",
    "\n",
    "LFAnalysis(L_val, lfs=lfs).lf_summary(Y_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment regarding the performance of LFs**\n",
    "\n",
    "From the tables above it can be seen that the LFs vary in their coverage, which describes what proportion of the dataset a label covers. \n",
    "\n",
    "Indeed, the label with highest coverage is 'mexican_des_keywords' (34.27%) while the lowest are 'currymeat', 'sweet_spicy', 'sundried_tomatoes', 'canned_food' (all nearly 0). It can be argued that this disproportion ranges due to the specificity of the latter LFs. Secondly, 12 out of 14 LFs have conflicts and overlaps. This should be also visible which will follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bucket or something here, some explanations for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our labeling functions vary in coverage, accuracy, and how much they overlap/conflict with one another. We can view a histogram of how many LF labels the data points in our dev set have to get an idea of our total coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_label_frequency(L):\n",
    "    plt.hist((L != ABSTAIN).sum(axis=1), density=True, bins=range(L.shape[1]))\n",
    "    plt.xlabel(\"Number of labels\")\n",
    "    plt.ylabel(\"Fraction of dataset\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_label_frequency(L_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that around half of our train dataset data points have 2 or fewer labels from LFs. Fortunately, the signal we do have can be used to train a classifier over the comment text directly, allowing it to generalize beyond what we've specified via our LFs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compared against Y_test - actual values of L\n",
    "L_test = applier.apply(df_ts.rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we use LabelModel presented by spark which (as documentation) claims to produce probablity-aware labels to train the further classification model. However, comparison between Label Model and Majority Vote is also provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy:   70.5%\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling import MajorityLabelVoter\n",
    "\n",
    "majority_model = MajorityLabelVoter(cardinality = 3)\n",
    "preds_train = majority_model.predict(L = L_train)\n",
    "\n",
    "majority_acc = majority_model.score(L = L_test, Y = Y_test, tie_break_policy = \"random\")[\"accuracy\"]\n",
    "print(f\"{'Majority Vote Accuracy:':<25} {majority_acc * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Model Accuracy:     70.6%\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling import LabelModel\n",
    "\n",
    "label_model = LabelModel(cardinality = 3, verbose = True)\n",
    "label_model.fit(L_train, n_epochs = 100, log_freq = 100, seed = 11)\n",
    "\n",
    "label_model_acc = label_model.score(L = L_test, Y = Y_test, tie_break_policy = \"random\")[\"accuracy\"]\n",
    "print(f\"{'Label Model Accuracy:':<25} {label_model_acc * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above Label Model performs better in labelling test split. (NOTE from Max - n_epochs affect Label model). Cardinality = number of labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a classifier with MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLlib supports two linear methods for classification: linear Support Vector Machines (SVMs) and logistic regression. However, Linear SVMs supports only binary classification. Thus, we will use logistic regression which is also suitable for multiclass classification problems like ours.\n",
    "\n",
    "\n",
    "Specifically, we are dealing with a multiclass classification as our number of labels for each data point is larget than 2. We note that predictions and labels are under the context of a particular class. Thus, to quote the documentation, \"a true positive occurs whenever the prediction and the label match, while a true negative occurs when neither the prediction nor the label take on the value of a given class\". This is the reason why ultimately it is possible that our results will yield multiple true negatives for our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import models \n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load training data in LIBSVM format\n",
    "\n",
    "#data = MLUtils.loadLibSVMFile(sc, \"data/mllib/sample_multiclass_classification_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training (60%) and test (40%)\n",
    "#training, test = data.randomSplit([0.6, 0.4], seed=11)\n",
    "#training.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'first'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-f4a298fe0d54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# comment from dominik: was \"training\" before, now L_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegressionWithLBFGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumClasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda/envs/Python3/lib/python3.6/site-packages/pyspark/mllib/classification.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cls, data, iterations, initialWeights, regParam, regType, intercept, corrections, tolerance, validateData, numClasses)\u001b[0m\n\u001b[1;32m    399\u001b[0m                     \u001b[0minitialWeights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnumClasses\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                     \u001b[0minitialWeights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnumClasses\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_regression_train_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLogisticRegressionModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitialWeights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'first'"
     ]
    }
   ],
   "source": [
    "# Run training algorithm to build the model\n",
    "\n",
    "# comment from dominik: was \"training\" before, now L_train\n",
    "model = LogisticRegressionWithLBFGS.train(L_train, numClasses = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute raw scores on the test set\n",
    "predictionAndLabels = test.map(lambda lp: (float(model.predict(lp.features)), lp.label))\n",
    "\n",
    "# Instantiate metrics object\n",
    "metrics = MulticlassMetrics(predictionAndLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall statistics\n",
    "precision = metrics.precision()\n",
    "recall = metrics.recall()\n",
    "f1Score = metrics.fMeasure()\n",
    "print(\"Summary Stats\")\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1 Score = %s\" % f1Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics by class\n",
    "labels = data.map(lambda lp: lp.label).distinct().collect()\n",
    "for label in sorted(labels):\n",
    "    print(\"Class %s precision = %s\" % (label, metrics.precision(label)))\n",
    "    print(\"Class %s recall = %s\" % (label, metrics.recall(label)))\n",
    "    print(\"Class %s F1 Measure = %s\" % (label, metrics.fMeasure(label, beta=1.0)))\n",
    "\n",
    "# Weighted stats\n",
    "print(\"Weighted recall = %s\" % metrics.weightedRecall)\n",
    "print(\"Weighted precision = %s\" % metrics.weightedPrecision)\n",
    "print(\"Weighted F(1) Score = %s\" % metrics.weightedFMeasure())\n",
    "print(\"Weighted F(0.5) Score = %s\" % metrics.weightedFMeasure(beta=0.5))\n",
    "print(\"Weighted false positive rate = %s\" % metrics.weightedFalsePositiveRate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
