{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipe Labelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up for the work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WL0ih9fRhPEh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting snorkel\n",
      "  Using cached snorkel-0.9.3-py3-none-any.whl (139 kB)\n",
      "Collecting tqdm<5.0.0,>=4.33.0\n",
      "  Using cached tqdm-4.43.0-py2.py3-none-any.whl (59 kB)\n",
      "Collecting torch<1.2.0,>=1.1.0\n",
      "  Using cached torch-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (676.9 MB)\n",
      "Collecting pandas<0.26.0,>=0.25.0\n",
      "  Using cached pandas-0.25.3-cp36-cp36m-manylinux1_x86_64.whl (10.4 MB)\n",
      "Collecting munkres==1.1.2\n",
      "  Using cached munkres-1.1.2-py2.py3-none-any.whl (6.8 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.16.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from snorkel) (1.18.1)\n",
      "Processing /home/faculty/.cache/pip/wheels/81/dc/bb/fbde77ddcbf8d5a04787faf6cc9f1edf4c70a67961d7c75abf/networkx-2.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.2.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from snorkel) (1.4.1)\n",
      "Collecting scikit-learn<0.22.0,>=0.20.2\n",
      "  Using cached scikit_learn-0.21.3-cp36-cp36m-manylinux1_x86_64.whl (6.7 MB)\n",
      "Collecting tensorboardX<2.0,>=1.6\n",
      "  Using cached tensorboardX-1.9-py2.py3-none-any.whl (190 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from pandas<0.26.0,>=0.25.0->snorkel) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from pandas<0.26.0,>=0.25.0->snorkel) (2019.3)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from networkx<2.4,>=2.2->snorkel) (4.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from scikit-learn<0.22.0,>=0.20.2->snorkel) (0.14.1)\n",
      "Requirement already satisfied: six in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from tensorboardX<2.0,>=1.6->snorkel) (1.14.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from tensorboardX<2.0,>=1.6->snorkel) (3.11.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorboardX<2.0,>=1.6->snorkel) (45.2.0.post20200210)\n",
      "\u001b[31mERROR: faculty-models 0.1.1 has requirement mlflow-faculty>=0.4.3, but you'll have mlflow-faculty 0.4.2 which is incompatible.\u001b[0m\n",
      "Installing collected packages: tqdm, torch, pandas, munkres, networkx, scikit-learn, tensorboardX, snorkel\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.0.1\n",
      "    Uninstalling pandas-1.0.1:\n",
      "      Successfully uninstalled pandas-1.0.1\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 2.4\n",
      "    Uninstalling networkx-2.4:\n",
      "      Successfully uninstalled networkx-2.4\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.22.1\n",
      "    Uninstalling scikit-learn-0.22.1:\n",
      "      Successfully uninstalled scikit-learn-0.22.1\n",
      "Successfully installed munkres-1.1.2 networkx-2.3 pandas-0.25.3 scikit-learn-0.21.3 snorkel-0.9.3 tensorboardX-1.9 torch-1.1.0 tqdm-4.43.0\n"
     ]
    }
   ],
   "source": [
    "! pip install snorkel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RzMzZ5c8hPEn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-2.2.3-cp36-cp36m-manylinux1_x86_64.whl (10.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.4 MB 39.7 MB/s eta 0:00:01    |█████████████                   | 4.2 MB 39.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wasabi<1.1.0,>=0.4.0\n",
      "  Downloading wasabi-0.6.0-py3-none-any.whl (20 kB)\n",
      "Collecting blis<0.5.0,>=0.4.0\n",
      "  Downloading blis-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (3.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7 MB 42.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting catalogue<1.1.0,>=0.0.7\n",
      "  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from spacy) (2.22.0)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.2-cp36-cp36m-manylinux1_x86_64.whl (19 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.3-cp36-cp36m-manylinux1_x86_64.whl (32 kB)\n",
      "Collecting thinc<7.4.0,>=7.3.0\n",
      "  Downloading thinc-7.3.1-cp36-cp36m-manylinux1_x86_64.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 83.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from spacy) (1.18.1)\n",
      "Collecting srsly<1.1.0,>=0.1.0\n",
      "  Downloading srsly-1.0.2-cp36-cp36m-manylinux1_x86_64.whl (185 kB)\n",
      "\u001b[K     |████████████████████████████████| 185 kB 93.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.2-cp36-cp36m-manylinux1_x86_64.whl (119 kB)\n",
      "\u001b[K     |████████████████████████████████| 119 kB 95.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting plac<1.2.0,>=0.9.6\n",
      "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from spacy) (45.2.0.post20200210)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from thinc<7.4.0,>=7.3.0->spacy) (4.43.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (2.2.0)\n",
      "Installing collected packages: wasabi, blis, catalogue, murmurhash, cymem, preshed, srsly, plac, thinc, spacy\n",
      "Successfully installed blis-0.4.1 catalogue-1.0.0 cymem-2.0.3 murmurhash-1.0.2 plac-1.1.3 preshed-3.0.2 spacy-2.2.3 srsly-1.0.2 thinc-7.3.1 wasabi-0.6.0\n"
     ]
    }
   ],
   "source": [
    "! pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "PcrYo7xyhPEp",
    "outputId": "a176c5e6-0d61-4668-826a-c9e011f13ddc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.2.5\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.0 MB 727 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from en_core_web_sm==2.2.5) (2.2.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (45.2.0.post20200210)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.22.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.3.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.5.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.25.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.43.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.2.0)\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.2.5-py3-none-any.whl size=12011738 sha256=f37dbee787d66b4d4e0bc953ef5f1a30477462791ed258fd6017f7fa8bf7b7d7\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-7sl889qo/wheels/b5/94/56/596daa677d7e91038cbddfcf32b591d0c915a1b3a3e3d3c79d\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-2.2.5\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "tdbski8thPFQ",
    "outputId": "83732d81-a32c-4f55-d41c-6d54f8765fcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (3.4.5)\n",
      "Requirement already satisfied: six in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from nltk) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "Byvg_0k7hPFU",
    "outputId": "265e81ea-f562-4559-cf93-488213f3540b"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9d0626585e70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'punkt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wordnet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stopwords'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fdZMV8XFyXfL"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s8NMX7sJBFWa"
   },
   "outputs": [],
   "source": [
    "# Display full output rather than just the last line of output\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wVHdxlF4hPFH"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "PATH = 'https://raw.githubusercontent.com/uceikow/DataEngineeringGroupAO/master/Recipe_dataset/'\n",
    "\n",
    "indian = pd.read_json(PATH + \"data_indian.json\")\n",
    "indian['label'] = 'indian'\n",
    "italian = pd.read_json(PATH + \"data_italian.json\")\n",
    "italian['label'] = 'italian'\n",
    "mexican = pd.read_json(PATH + \"data_mexican.json\")\n",
    "mexican['label'] = 'mexican'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of each dataset\n",
    "len(indian)\n",
    "len(mexican)\n",
    "len(italian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "colab_type": "code",
    "id": "iYDYOIHqqi3z",
    "outputId": "d20da84c-cb5f-48e9-a199-384b58180452"
   },
   "outputs": [],
   "source": [
    "# Concat them into one dataset\n",
    "recipe = pd.concat([indian, italian, mexican],ignore_index = True)\n",
    "recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5B5GRAyfhPFS"
   },
   "source": [
    "## Clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BsgxdYe9p-TS"
   },
   "outputs": [],
   "source": [
    "# Clean the dataset\n",
    "# Lowercase\n",
    "recipe = recipe.apply(lambda row: row.str.lower())\n",
    "\n",
    "# Remove digits\n",
    "recipe['Title'] = recipe.apply((lambda row: ''.join([i for i in row['Title'] if not i.isdigit()])),axis = 1)\n",
    "recipe['Description'] = recipe.apply((lambda row: ''.join([i for i in row['Description'] if not i.isdigit()])),axis = 1)\n",
    "\n",
    "# Remove punctuations\n",
    "recipe['Title'] = recipe.apply((lambda row: ''.join([i for i in row['Title'] if i not in string.punctuation])),axis=1)\n",
    "recipe['Description'] = recipe.apply((lambda row: ''.join([i for i in row['Description'] if i not in string.punctuation])),axis=1)\n",
    "\n",
    "# Remove Stopwords\n",
    "stop = stopwords.words('english')\n",
    "recipe['Title'] = recipe['Title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "recipe['Description'] = recipe['Description'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "rPeGEU8Z5dIZ",
    "outputId": "160cab6b-5b0c-4022-a98e-74d282eca748"
   },
   "outputs": [],
   "source": [
    "recipe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hOFD7g02CwKl"
   },
   "source": [
    "## Pattern Exploration\n",
    "\n",
    "Before splitting the dataset and writing labelling function,  we might want to first get an idea of how our targetting labels look like. This gives us some basic information of how to start building the labelling function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 701
    },
    "colab_type": "code",
    "id": "BgzIrjKY9S9x",
    "outputId": "53f64bd4-685d-4e7a-fb91-775fb02ef590"
   },
   "outputs": [],
   "source": [
    "# Patterns from different recipes\n",
    "\n",
    "# Filter out different recipes\n",
    "recipe_ind = recipe[recipe['label'] == 'indian']\n",
    "recipe_ita = recipe[recipe['label'] == 'italian']\n",
    "recipe_mex = recipe[recipe['label'] == 'mexican']\n",
    "\n",
    "# Word frequency in 'Title'\n",
    "top_N = 15\n",
    "\n",
    "title1 = recipe_ind.Title.str.cat(sep=' ')\n",
    "words_in_title1 = nltk.tokenize.word_tokenize(title1)\n",
    "word_dist_title1 = nltk.FreqDist(words_in_title1)\n",
    "\n",
    "title2 = recipe_ita.Title.str.cat(sep=' ')\n",
    "words_in_title2 = nltk.tokenize.word_tokenize(title2)\n",
    "word_dist_title2 = nltk.FreqDist(words_in_title2)\n",
    "\n",
    "title3 = recipe_mex.Title.str.cat(sep=' ')\n",
    "words_in_title3 = nltk.tokenize.word_tokenize(title3)\n",
    "word_dist_title3 = nltk.FreqDist(words_in_title3)\n",
    "\n",
    "ind_freq = pd.DataFrame(word_dist_title1.most_common(top_N),\n",
    "                    columns=['Indian', 'Frequency'])\n",
    "ita_freq = pd.DataFrame(word_dist_title2.most_common(top_N),\n",
    "                    columns=['Italian', 'Frequency'])\n",
    "mex_freq = pd.DataFrame(word_dist_title3.most_common(top_N),\n",
    "                    columns=['Mexican', 'Frequency'])\n",
    "\n",
    "title_freq = pd.concat([ind_freq,ita_freq,mex_freq],axis = 1)\n",
    "title_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easier to find patterns and differences of recipes if we display titles of three recipes together. The same goes for description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word frequency in 'Description'\n",
    "top_N =20\n",
    "\n",
    "des1 = recipe_ind.Description.str.cat(sep=' ')\n",
    "words_in_des1 = nltk.tokenize.word_tokenize(des1)\n",
    "word_dist_des1 = nltk.FreqDist(words_in_des1)\n",
    "\n",
    "des2 = recipe_ita.Description.str.cat(sep=' ')\n",
    "words_in_des2 = nltk.tokenize.word_tokenize(des2)\n",
    "word_dist_des2 = nltk.FreqDist(words_in_des2)\n",
    "\n",
    "des3 = recipe_mex.Description.str.cat(sep=' ')\n",
    "words_in_des3 = nltk.tokenize.word_tokenize(des3)\n",
    "word_dist_des3 = nltk.FreqDist(words_in_des3)\n",
    "\n",
    "d1_freq = pd.DataFrame(word_dist_des1.most_common(top_N),\n",
    "                    columns=['Ind_description', 'Frequency'])\n",
    "d2_freq = pd.DataFrame(word_dist_des2.most_common(top_N),\n",
    "                    columns=['Ita_description', 'Frequency'])\n",
    "d3_freq = pd.DataFrame(word_dist_des3.most_common(top_N),\n",
    "                    columns=['Mex_description', 'Frequency'])\n",
    "\n",
    "des_freq = pd.concat([d1_freq,d2_freq,d3_freq],axis=1)\n",
    "des_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point, we see that there are something more to do with data cleaning. \n",
    "\n",
    "- Get rid of those \" ...\"\n",
    "- Are Verbs, Conjuctions and prepositions important in our case, if not, we can clear them, only leaving Nouns in our dataset.\n",
    "- Stemming words (*curry* and *curried* should be of the same thing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Re-clean the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is scraped from an online recipe website. We only captured part of the description, and some of them contains \"…\" if the sentence does not ended when capped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove \"...\" from description\n",
    "recipe['Description'] = recipe.Description.apply(lambda x: re.sub(r'…','',x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part-pf-speech is used to remove verbs, prepositions, conjunctions from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove verb, prepositions, conjunctions from the text\n",
    "\n",
    "# Part of speech\n",
    "recipe['tokens'] = recipe.Description.apply(lambda x: nltk.word_tokenize(x))\n",
    "recipe['tagged'] = recipe.tokens.apply(lambda x: nltk.pos_tag(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define remove function\n",
    "def re_pos(tuple_list):\n",
    "    for i in tuple_list:\n",
    "        if i[1] in ['VB','VBD','VBG','VBN','VBP','VBZ','CC','IN']:\n",
    "            tuple_list.remove(i)\n",
    "    return tuple_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unwanted words and change tokens\n",
    "recipe['tagged'] = recipe.tagged.apply(lambda x: re_pos(x))\n",
    "recipe['tokens'] = recipe.tagged.apply(lambda x: [i[0] for i in x])\n",
    "recipe['Description'] = recipe.tokens.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After re-cleaning, let's see the change of patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patterns after re-cleaning\n",
    "recipe_ind = recipe[recipe['label'] == 'indian']\n",
    "recipe_ita = recipe[recipe['label'] == 'italian']\n",
    "recipe_mex = recipe[recipe['label'] == 'mexican']\n",
    "\n",
    "top_N =20\n",
    "\n",
    "des1 = recipe_ind.Description.str.cat(sep=' ')\n",
    "words_in_des1 = nltk.tokenize.word_tokenize(des1)\n",
    "word_dist_des1 = nltk.FreqDist(words_in_des1)\n",
    "\n",
    "des2 = recipe_ita.Description.str.cat(sep=' ')\n",
    "words_in_des2 = nltk.tokenize.word_tokenize(des2)\n",
    "word_dist_des2 = nltk.FreqDist(words_in_des2)\n",
    "\n",
    "des3 = recipe_mex.Description.str.cat(sep=' ')\n",
    "words_in_des3 = nltk.tokenize.word_tokenize(des3)\n",
    "word_dist_des3 = nltk.FreqDist(words_in_des3)\n",
    "\n",
    "d1_freq = pd.DataFrame(word_dist_des1.most_common(top_N),\n",
    "                    columns=['Ind_description', 'Frequency'])\n",
    "d2_freq = pd.DataFrame(word_dist_des2.most_common(top_N),\n",
    "                    columns=['Ita_description', 'Frequency'])\n",
    "d3_freq = pd.DataFrame(word_dist_des3.most_common(top_N),\n",
    "                    columns=['Mex_description', 'Frequency'])\n",
    "\n",
    "des_freq = pd.concat([d1_freq,d2_freq,d3_freq],axis=1)\n",
    "des_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "As you might have noticed, indian food and mexican food share some similarities, such as spicy-related words, sauce-related words, rice, etc. This is somewhere that we need to keep an eye on.\n",
    "</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5EEimMn1BXzs"
   },
   "source": [
    "**Ideas of building Labelling functions:**\n",
    "\n",
    "- Single word (specific ones), such as curry, masala, paneer and chutney for indian repice. These words can label one type of recipe quite well because of their specialty (they will not appear in other recipes). Except for words included in top 15 frequency list, they must be other special words, which might need go through the whole dataset to find.\n",
    "\n",
    "- Word combos, such as **curry + chicken = indian** :) You might find that chicken are used a lot in both indian and mexican recipe, while a way to label them might be find a **word combos** (function = special word + main ingrediant).\n",
    "\n",
    "- Unique wordlist difference! Check if any of word are unique for that kind of recipe but not in other recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the orginal dataset for further pattern exploration\n",
    "recipe_ex = recipe.copy()\n",
    "recipe = recipe.drop(columns = ['tokens','tagged'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nXbdMtOGEFPi"
   },
   "source": [
    "## Split the dataset\n",
    "\n",
    "As being discussed in group meeting, we split the dataset into training, validation, development and test datasets.\n",
    "\n",
    "If we do multi-labelling, we need to make sure that all datasets above contains same proportion of the 3 recipes. I decided to have 30% labelled data, in which 10% for dev set, 10% for validation set, and the remaining 10% for test set. We left 70% data to training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Agcde9nWCooH"
   },
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "# Use ShuffleStratifiedSplit to ensure same proportion of each dataset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=3, test_size=0.3, random_state=0)\n",
    "\n",
    "# Get different labelled data\n",
    "ind = recipe[recipe['label'] == 'indian']\n",
    "ind.reset_index(drop=True,inplace=True)\n",
    "ita = recipe[recipe['label'] == 'italian']\n",
    "ita.reset_index(drop=True,inplace=True)\n",
    "mex = recipe[recipe['label'] == 'mexican']\n",
    "mex.reset_index(drop=True,inplace=True)\n",
    "\n",
    "# Split function (leave 70% for training)\n",
    "def shuffle_split(df,sss):\n",
    "  X = df[['Title','Description']]\n",
    "  y = df['label']\n",
    "  for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "ind_X_train, ind_X_test, ind_y_train, ind_y_test = shuffle_split(ind,sss)\n",
    "ita_X_train, ita_X_test, ita_y_train, ita_y_test = shuffle_split(ita,sss)\n",
    "mex_X_train, mex_X_test, mex_y_train, mex_y_test = shuffle_split(mex,sss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('indian: ',len(ind_X_train),len(ind_X_test))\n",
    "print('italian: ',len(ita_X_train),len(ita_X_test))\n",
    "print('mexican: ',len(mex_X_train),len(mex_X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-OxgR17ORUlH"
   },
   "outputs": [],
   "source": [
    "# Combine training and test dataset\n",
    "X_train = pd.concat([ind_X_train,ita_X_train,mex_X_train],axis=0)\n",
    "y_train = pd.concat([ind_y_train,ita_y_train,mex_y_train],axis=0)\n",
    "X_test =  pd.concat([ind_X_test,ita_X_test,mex_X_test],axis=0)\n",
    "y_test =  pd.concat([ind_y_test,ita_y_test,mex_y_test],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine training dataset\n",
    "train = pd.concat([X_train,y_train],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the test dataset for next splitting\n",
    "test = pd.concat([X_test,y_test],axis=1)\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split development and validation dataset from test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From randomly sampled test set get dev set and validation set.\n",
    "\n",
    "ind_val, ind_dev = test[:48], test[48:96]\n",
    "ita_val, ita_dev = test[144:184], test[184:224]\n",
    "mex_val, mex_dev = test[264:326], test[326:388]\n",
    "ind_test, ita_test, mex_test = test[96:144],test[224:264],test[388:450]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine val, dev and test set\n",
    "\n",
    "val = pd.concat([ind_val,ita_val,mex_val],axis=0)\n",
    "dev = pd.concat([ind_dev,ita_dev,mex_dev],axis=0)\n",
    "test_n = pd.concat([ind_test,ita_test,mex_test],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we split the dataset by different countries, we need to shuffle them before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "train = shuffle(train, random_state = 42)\n",
    "test = shuffle(test_n, random_state = 42)\n",
    "val = shuffle(val, random_state = 42)\n",
    "dev = shuffle(dev, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply LFAnalysis, we need to change labels to number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change labels to number\n",
    "def label_to_num(df):\n",
    "    df.label = df.label.apply(lambda x: 0 if x == 'indian' else(1 if x == 'italian' else 2))\n",
    "    return df\n",
    "    \n",
    "test = label_to_num(test)\n",
    "val = label_to_num(val)\n",
    "dev = label_to_num(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for later training\n",
    "df_train = train.iloc[:,:2]\n",
    "df_val = val.iloc[:,:2]\n",
    "df_dev = dev.iloc[:,:2]\n",
    "Y_val = val.iloc[:,-1].values\n",
    "Y_dev = dev.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labelling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import labeling_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For clarity, we define constants to represent the class labels and abstaining.\n",
    "ABSTAIN = -1\n",
    "INDIAN = 0\n",
    "ITALIAN = 1\n",
    "MEXICAN = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_keywords = ['curry','indian','masala','paneer','chutney','curried',\n",
    "                'simmered','cumin','yogurt','coconut']\n",
    "\n",
    "@labeling_function()\n",
    "def indian_keywords(x):\n",
    "        if any(word in x.Title for word in ind_keywords):\n",
    "            return INDIAN\n",
    "        else:\n",
    "            return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique word checking\n",
    "import numpy as np\n",
    "\n",
    "ind_values = np.unique(words_in_des1)\n",
    "ita_values = np.unique(words_in_des2)\n",
    "mex_values = np.unique(words_in_des3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check words in indian but not in mexican and italian\n",
    "word_diff_ind_mex = np.setdiff1d(ind_values,mex_values)\n",
    "word_diff_ind_ita = np.setdiff1d(ind_values,ita_values)\n",
    "word_for_ind = np.intersect1d(word_diff_ind_mex,word_diff_ind_ita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def ind_unique_words(x):\n",
    "        if any(word in x.Description for word in word_for_ind):\n",
    "            return INDIAN\n",
    "        else:\n",
    "            return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Combo curry + meat\n",
    "@labeling_function()\n",
    "def currymeat(x):\n",
    "    return INDIAN if re.search(r\"(?=.*curry)(?=.*(chicken|lamb|beef))\", x.Description, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some words in indian recipe, such as *rice, sauce, etc* might comflict with mexican recipe, however, as country_name + rice / sauce can be a good seperator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# County name + food name\n",
    "@labeling_function()\n",
    "def ind_food(x):\n",
    "    return INDIAN if re.search(r\"(?=.*indian)(?=.*(rice|sauce|potatoes))\", x.Description, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import PandasLFApplier\n",
    "\n",
    "lfs = [indian_keywords, currymeat, ind_food, ind_unique_words]\n",
    "\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)\n",
    "L_dev = applier.apply(df=df_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import LFAnalysis\n",
    "\n",
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFAnalysis(L=L_dev, lfs=lfs).lf_summary(Y=Y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one misclassified case in the first LF and nearly half misclassified in the third LF. Let's check them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.analysis import get_label_buckets\n",
    "\n",
    "buckets1 = get_label_buckets(Y_dev, L_dev[:, 0])\n",
    "df_dev.iloc[buckets1[(MEXICAN, INDIAN)]]\n",
    "\n",
    "buckets2 = get_label_buckets(Y_dev, L_dev[:, 3])\n",
    "df_dev.iloc[buckets2[(MEXICAN, INDIAN)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It mixed with mexican recipe."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "snorkel_lf.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
