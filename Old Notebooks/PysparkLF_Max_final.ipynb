{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some pictures and our coursework title here! Anyone has some ideas plz update here :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "import pyspark\n",
    "\n",
    "number_cores = int(os.environ['NUM_CPUS'])\n",
    "memory_gb = int(os.environ['AVAILABLE_MEMORY_MB']) // 1024\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setMaster('local[{}]'.format(number_cores))\n",
    "        .set('spark.driver.memory', '{}g'.format(memory_gb))\n",
    ")\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[4] appName=pyspark-shell>\n"
     ]
    }
   ],
   "source": [
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "indian = sqlContext.read.csv(\"/project/Project/DataEngineeringGroupAO/Recipe_dataset/data_indian.csv\",header=True)\n",
    "italian = sqlContext.read.csv(\"/project/Project/DataEngineeringGroupAO/Recipe_dataset/data_italian.csv\",header=True)\n",
    "mexican = sqlContext.read.csv(\"/project/Project/DataEngineeringGroupAO/Recipe_dataset/data_mexican.csv\",header=True)\n",
    "new_recipe = sqlContext.read.csv(\"/project/Project/DataEngineeringGroupAO/Recipe_dataset/final_scrape_not_cleaned.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indian:  480  italian:  400  mexican:  620  new_recipe:  2340\n"
     ]
    }
   ],
   "source": [
    "# See the size of the dataset\n",
    "print('indian: ', indian.count(),' italian: ', italian.count(), ' mexican: ', mexican.count(),\n",
    "     ' new_recipe: ', new_recipe.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first three datasets, we do not have labels for them. We could use them as training set but three of them combined together is not enough. In this case, we decided to combine them together first, and then do splitting, and then remove the labels of training set. So, we have to first label the unlabelled dataset manually, which is shown as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label the data\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "indian = indian.withColumn(\"label\",lit(\"indian\"))\n",
    "italian = italian.withColumn(\"label\",lit(\"italian\"))\n",
    "mexican = mexican.withColumn(\"label\",lit(\"mexican\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+\n",
      "|               Title|         Description| label|\n",
      "+--------------------+--------------------+------+\n",
      "|  Indian Peanut Stew|This is an easy, ...|indian|\n",
      "|        Roomali Roti|There is no leave...|indian|\n",
      "|Spicy Sweet Potat...|It's important to...|indian|\n",
      "|        Chicken Saag|The classic India...|indian|\n",
      "|Paleo Slow Cooker...|Boneless pork loi...|indian|\n",
      "|Bombay Chicken an...|Chicken parts are...|indian|\n",
      "|Indian Carrots, P...|Potatoes, peas an...|indian|\n",
      "|Wendy's Indian Bu...|This recipe resem...|indian|\n",
      "|    Indian Chickpeas|Garbanzo beans, o...|indian|\n",
      "|Dal Makhani (Indi...|These richly spic...|indian|\n",
      "|               Raita|Chopped tomatoes ...|indian|\n",
      "|Yogurt-Marinated ...|A yogurt-based ma...|indian|\n",
      "|Indian-Spiced Roa...|Spicy roasted chi...|indian|\n",
      "|Cauliflower and T...|Pressed tofu cube...|indian|\n",
      "|Channa Masala (Ch...|This fantastic In...|indian|\n",
      "|Bengali Chicken C...|Thy this deliciou...|indian|\n",
      "|  Indian Sweet Bread|A crisp and sweet...|indian|\n",
      "| Rosy's Palak Paneer|An Indian friend ...|indian|\n",
      "|Roti Bread from I...|This version of t...|indian|\n",
      "|Indian Vegetable ...|Basmati rice enha...|indian|\n",
      "+--------------------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combine all datasets into one\n",
    "\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def unionAll(dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "dfs = [indian, italian, mexican, new_recipe]\n",
    "recipe = unionAll(dfs)\n",
    "recipe.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As these two scraping are from the same website, but using different method, there might be some duplicates in the combined datasets. We try to remove the duplicates before data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3840\n"
     ]
    }
   ],
   "source": [
    "# Size before removing duplicates\n",
    "print(recipe.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "recipe = recipe.dropDuplicates([\"Title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3793\n"
     ]
    }
   ],
   "source": [
    "# Size after dropping duplicates\n",
    "print(recipe.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we come to the data cleaning part! Our dataset is all about **text**, and it needs to be transformed a bit for better analysis.\n",
    "\n",
    "- Lowercase\n",
    "- Remove punctuations and digits. \n",
    "\n",
    "In NLTK, we have packages which helps remove punctuations and digits, but in our case, we try to use **regular expression** to remove unwanted signs in our dataset.\n",
    "- Remove stopwords\n",
    "- Stemming the text \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all packages needed for data cleaning\n",
    "\n",
    "from pyspark.sql.functions import udf, regexp_replace, lower, col\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from pyspark.sql.types import IntegerType, ArrayType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+\n",
      "|               Title|         Description| label|\n",
      "+--------------------+--------------------+------+\n",
      "|  indian peanut stew|this is an easy, ...|indian|\n",
      "|        roomali roti|there is no leave...|indian|\n",
      "|spicy sweet potat...|it's important to...|indian|\n",
      "|        chicken saag|the classic india...|indian|\n",
      "|paleo slow cooker...|boneless pork loi...|indian|\n",
      "|bombay chicken an...|chicken parts are...|indian|\n",
      "|indian carrots, p...|potatoes, peas an...|indian|\n",
      "|wendy's indian bu...|this recipe resem...|indian|\n",
      "|    indian chickpeas|garbanzo beans, o...|indian|\n",
      "|dal makhani (indi...|these richly spic...|indian|\n",
      "|               raita|chopped tomatoes ...|indian|\n",
      "|yogurt-marinated ...|a yogurt-based ma...|indian|\n",
      "|indian-spiced roa...|spicy roasted chi...|indian|\n",
      "|cauliflower and t...|pressed tofu cube...|indian|\n",
      "|channa masala (ch...|this fantastic in...|indian|\n",
      "|bengali chicken c...|thy this deliciou...|indian|\n",
      "|  indian sweet bread|a crisp and sweet...|indian|\n",
      "| rosy's palak paneer|an indian friend ...|indian|\n",
      "|roti bread from i...|this version of t...|indian|\n",
      "|indian vegetable ...|basmati rice enha...|indian|\n",
      "+--------------------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lowercase\n",
    "\n",
    "recipe = recipe.select(*[lower(col(col_name)).name(col_name) for col_name in recipe.columns])\n",
    "recipe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+\n",
      "|               title|                 des| label|\n",
      "+--------------------+--------------------+------+\n",
      "|  indian peanut stew|this is an easy a...|indian|\n",
      "|        roomali roti|there is no leave...|indian|\n",
      "|spicy sweet potat...|its important to ...|indian|\n",
      "|        chicken saag|the classic india...|indian|\n",
      "|paleo slow cooker...|boneless pork loi...|indian|\n",
      "|bombay chicken an...|chicken parts are...|indian|\n",
      "|indian carrots pe...|potatoes peas and...|indian|\n",
      "|wendys indian but...|this recipe resem...|indian|\n",
      "|    indian chickpeas|garbanzo beans on...|indian|\n",
      "|dal makhani india...|these richly spic...|indian|\n",
      "|               raita|chopped tomatoes ...|indian|\n",
      "|yogurtmarinated s...|a yogurtbased mar...|indian|\n",
      "|indianspiced roas...|spicy roasted chi...|indian|\n",
      "|cauliflower and t...|pressed tofu cube...|indian|\n",
      "|channa masala chi...|this fantastic in...|indian|\n",
      "|bengali chicken c...|thy this deliciou...|indian|\n",
      "|  indian sweet bread|a crisp and sweet...|indian|\n",
      "|  rosys palak paneer|an indian friend ...|indian|\n",
      "|roti bread from i...|this version of t...|indian|\n",
      "|indian vegetable ...|basmati rice enha...|indian|\n",
      "+--------------------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove punctuation and digits\n",
    "\n",
    "recipe_clean = recipe.select(regexp_replace('Title', \"[^a-zA-Z\\\\s]\", \"\").alias('title'), \n",
    "    (regexp_replace('Description', \"[^a-zA-Z\\\\s]\", \"\").alias('des')),'label')\n",
    "recipe_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+\n",
      "|               title|           des_clean| label|\n",
      "+--------------------+--------------------+------+\n",
      "|  indian peanut stew|[easy, authentic,...|indian|\n",
      "|        roomali roti|[leavening, simpl...|indian|\n",
      "|spicy sweet potat...|[important, use, ...|indian|\n",
      "|        chicken saag|[classic, indian,...|indian|\n",
      "|paleo slow cooker...|[boneless, pork, ...|indian|\n",
      "|bombay chicken an...|[chicken, parts, ...|indian|\n",
      "|indian carrots pe...|[potatoes, peas, ...|indian|\n",
      "|wendys indian but...|[recipe, resemble...|indian|\n",
      "|    indian chickpeas|[garbanzo, beans,...|indian|\n",
      "|dal makhani india...|[richly, spiced, ...|indian|\n",
      "|               raita|[chopped, tomatoe...|indian|\n",
      "|yogurtmarinated s...|[yogurtbased, mar...|indian|\n",
      "|indianspiced roas...|[spicy, roasted, ...|indian|\n",
      "|cauliflower and t...|[pressed, tofu, c...|indian|\n",
      "|channa masala chi...|[fantastic, india...|indian|\n",
      "|bengali chicken c...|[thy, delicious, ...|indian|\n",
      "|  indian sweet bread|[crisp, sweet, fl...|indian|\n",
      "|  rosys palak paneer|[indian, friend, ...|indian|\n",
      "|roti bread from i...|[version, delicat...|indian|\n",
      "|indian vegetable ...|[basmati, rice, e...|indian|\n",
      "+--------------------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove Stopwords\n",
    "\n",
    "# Tokenize text\n",
    "tokenizer = Tokenizer(inputCol=\"des\", outputCol=\"des_token\")\n",
    "recipe = tokenizer.transform(recipe_clean).select('title','des','des_token','label')\n",
    "# tokenized.select(\"Description\", \"Des_words\")\\\n",
    "    #.withColumn(\"tokens\", countTokens(col(\"Des_words\"))).show(truncate=False)\n",
    "\n",
    "# Remove stopwords\n",
    "remover = StopWordsRemover(inputCol='des_token', outputCol='des_clean')\n",
    "recipe_no_stopw = remover.transform(recipe).select('title','des_clean', 'label')\n",
    "recipe_no_stopw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+\n",
      "|               title|         des_stemmed| label|\n",
      "+--------------------+--------------------+------+\n",
      "|  indian peanut stew|[easi, authent, d...|indian|\n",
      "|        roomali roti|[leaven, simpl, t...|indian|\n",
      "|spicy sweet potat...|[import, use, goo...|indian|\n",
      "|        chicken saag|[classic, indian,...|indian|\n",
      "|paleo slow cooker...|[boneless, pork, ...|indian|\n",
      "|bombay chicken an...|[chicken, part, b...|indian|\n",
      "|indian carrots pe...|[potato, pea, car...|indian|\n",
      "|wendys indian but...|[recip, resembl, ...|indian|\n",
      "|    indian chickpeas|[garbanzo, bean, ...|indian|\n",
      "|dal makhani india...|[rich, spice, len...|indian|\n",
      "|               raita|[chop, tomato, cu...|indian|\n",
      "|yogurtmarinated s...|[yogurtbas, marin...|indian|\n",
      "|indianspiced roas...|[spici, roast, ch...|indian|\n",
      "|cauliflower and t...|[press, tofu, cub...|indian|\n",
      "|channa masala chi...|[fantast, indian,...|indian|\n",
      "|bengali chicken c...|[thi, delici, aut...|indian|\n",
      "|  indian sweet bread|[crisp, sweet, fl...|indian|\n",
      "|  rosys palak paneer|[indian, friend, ...|indian|\n",
      "|roti bread from i...|[version, delic, ...|indian|\n",
      "|indian vegetable ...|[basmati, rice, e...|indian|\n",
      "+--------------------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stem text\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
    "recipe = recipe_no_stopw.withColumn(\"des_stemmed\", stemmer_udf(\"des_clean\")).select('title', 'des_stemmed','label')\n",
    "recipe.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splitting & Pattern Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting\n",
    "\n",
    "maxim's wording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out different recipes\n",
    "# Create temp table\n",
    "recipe.createOrReplaceTempView('recipes')\n",
    "\n",
    "recipe_ind = sqlContext.sql(\"SELECT * FROM recipes WHERE label == 'indian'\")\n",
    "recipe_ita = sqlContext.sql(\"SELECT * FROM recipes WHERE label == 'italian'\")\n",
    "recipe_mex = sqlContext.sql(\"SELECT * FROM recipes WHERE label == 'mexican'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We asssume that we do not know labels for the majority of data points, \n",
    "# hence further explore only test split\n",
    "recipe_ind_tr, recipe_ind_ts, recipe_ind_d,recipe_ind_v  = recipe_ind.randomSplit([0.6,0.2,0.1,0.1],seed = 11)\n",
    "recipe_ita_tr, recipe_ita_ts, recipe_ita_d, recipe_ita_v = recipe_ita.randomSplit([0.6,0.2,0.1,0.1],seed = 11)\n",
    "recipe_mex_tr, recipe_mex_ts, recipe_mex_d, recipe_mex_v = recipe_mex.randomSplit([0.6,0.2,0.1,0.1],seed = 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern Exploration\n",
    "\n",
    "Before writing the label functions, we want to get our head around how our dataset look like and see if we can get some ideas from it for writing label functions.\n",
    "\n",
    "Since we want to label the type of the recipe. We made a word frequency dataframe, which displays top 15 words in each type of the recipe to help us identify the keywords connected to each cuise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create frequency list of description\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "top_n = 20\n",
    "\n",
    "ind_counts = recipe_ind_d.select(f.explode('des_stemmed').alias('col')).groupBy('col').count()\n",
    "ind_des_freq = ind_counts.orderBy(ind_counts[\"count\"].desc()).limit(top_n)\n",
    "\n",
    "ita_counts = recipe_ita_d.select(f.explode('des_stemmed').alias('col')).groupBy('col').count()\n",
    "ita_des_freq = ita_counts.orderBy(ita_counts[\"count\"].desc()).limit(top_n)\n",
    "\n",
    "mex_counts = recipe_mex_d.select(f.explode('des_stemmed').alias('col')).groupBy('col').count()\n",
    "mex_des_freq = mex_counts.orderBy(mex_counts[\"count\"].desc()).limit(top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-------+-----+--------+-----+\n",
      "|    col|count|    col|count|     col|count|\n",
      "+-------+-----+-------+-----+--------+-----+\n",
      "|  curri|   52|italian|   41| mexican|   35|\n",
      "| indian|   41|   sauc|   38| chicken|   34|\n",
      "|   rice|   40|  pasta|   35|    serv|   31|\n",
      "|  recip|   38| tomato|   33|  tomato|   30|\n",
      "|  spice|   35|  recip|   32|tortilla|   28|\n",
      "|   dish|   34|   dish|   31|    make|   27|\n",
      "|   make|   31|   serv|   30|    dish|   24|\n",
      "|chicken|   30|   make|   26|    bean|   23|\n",
      "|   serv|   28|  bread|   24|   recip|   22|\n",
      "|   cook|   27|  chees|   24|  chilli|   19|\n",
      "|flavour|   25|  fresh|   23|   spici|   19|\n",
      "|    use|   22| garlic|   22|   onion|   17|\n",
      "|   easi|   19|    use|   21|   salsa|   17|\n",
      "|   sauc|   18| delici|   21|   fresh|   16|\n",
      "|   naan|   17|   bake|   20|    sauc|   16|\n",
      "| delici|   16|   easi|   18|    corn|   16|\n",
      "| tomato|   13|  simpl|   17|    cook|   16|\n",
      "|   meal|   13|  salad|   17| avocado|   15|\n",
      "|    hot|   13|chicken|   17|   great|   14|\n",
      "|   made|   13|       |   15|     use|   14|\n",
      "+-------+-----+-------+-----+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View then in one dataframe\n",
    "from pyspark.sql.functions import monotonically_increasing_id \n",
    "\n",
    "df1 = ind_des_freq.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "df2 = ita_des_freq.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "df3 = mex_des_freq.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "\n",
    "des_freq = df1.join(df2,(\"row_id\")).join(df3,(\"row_id\")).drop(\"row_id\")\n",
    "des_freq.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maxim continue with your wording for data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "# train split full / remove lists\n",
    "df_tr = recipe_ind_tr.union(recipe_ita_tr)\n",
    "df_tr = df_tr.union(recipe_mex_tr).withColumn(\"des_stemmed\", concat_ws(\" \", \"des_stemmed\"))\n",
    "\n",
    "# test split \n",
    "df_ts = recipe_ind_ts.union(recipe_ita_ts)\n",
    "df_ts = df_ts.union(recipe_mex_ts).withColumn(\"des_stemmed\", concat_ws(\" \", \"des_stemmed\"))\n",
    "\n",
    "# dev split - labelled\n",
    "df_d = recipe_ind_d.union(recipe_ita_d)\n",
    "df_d = df_d.union(recipe_mex_d).withColumn(\"des_stemmed\", concat_ws(\" \", \"des_stemmed\"))\n",
    "\n",
    "# val split - labelled\n",
    "df_v = recipe_ind_d.union(recipe_ita_v)\n",
    "df_v = df_v.union(recipe_mex_v).withColumn(\"des_stemmed\", concat_ws(\" \", \"des_stemmed\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop train split for labelling\n",
    "# df_tr.drop('label').first()\n",
    "df_tr = df_tr.drop('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'indian':0, 'italian':1, 'mexican':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+\n",
      "|               title|         des_stemmed|num_label|\n",
      "+--------------------+--------------------+---------+\n",
      "|           aloo gobi|tradit indian dis...|        0|\n",
      "|andreas dal for i...|india dal mean nu...|        0|\n",
      "|black pepper goat...|black peppercorn ...|        0|\n",
      "|bombay chicken wings|chicken wing ligh...|        0|\n",
      "|butternut squash ...|butternut squash ...|        0|\n",
      "|chaat  dahi batat...|fragrant aromat d...|        0|\n",
      "|chicken roti from...|fragrant homemad ...|        0|\n",
      "|chickpea coconut ...|garbanzo bean get...|        0|\n",
      "|    cilantro chutney|green chile peppe...|        0|\n",
      "|creamy cashew chi...|marin curri rub c...|        0|\n",
      "|cucumber peanut s...|fresh delici cucu...|        0|\n",
      "|     easy curry rice|rice side dish re...|        0|\n",
      "|four seasons chic...|hot sweet sour fr...|        0|\n",
      "|fruited tofu curr...|vegetarian tofu s...|        0|\n",
      "|  goan pork vindaloo|tradit goan pork ...|        0|\n",
      "|gobi masala cauli...|small floret stir...|        0|\n",
      "|indian barbeque c...|marin chicken ove...|        0|\n",
      "|indian chole aloo...|delici vegan indi...|        0|\n",
      "|indian curry pota...|potato cube fri i...|        0|\n",
      "| indian sabji recipe|cabbag potato fla...|        0|\n",
      "+--------------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import create_map\n",
    "from itertools import chain\n",
    "\n",
    "mapping_func = create_map([lit(x) for x in chain(*mapping.items())])\n",
    "\n",
    "# Label for validation set\n",
    "df_v = df_v.withColumn(\"num_label\", mapping_func.getItem(col(\"label\")))\n",
    "# df_dv = df_dv.select('title','des_clean', 'num_label')\n",
    "df_v = df_v.select('title','des_stemmed', 'num_label')\n",
    "df_v.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+\n",
      "|               title|         des_stemmed|num_label|\n",
      "+--------------------+--------------------+---------+\n",
      "|           aloo gobi|tradit indian dis...|        0|\n",
      "|andreas dal for i...|india dal mean nu...|        0|\n",
      "|black pepper goat...|black peppercorn ...|        0|\n",
      "|bombay chicken wings|chicken wing ligh...|        0|\n",
      "|butternut squash ...|butternut squash ...|        0|\n",
      "|chaat  dahi batat...|fragrant aromat d...|        0|\n",
      "|chicken roti from...|fragrant homemad ...|        0|\n",
      "|chickpea coconut ...|garbanzo bean get...|        0|\n",
      "|    cilantro chutney|green chile peppe...|        0|\n",
      "|creamy cashew chi...|marin curri rub c...|        0|\n",
      "|cucumber peanut s...|fresh delici cucu...|        0|\n",
      "|     easy curry rice|rice side dish re...|        0|\n",
      "|four seasons chic...|hot sweet sour fr...|        0|\n",
      "|fruited tofu curr...|vegetarian tofu s...|        0|\n",
      "|  goan pork vindaloo|tradit goan pork ...|        0|\n",
      "|gobi masala cauli...|small floret stir...|        0|\n",
      "|indian barbeque c...|marin chicken ove...|        0|\n",
      "|indian chole aloo...|delici vegan indi...|        0|\n",
      "|indian curry pota...|potato cube fri i...|        0|\n",
      "| indian sabji recipe|cabbag potato fla...|        0|\n",
      "+--------------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Label for development set\n",
    "df_d = df_d.withColumn(\"num_label\", mapping_func.getItem(col(\"label\")))\n",
    "# df_dv = df_dv.select('title','des_clean', 'num_label')\n",
    "df_d = df_d.select('title','des_stemmed', 'num_label')\n",
    "df_d.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_label for test split for accuracy - INCLUDE LABEL TO GET REAL GOLD:)\n",
    "df_ts = df_ts.withColumn(\"num_label\", mapping_func.getItem(col(\"label\")))\n",
    "df_ts = df_ts.select('title','des_stemmed', 'num_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column with actual values \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "Y_v = df_v.select('num_label').rdd.flatMap(lambda x: x).collect()\n",
    "Y_v = np.array(Y_v)\n",
    "\n",
    "Y_dev = df_d.select('num_label').rdd.flatMap(lambda x: x).collect()\n",
    "Y_dev = np.array(Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = df_ts.select('num_label').rdd.flatMap(lambda x: x).collect()\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For clarity, we define constants to represent the class labels and abstaining.\n",
    "ABSTAIN = -1\n",
    "INDIAN = 0\n",
    "ITALIAN = 1\n",
    "MEXICAN = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labelling Functions Introduction\n",
    "\n",
    "Once the pattern exploration has been finished, it was possible to write Labelling Functions. Hereby, due to the fact that there were no structural differences (e.g. there were no visible patterns for one cuisine) between the names and description of any cuisine, the Labelling Functions mainly search for aforementioned keywords or their combinations (obtained through careful exploration of the scraped datasets) that may be relevent to just one cuisine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snorkel in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (0.9.3)\n",
      "Requirement already satisfied: tensorboardX<2.0,>=1.6 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from snorkel) (1.9)\n",
      "Requirement already satisfied: munkres==1.1.2 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from snorkel) (1.1.2)\n",
      "Requirement already satisfied: torch<1.2.0,>=1.1.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from snorkel) (1.1.0)\n",
      "Requirement already satisfied: networkx<2.4,>=2.2 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from snorkel) (2.3)\n",
      "Collecting scikit-learn<0.22.0,>=0.20.2\n",
      "  Using cached scikit_learn-0.21.3-cp36-cp36m-manylinux1_x86_64.whl (6.7 MB)\n",
      "Collecting pandas<0.26.0,>=0.25.0\n",
      "  Using cached pandas-0.25.3-cp36-cp36m-manylinux1_x86_64.whl (10.4 MB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.16.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from snorkel) (1.18.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.33.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from snorkel) (4.43.0)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.2.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from snorkel) (1.4.1)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from tensorboardX<2.0,>=1.6->snorkel) (3.11.3)\n",
      "Requirement already satisfied: six in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from tensorboardX<2.0,>=1.6->snorkel) (1.14.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from networkx<2.4,>=2.2->snorkel) (4.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from scikit-learn<0.22.0,>=0.20.2->snorkel) (0.14.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from pandas<0.26.0,>=0.25.0->snorkel) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from pandas<0.26.0,>=0.25.0->snorkel) (2.8.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorboardX<2.0,>=1.6->snorkel) (45.2.0.post20200210)\n",
      "\u001b[31mERROR: faculty-models 0.1.1 has requirement mlflow-faculty>=0.4.3, but you'll have mlflow-faculty 0.4.2 which is incompatible.\u001b[0m\n",
      "Installing collected packages: scikit-learn, pandas\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.22.2.post1\n",
      "    Uninstalling scikit-learn-0.22.2.post1:\n",
      "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.0.1\n",
      "    Uninstalling pandas-1.0.1:\n",
      "      Successfully uninstalled pandas-1.0.1\n",
      "Successfully installed pandas-0.25.3 scikit-learn-0.22.1\n"
     ]
    }
   ],
   "source": [
    "! pip install snorkel\n",
    "from snorkel.labeling.apply.spark import SparkLFApplier\n",
    "from snorkel.labeling import LFAnalysis\n",
    "from snorkel.labeling import labeling_function\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indian LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling fuction that checks whether identified indian keywords are in the title column of the RDD\n",
    "ind_keywords = ['curri','masala','paneer','chutney','simmered','cumin','yogurt',\n",
    "                'coconut','lentil','veget','chickpea','korma','dal','tandoori','tikka']\n",
    "\n",
    "@labeling_function()\n",
    "def indian_title_keywords(x):\n",
    "        if any(word in x.title for word in ind_keywords):\n",
    "            return INDIAN\n",
    "        else:\n",
    "            return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling fuction that checks whether identified indian keywords are in the description column of the RDD\n",
    "ind_keywords_des = ['naan','curri','vegetarian','tradit','veget']\n",
    "@labeling_function()\n",
    "def indian_des_keywords(x):\n",
    "        if any(word in x.des_stemmed for word in ind_keywords_des):\n",
    "            return INDIAN\n",
    "        else:\n",
    "            return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling fuction that checks whether combination of curry + different kinds of meat is in the description column of the RDD\n",
    "@labeling_function()\n",
    "def currymeat(x):\n",
    "    return INDIAN if re.search(r\"(?=.*curry)(?=.*(chicken|lamb|beef|prawn))\", x.des_stemmed, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling fuction that checks whether combination of quick or easy + different kinds of ingredients is in the description column of the RDD\n",
    "@labeling_function()\n",
    "def cook_food(x):\n",
    "    return INDIAN if re.search(r\"(?=.*(quick|easy))(?=.*(rice|sauc|potato|tomato|bread))\", x.des_stemmed, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling fuction that checks whether combination of sweet + spicy is in the description column of the RDD\n",
    "\n",
    "@labeling_function()\n",
    "def sweet_spicy(x):\n",
    "    return INDIAN if re.search(r\"(?=.*sweet)(?=.*(spici))\", x.des_stemmed, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling fuction that checks whether combination of slow + cook is in the description column of the RDD\n",
    "\n",
    "@labeling_function()\n",
    "def slow_cook(x):\n",
    "    return INDIAN if re.search(r\"(?=.*slow)(?=.*(cook))\", x.des_stemmed, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Italian LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling fuction that checks whether identified italian keywords from are in the title column of the RDD\n",
    "ita_keywords = ['pasta','spaghetti','risotto','sausag','mushroom','lasagn','lasagna','bolognes'\n",
    "                ,'pizza','parmesan','tiramisu','ricotta','bruschetta','bake']\n",
    "\n",
    "@labeling_function()\n",
    "def italian_title_keywords(x):\n",
    "        if any(word in x.title for word in ita_keywords):\n",
    "            return ITALIAN\n",
    "        else:\n",
    "            return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling fuction that checks whether identified indian keywords are in the description column of the RDD\n",
    "ita_keywords_des = ['garlic','classic','oliv','parmesan','pasta','rich','mozzarella','lasagna','pesto','dente']\n",
    "\n",
    "@labeling_function()\n",
    "def italian_des_keywords(x):\n",
    "        if any(word in x.des_stemmed for word in ita_keywords_des):\n",
    "            return ITALIAN\n",
    "        else:\n",
    "            return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling fuction that checks whether combination of pasta + different kinds of ingredients is in the description column of the RDD\n",
    "@labeling_function()\n",
    "def pasta_with(x):\n",
    "    return ITALIAN if re.search(r\"(?=.*pasta)(?=.*(chicken|lamb|beef|pesto|creami|cream|shrimp|chees))\", x.des_stemmed, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling fuction that checks whether combination of tomato +  sun-dried or sundried is in the description column of the RDD\n",
    "@labeling_function()\n",
    "def sundried_tomatoes(x):\n",
    "    return ITALIAN if re.search(r\"(?=.*tomato)(?=.*(sun-dri|sundri))\", x.des_stemmed, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling fuction that checks whether any of the identified regions is in the description column of the RDD\n",
    "ita_regions = ['tuscan','sicilian', 'romano', 'romaine', 'mediterranean','meditterranean' ]\n",
    "\n",
    "@labeling_function()\n",
    "def ita_regions(x):\n",
    "        if any(word in x.des_stemmed for word in ita_keywords):\n",
    "            return ITALIAN\n",
    "        else:\n",
    "            return ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mexican LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling fuction that checks whether identified mexican keywords from are in the title column of the RDD\n",
    "mex_keywords = ['enchilada','taco','bean','salsa','soup','fahita','pork','guacamol',\n",
    "                'de','quesadilla','casserol','black','dip','burrito','corn']\n",
    "\n",
    "@labeling_function()\n",
    "def mexican_title_keywords(x):\n",
    "        if any(word in x.title for word in mex_keywords):\n",
    "            return MEXICAN\n",
    "        else:\n",
    "            return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling fuction that checks whether identified mexican keywords are in the description column of the RDD\n",
    "mex_keywords_des = ['tortilla','corn','bean','salsa','chilli','spici','chip','simmer','taco','season','soup']\n",
    "\n",
    "@labeling_function()\n",
    "def mexican_des_keywords(x):\n",
    "        if any(word in x.des_stemmed for word in mex_keywords_des):\n",
    "            return MEXICAN\n",
    "        else:\n",
    "            return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling fuction that checks whether combination of tortilla + corn/salsa/bean is in the description column of the RDD\n",
    "\n",
    "@labeling_function()\n",
    "def tortilla_corn(x):\n",
    "    return MEXICAN if re.search(r\"(?=.*tortilla)(?=.*(corn|salsa|bean))\", x.des_stemmed, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling fuction that checks whether combination of salsa + chill/sauce/soup/fill is in the description column of the RDD\n",
    "@labeling_function()\n",
    "def salsa_food(x):\n",
    "    return MEXICAN if re.search(r\"(?=.*(salsa))(?=.*(chilli|sauc|sauce|soup|fill))\", x.des_stemmed, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling fuction that checks whether combination of chip + chicken/shrimp/chilli/pepper/sauce/fill is in the description column of the RDD\n",
    "@labeling_function()\n",
    "def chip(x):\n",
    "    return MEXICAN if re.search(r\"(?=.*(chip))(?=.*(chicken|shrimp|chilli|pepper|sauc|sauce||fill))\", x.des_stemmed, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling fuction that checks whether combination of canned + chilli/soup/sauce is in the description column of the RDD\n",
    "@labeling_function()\n",
    "def canned_food(x):\n",
    "    return MEXICAN if re.search(r\"(?=.*(cann))(?=.*(chilli|soup|sauc|sauce))\", x.des_stemmed, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate performance on training and dev sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting PySpark DataFrames to RDD\n",
    "\n",
    "df_tr_rdd = df_tr.rdd\n",
    "df_d_rdd = df_d.rdd\n",
    "\n",
    "# Pooling all LFs into one list that will be passed to SparkLFApplier\n",
    "lfs = [indian_title_keywords, \n",
    "       indian_des_keywords, \n",
    "       currymeat, \n",
    "       cook_food, \n",
    "       sweet_spicy, \n",
    "       slow_cook, \n",
    "       italian_title_keywords, \n",
    "       italian_des_keywords, \n",
    "       pasta_with, \n",
    "       sundried_tomatoes, \n",
    "       ita_regions,\n",
    "       mexican_title_keywords, \n",
    "       mexican_des_keywords, \n",
    "       tortilla_corn, \n",
    "       salsa_food, \n",
    "       chip, \n",
    "       canned_food]\n",
    "\n",
    "# Using SparkLFApplier\n",
    "applier = SparkLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df_tr_rdd)\n",
    "L_val = applier.apply(df_v_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>indian_title_keywords</th>\n",
       "      <td>0</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.147520</td>\n",
       "      <td>0.109792</td>\n",
       "      <td>0.066130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indian_des_keywords</th>\n",
       "      <td>1</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.238237</td>\n",
       "      <td>0.166596</td>\n",
       "      <td>0.117847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>currymeat</th>\n",
       "      <td>2</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.001696</td>\n",
       "      <td>0.001696</td>\n",
       "      <td>0.001272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cook_food</th>\n",
       "      <td>3</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.032217</td>\n",
       "      <td>0.028402</td>\n",
       "      <td>0.022891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sweet_spicy</th>\n",
       "      <td>4</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.005935</td>\n",
       "      <td>0.005935</td>\n",
       "      <td>0.005935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slow_cook</th>\n",
       "      <td>5</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.022891</td>\n",
       "      <td>0.018228</td>\n",
       "      <td>0.012717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>italian_title_keywords</th>\n",
       "      <td>6</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.181009</td>\n",
       "      <td>0.159390</td>\n",
       "      <td>0.071641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>italian_des_keywords</th>\n",
       "      <td>7</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.223400</td>\n",
       "      <td>0.190335</td>\n",
       "      <td>0.112336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pasta_with</th>\n",
       "      <td>8</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.038152</td>\n",
       "      <td>0.038152</td>\n",
       "      <td>0.018652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sundried_tomatoes</th>\n",
       "      <td>9</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.004663</td>\n",
       "      <td>0.004239</td>\n",
       "      <td>0.000848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ita_regions</th>\n",
       "      <td>10</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.221704</td>\n",
       "      <td>0.205596</td>\n",
       "      <td>0.101738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mexican_title_keywords</th>\n",
       "      <td>11</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.260704</td>\n",
       "      <td>0.208139</td>\n",
       "      <td>0.102586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mexican_des_keywords</th>\n",
       "      <td>12</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.340822</td>\n",
       "      <td>0.289106</td>\n",
       "      <td>0.171259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tortilla_corn</th>\n",
       "      <td>13</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.033489</td>\n",
       "      <td>0.033489</td>\n",
       "      <td>0.010598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salsa_food</th>\n",
       "      <td>14</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.011446</td>\n",
       "      <td>0.011446</td>\n",
       "      <td>0.005087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chip</th>\n",
       "      <td>15</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.028826</td>\n",
       "      <td>0.028826</td>\n",
       "      <td>0.008478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>canned_food</th>\n",
       "      <td>16</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.006359</td>\n",
       "      <td>0.006359</td>\n",
       "      <td>0.005511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         j Polarity  Coverage  Overlaps  Conflicts\n",
       "indian_title_keywords    0      [0]  0.147520  0.109792   0.066130\n",
       "indian_des_keywords      1      [0]  0.238237  0.166596   0.117847\n",
       "currymeat                2      [0]  0.001696  0.001696   0.001272\n",
       "cook_food                3      [0]  0.032217  0.028402   0.022891\n",
       "sweet_spicy              4      [0]  0.005935  0.005935   0.005935\n",
       "slow_cook                5      [0]  0.022891  0.018228   0.012717\n",
       "italian_title_keywords   6      [1]  0.181009  0.159390   0.071641\n",
       "italian_des_keywords     7      [1]  0.223400  0.190335   0.112336\n",
       "pasta_with               8      [1]  0.038152  0.038152   0.018652\n",
       "sundried_tomatoes        9      [1]  0.004663  0.004239   0.000848\n",
       "ita_regions             10      [1]  0.221704  0.205596   0.101738\n",
       "mexican_title_keywords  11      [2]  0.260704  0.208139   0.102586\n",
       "mexican_des_keywords    12      [2]  0.340822  0.289106   0.171259\n",
       "tortilla_corn           13      [2]  0.033489  0.033489   0.010598\n",
       "salsa_food              14      [2]  0.011446  0.011446   0.005087\n",
       "chip                    15      [2]  0.028826  0.028826   0.008478\n",
       "canned_food             16      [2]  0.006359  0.006359   0.005511"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking Statistics of Labelling Functions\n",
    "from snorkel.labeling import LFAnalysis\n",
    "\n",
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment regarding the performance of LFs**\n",
    "\n",
    "From the tables above it can be seen that the LFs vary in their coverage, which describes what proportion of the dataset a label covers. \n",
    "\n",
    "Indeed, the label with highest coverage is 'mexican_des_keywords' (34.08%) while the lowest are 'currymeat', 'sweet_spicy', 'sundried_tomatoes', 'canned_food' (all nearly 0). It can be argued that this disproportion ranges due to the specificity of the latter LFs.Secondly, 12 out of 17 Labelling Functions have conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Incorrect</th>\n",
       "      <th>Emp. Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>indian_title_keywords</th>\n",
       "      <td>0</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.160326</td>\n",
       "      <td>0.119565</td>\n",
       "      <td>0.081522</td>\n",
       "      <td>50</td>\n",
       "      <td>9</td>\n",
       "      <td>0.847458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indian_des_keywords</th>\n",
       "      <td>1</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.252717</td>\n",
       "      <td>0.179348</td>\n",
       "      <td>0.127717</td>\n",
       "      <td>67</td>\n",
       "      <td>26</td>\n",
       "      <td>0.720430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>currymeat</th>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cook_food</th>\n",
       "      <td>3</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.040761</td>\n",
       "      <td>0.027174</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>0.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sweet_spicy</th>\n",
       "      <td>4</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.008152</td>\n",
       "      <td>0.008152</td>\n",
       "      <td>0.008152</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slow_cook</th>\n",
       "      <td>5</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.016304</td>\n",
       "      <td>0.005435</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>italian_title_keywords</th>\n",
       "      <td>6</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.165761</td>\n",
       "      <td>0.141304</td>\n",
       "      <td>0.059783</td>\n",
       "      <td>56</td>\n",
       "      <td>5</td>\n",
       "      <td>0.918033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>italian_des_keywords</th>\n",
       "      <td>7</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.233696</td>\n",
       "      <td>0.211957</td>\n",
       "      <td>0.133152</td>\n",
       "      <td>57</td>\n",
       "      <td>29</td>\n",
       "      <td>0.662791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pasta_with</th>\n",
       "      <td>8</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.024457</td>\n",
       "      <td>0.024457</td>\n",
       "      <td>0.013587</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sundried_tomatoes</th>\n",
       "      <td>9</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.005435</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ita_regions</th>\n",
       "      <td>10</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.201087</td>\n",
       "      <td>0.184783</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>63</td>\n",
       "      <td>11</td>\n",
       "      <td>0.851351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mexican_title_keywords</th>\n",
       "      <td>11</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.236413</td>\n",
       "      <td>0.190217</td>\n",
       "      <td>0.105978</td>\n",
       "      <td>65</td>\n",
       "      <td>22</td>\n",
       "      <td>0.747126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mexican_des_keywords</th>\n",
       "      <td>12</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.350543</td>\n",
       "      <td>0.293478</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>74</td>\n",
       "      <td>55</td>\n",
       "      <td>0.573643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tortilla_corn</th>\n",
       "      <td>13</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.035326</td>\n",
       "      <td>0.035326</td>\n",
       "      <td>0.019022</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salsa_food</th>\n",
       "      <td>14</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.013587</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chip</th>\n",
       "      <td>15</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.027174</td>\n",
       "      <td>0.027174</td>\n",
       "      <td>0.008152</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>canned_food</th>\n",
       "      <td>16</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.005435</td>\n",
       "      <td>0.005435</td>\n",
       "      <td>0.005435</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         j Polarity  Coverage  Overlaps  Conflicts  Correct  \\\n",
       "indian_title_keywords    0      [0]  0.160326  0.119565   0.081522       50   \n",
       "indian_des_keywords      1      [0]  0.252717  0.179348   0.127717       67   \n",
       "currymeat                2       []  0.000000  0.000000   0.000000        0   \n",
       "cook_food                3      [0]  0.043478  0.040761   0.027174        7   \n",
       "sweet_spicy              4      [0]  0.008152  0.008152   0.008152        1   \n",
       "slow_cook                5      [0]  0.021739  0.016304   0.005435        5   \n",
       "italian_title_keywords   6      [1]  0.165761  0.141304   0.059783       56   \n",
       "italian_des_keywords     7      [1]  0.233696  0.211957   0.133152       57   \n",
       "pasta_with               8      [1]  0.024457  0.024457   0.013587        9   \n",
       "sundried_tomatoes        9      [1]  0.005435  0.002717   0.002717        2   \n",
       "ita_regions             10      [1]  0.201087  0.184783   0.086957       63   \n",
       "mexican_title_keywords  11      [2]  0.236413  0.190217   0.105978       65   \n",
       "mexican_des_keywords    12      [2]  0.350543  0.293478   0.195652       74   \n",
       "tortilla_corn           13      [2]  0.035326  0.035326   0.019022       13   \n",
       "salsa_food              14      [2]  0.021739  0.021739   0.013587        8   \n",
       "chip                    15      [2]  0.027174  0.027174   0.008152        9   \n",
       "canned_food             16      [2]  0.005435  0.005435   0.005435        0   \n",
       "\n",
       "                        Incorrect  Emp. Acc.  \n",
       "indian_title_keywords           9   0.847458  \n",
       "indian_des_keywords            26   0.720430  \n",
       "currymeat                       0   0.000000  \n",
       "cook_food                       9   0.437500  \n",
       "sweet_spicy                     2   0.333333  \n",
       "slow_cook                       3   0.625000  \n",
       "italian_title_keywords          5   0.918033  \n",
       "italian_des_keywords           29   0.662791  \n",
       "pasta_with                      0   1.000000  \n",
       "sundried_tomatoes               0   1.000000  \n",
       "ita_regions                    11   0.851351  \n",
       "mexican_title_keywords         22   0.747126  \n",
       "mexican_des_keywords           55   0.573643  \n",
       "tortilla_corn                   0   1.000000  \n",
       "salsa_food                      0   1.000000  \n",
       "chip                            1   0.900000  \n",
       "canned_food                     2   0.000000  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting summary on VALIDATION set\n",
    "\n",
    "LFAnalysis(L_val, lfs=lfs).lf_summary(Y_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the validation set, we can see that some of the LFs perform really bad (lower than 60%), so we decided to remove them. For other LFs, we can also use val set to check where things go wrong and see if we can further improve our LFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply refined label functions\n",
    "lfs_refined = [indian_title_keywords, \n",
    "       indian_des_keywords, \n",
    "       currymeat, \n",
    "       slow_cook, \n",
    "       italian_title_keywords, \n",
    "       italian_des_keywords, \n",
    "       pasta_with, \n",
    "       sundried_tomatoes, \n",
    "       ita_regions,\n",
    "       mexican_title_keywords,  \n",
    "       tortilla_corn, \n",
    "       salsa_food, \n",
    "       chip, \n",
    "       canned_food]\n",
    "\n",
    "# Using SparkLFApplier\n",
    "applier = SparkLFApplier(lfs=lfs_refined)\n",
    "L_train_r = applier.apply(df_tr_rdd)\n",
    "L_val_r = applier.apply(df_v_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>indian_title_keywords</th>\n",
       "      <td>0</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.147520</td>\n",
       "      <td>0.099618</td>\n",
       "      <td>0.041967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indian_des_keywords</th>\n",
       "      <td>1</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.238237</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.081390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>currymeat</th>\n",
       "      <td>2</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.001696</td>\n",
       "      <td>0.001696</td>\n",
       "      <td>0.001272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slow_cook</th>\n",
       "      <td>3</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.022891</td>\n",
       "      <td>0.016532</td>\n",
       "      <td>0.009750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>italian_title_keywords</th>\n",
       "      <td>4</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.181009</td>\n",
       "      <td>0.156422</td>\n",
       "      <td>0.047478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>italian_des_keywords</th>\n",
       "      <td>5</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.223400</td>\n",
       "      <td>0.182281</td>\n",
       "      <td>0.079695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pasta_with</th>\n",
       "      <td>6</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.038152</td>\n",
       "      <td>0.038152</td>\n",
       "      <td>0.009750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sundried_tomatoes</th>\n",
       "      <td>7</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.004663</td>\n",
       "      <td>0.004239</td>\n",
       "      <td>0.000424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ita_regions</th>\n",
       "      <td>8</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.221704</td>\n",
       "      <td>0.202204</td>\n",
       "      <td>0.070369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mexican_title_keywords</th>\n",
       "      <td>9</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.260704</td>\n",
       "      <td>0.124205</td>\n",
       "      <td>0.097075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tortilla_corn</th>\n",
       "      <td>10</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.033489</td>\n",
       "      <td>0.030097</td>\n",
       "      <td>0.009326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salsa_food</th>\n",
       "      <td>11</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.011446</td>\n",
       "      <td>0.011022</td>\n",
       "      <td>0.005087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chip</th>\n",
       "      <td>12</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.028826</td>\n",
       "      <td>0.023315</td>\n",
       "      <td>0.006783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>canned_food</th>\n",
       "      <td>13</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.006359</td>\n",
       "      <td>0.005511</td>\n",
       "      <td>0.005087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         j Polarity  Coverage  Overlaps  Conflicts\n",
       "indian_title_keywords    0      [0]  0.147520  0.099618   0.041967\n",
       "indian_des_keywords      1      [0]  0.238237  0.142857   0.081390\n",
       "currymeat                2      [0]  0.001696  0.001696   0.001272\n",
       "slow_cook                3      [0]  0.022891  0.016532   0.009750\n",
       "italian_title_keywords   4      [1]  0.181009  0.156422   0.047478\n",
       "italian_des_keywords     5      [1]  0.223400  0.182281   0.079695\n",
       "pasta_with               6      [1]  0.038152  0.038152   0.009750\n",
       "sundried_tomatoes        7      [1]  0.004663  0.004239   0.000424\n",
       "ita_regions              8      [1]  0.221704  0.202204   0.070369\n",
       "mexican_title_keywords   9      [2]  0.260704  0.124205   0.097075\n",
       "tortilla_corn           10      [2]  0.033489  0.030097   0.009326\n",
       "salsa_food              11      [2]  0.011446  0.011022   0.005087\n",
       "chip                    12      [2]  0.028826  0.023315   0.006783\n",
       "canned_food             13      [2]  0.006359  0.005511   0.005087"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking Statistics of Refined Labelling Functions\n",
    "from snorkel.labeling import LFAnalysis\n",
    "\n",
    "LFAnalysis(L=L_train_r, lfs=lfs_refined).lf_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Incorrect</th>\n",
       "      <th>Emp. Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>indian_title_keywords</th>\n",
       "      <td>0</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.160326</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.046196</td>\n",
       "      <td>50</td>\n",
       "      <td>9</td>\n",
       "      <td>0.847458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indian_des_keywords</th>\n",
       "      <td>1</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.252717</td>\n",
       "      <td>0.141304</td>\n",
       "      <td>0.084239</td>\n",
       "      <td>67</td>\n",
       "      <td>26</td>\n",
       "      <td>0.720430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>currymeat</th>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slow_cook</th>\n",
       "      <td>3</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.016304</td>\n",
       "      <td>0.005435</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>italian_title_keywords</th>\n",
       "      <td>4</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.165761</td>\n",
       "      <td>0.141304</td>\n",
       "      <td>0.035326</td>\n",
       "      <td>56</td>\n",
       "      <td>5</td>\n",
       "      <td>0.918033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>italian_des_keywords</th>\n",
       "      <td>5</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.233696</td>\n",
       "      <td>0.203804</td>\n",
       "      <td>0.105978</td>\n",
       "      <td>57</td>\n",
       "      <td>29</td>\n",
       "      <td>0.662791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pasta_with</th>\n",
       "      <td>6</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.024457</td>\n",
       "      <td>0.024457</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sundried_tomatoes</th>\n",
       "      <td>7</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.005435</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ita_regions</th>\n",
       "      <td>8</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.201087</td>\n",
       "      <td>0.184783</td>\n",
       "      <td>0.057065</td>\n",
       "      <td>63</td>\n",
       "      <td>11</td>\n",
       "      <td>0.851351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mexican_title_keywords</th>\n",
       "      <td>9</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.236413</td>\n",
       "      <td>0.119565</td>\n",
       "      <td>0.100543</td>\n",
       "      <td>65</td>\n",
       "      <td>22</td>\n",
       "      <td>0.747126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tortilla_corn</th>\n",
       "      <td>10</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.035326</td>\n",
       "      <td>0.032609</td>\n",
       "      <td>0.013587</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salsa_food</th>\n",
       "      <td>11</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.008152</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chip</th>\n",
       "      <td>12</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.027174</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.005435</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>canned_food</th>\n",
       "      <td>13</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.005435</td>\n",
       "      <td>0.005435</td>\n",
       "      <td>0.005435</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         j Polarity  Coverage  Overlaps  Conflicts  Correct  \\\n",
       "indian_title_keywords    0      [0]  0.160326  0.097826   0.046196       50   \n",
       "indian_des_keywords      1      [0]  0.252717  0.141304   0.084239       67   \n",
       "currymeat                2       []  0.000000  0.000000   0.000000        0   \n",
       "slow_cook                3      [0]  0.021739  0.016304   0.005435        5   \n",
       "italian_title_keywords   4      [1]  0.165761  0.141304   0.035326       56   \n",
       "italian_des_keywords     5      [1]  0.233696  0.203804   0.105978       57   \n",
       "pasta_with               6      [1]  0.024457  0.024457   0.010870        9   \n",
       "sundried_tomatoes        7      [1]  0.005435  0.002717   0.002717        2   \n",
       "ita_regions              8      [1]  0.201087  0.184783   0.057065       63   \n",
       "mexican_title_keywords   9      [2]  0.236413  0.119565   0.100543       65   \n",
       "tortilla_corn           10      [2]  0.035326  0.032609   0.013587       13   \n",
       "salsa_food              11      [2]  0.021739  0.021739   0.008152        8   \n",
       "chip                    12      [2]  0.027174  0.021739   0.005435        9   \n",
       "canned_food             13      [2]  0.005435  0.005435   0.005435        0   \n",
       "\n",
       "                        Incorrect  Emp. Acc.  \n",
       "indian_title_keywords           9   0.847458  \n",
       "indian_des_keywords            26   0.720430  \n",
       "currymeat                       0   0.000000  \n",
       "slow_cook                       3   0.625000  \n",
       "italian_title_keywords          5   0.918033  \n",
       "italian_des_keywords           29   0.662791  \n",
       "pasta_with                      0   1.000000  \n",
       "sundried_tomatoes               0   1.000000  \n",
       "ita_regions                    11   0.851351  \n",
       "mexican_title_keywords         22   0.747126  \n",
       "tortilla_corn                   0   1.000000  \n",
       "salsa_food                      0   1.000000  \n",
       "chip                            1   0.900000  \n",
       "canned_food                     2   0.000000  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting summary on VALIDATION set\n",
    "\n",
    "LFAnalysis(L_val_r, lfs=lfs_refined).lf_summary(Y_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our labeling functions vary in coverage, accuracy, and how much they overlap/conflict with one another. We can view a histogram of how many LF labels the data points in our dev set have to get an idea of our total coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_label_frequency(L):\n",
    "    plt.hist((L != ABSTAIN).sum(axis=1), density=True, bins=range(L.shape[1]))\n",
    "    plt.xlabel(\"Number of labels\")\n",
    "    plt.ylabel(\"Fraction of dataset\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_label_frequency(L_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that around half of our train dataset data points have 2 or fewer labels from LFs. Fortunately, the signal we do have can be used to train a classifier over the comment text directly, allowing it to generalize beyond what we've specified via our LFs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compared against Y_test - actual values of L\n",
    "L_test = applier.apply(df_ts.rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(691, 14)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(691,)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we use LabelModel presented by spark which (as documentation) claims to produce probablity-aware labels to train the further classification model. However, comparison between Label Model and Majority Vote is also provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy:   70.8%\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling import MajorityLabelVoter\n",
    "\n",
    "majority_model = MajorityLabelVoter(cardinality = 3)\n",
    "preds_train = majority_model.predict(L = L_train)\n",
    "\n",
    "majority_acc = majority_model.score(L=L_test, Y=Y_test, tie_break_policy=\"random\")[\"accuracy\"]\n",
    "print(f\"{'Majority Vote Accuracy:':<25} {majority_acc * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 51 is different from 42)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-efcdc8cb8d6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlabel_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mlabel_model_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mL_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtie_break_policy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"random\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{'Label Model Accuracy:':<25} {label_model_acc * 100:.1f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/Python3/lib/python3.6/site-packages/snorkel/labeling/model/label_model.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, L, Y, metrics, tie_break_policy)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         Y_pred, Y_prob = self.predict(\n\u001b[0;32m--> 505\u001b[0;31m             \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_probs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtie_break_policy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtie_break_policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m         )\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/Python3/lib/python3.6/site-packages/snorkel/labeling/model/label_model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, L, return_probs, tie_break_policy)\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \"\"\"\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0mY_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m         \u001b[0mY_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs_to_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtie_break_policy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_probs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/Python3/lib/python3.6/site-packages/snorkel/labeling/model/label_model.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, L)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;31m# Note: We omit abstains, effectively assuming uniform distribution here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL_aug\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjtm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m         \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcardinality\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 51 is different from 42)"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling import LabelModel\n",
    "\n",
    "label_model = LabelModel(cardinality=3, verbose=True)\n",
    "label_model.fit(L_train, n_epochs=100, log_freq=100, seed=11)\n",
    "\n",
    "label_model_acc = label_model.score(L=L_test, Y=Y_test, tie_break_policy=\"random\")[\"accuracy\"]\n",
    "print(f\"{'Label Model Accuracy:':<25} {label_model_acc * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above Label Model performs better in labelling test split. (NOTE from Max - n_epochs affect Label model). Cardinality = number of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bucket or something here, some explanations for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.analysis import get_label_buckets\n",
    "\n",
    "probs_val = majority_model.predict_proba(L=L_val)\n",
    "preds_val = probs_val >= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(368, 3)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+\n",
      "|               title|         des_stemmed|num_label|\n",
      "+--------------------+--------------------+---------+\n",
      "|           aloo gobi|tradit indian dis...|        0|\n",
      "|andreas dal for i...|india dal mean nu...|        0|\n",
      "|black pepper goat...|black peppercorn ...|        0|\n",
      "|bombay chicken wings|chicken wing ligh...|        0|\n",
      "|butternut squash ...|butternut squash ...|        0|\n",
      "|chaat  dahi batat...|fragrant aromat d...|        0|\n",
      "|chicken roti from...|fragrant homemad ...|        0|\n",
      "|chickpea coconut ...|garbanzo bean get...|        0|\n",
      "|    cilantro chutney|green chile peppe...|        0|\n",
      "|creamy cashew chi...|marin curri rub c...|        0|\n",
      "|cucumber peanut s...|fresh delici cucu...|        0|\n",
      "|     easy curry rice|rice side dish re...|        0|\n",
      "|four seasons chic...|hot sweet sour fr...|        0|\n",
      "|fruited tofu curr...|vegetarian tofu s...|        0|\n",
      "|  goan pork vindaloo|tradit goan pork ...|        0|\n",
      "|gobi masala cauli...|small floret stir...|        0|\n",
      "|indian barbeque c...|marin chicken ove...|        0|\n",
      "|indian chole aloo...|delici vegan indi...|        0|\n",
      "|indian curry pota...|potato cube fri i...|        0|\n",
      "| indian sabji recipe|cabbag potato fla...|        0|\n",
      "+--------------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_v.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preds_dev' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-2ee800eac977>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprobs_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmajority_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mL_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpreds_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs_val\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbuckets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_label_buckets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_dev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf_fn_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_dev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbuckets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSPAM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHAM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preds_dev' is not defined"
     ]
    }
   ],
   "source": [
    "from snorkel.analysis import get_label_buckets\n",
    "\n",
    "probs_val = majority_model.predict_proba(L=L_val)\n",
    "preds_val = probs_val >= 0.5\n",
    "buckets = get_label_buckets(Y_v, preds_val[:, 1])\n",
    "\n",
    "df_fn_val = df_v[[\"text\", \"label\"]].iloc[buckets[(, )]]\n",
    "df_fn_val[\"probability\"] = probs_dev[buckets[(SPAM, HAM)], 1]\n",
    "\n",
    "df_fn_dev.sample(5, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
