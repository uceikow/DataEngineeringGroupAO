{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "import pyspark\n",
    "\n",
    "number_cores = int(os.environ['NUM_CPUS'])\n",
    "memory_gb = int(os.environ['AVAILABLE_MEMORY_MB']) // 1024\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setMaster('local[{}]'.format(number_cores))\n",
    "        .set('spark.driver.memory', '{}g'.format(memory_gb))\n",
    ")\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[4] appName=pyspark-shell>\n"
     ]
    }
   ],
   "source": [
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "indian = sqlContext.read.csv(\"/project/Project/DataEngineeringGroupAO/Recipe_dataset/data_indian.csv\",header=True)\n",
    "italian = sqlContext.read.csv(\"/project/Project/DataEngineeringGroupAO/Recipe_dataset/data_italian.csv\",header=True)\n",
    "mexican = sqlContext.read.csv(\"/project/Project/DataEngineeringGroupAO/Recipe_dataset/data_mexican.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label the data\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "indian = indian.withColumn(\"label\",lit(\"indian\"))\n",
    "italian = italian.withColumn(\"label\",lit(\"italian\"))\n",
    "mexican = mexican.withColumn(\"label\",lit(\"mexican\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+\n",
      "|               Title|         Description| label|\n",
      "+--------------------+--------------------+------+\n",
      "|  Indian Peanut Stew|This is an easy, ...|indian|\n",
      "|        Roomali Roti|There is no leave...|indian|\n",
      "|Spicy Sweet Potat...|It's important to...|indian|\n",
      "|        Chicken Saag|The classic India...|indian|\n",
      "|Paleo Slow Cooker...|Boneless pork loi...|indian|\n",
      "|Bombay Chicken an...|Chicken parts are...|indian|\n",
      "|Indian Carrots, P...|Potatoes, peas an...|indian|\n",
      "|Wendy's Indian Bu...|This recipe resem...|indian|\n",
      "|    Indian Chickpeas|Garbanzo beans, o...|indian|\n",
      "|Dal Makhani (Indi...|These richly spic...|indian|\n",
      "|               Raita|Chopped tomatoes ...|indian|\n",
      "|Yogurt-Marinated ...|A yogurt-based ma...|indian|\n",
      "|Indian-Spiced Roa...|Spicy roasted chi...|indian|\n",
      "|Cauliflower and T...|Pressed tofu cube...|indian|\n",
      "|Channa Masala (Ch...|This fantastic In...|indian|\n",
      "|Bengali Chicken C...|Thy this deliciou...|indian|\n",
      "|  Indian Sweet Bread|A crisp and sweet...|indian|\n",
      "| Rosy's Palak Paneer|An Indian friend ...|indian|\n",
      "|Roti Bread from I...|This version of t...|indian|\n",
      "|Indian Vegetable ...|Basmati rice enha...|indian|\n",
      "+--------------------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combine 3 dataset into one\n",
    "\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def unionAll(dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "dfs = [indian, italian, mexican]\n",
    "recipe = unionAll(dfs)\n",
    "recipe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipe.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert it to RDD\n",
    "recipe_rdd = recipe.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Title='Indian Peanut Stew', Description='This is an easy, authentic dish from South Asia that appeals to a wide range of tastes. The…', label='indian'),\n",
       " Row(Title='Roomali Roti', Description='There is no leavening in this simple, tender Indian flatbread of bread flour, oil, salt and…', label='indian'),\n",
       " Row(Title='Spicy Sweet Potato Salad', Description=\"It's important to use good mayonnaise in this recipe, and to let the cooked potatoes chill…\", label='indian'),\n",
       " Row(Title='Chicken Saag', Description='The classic Indian chicken and spinach dish gets richness from sour cream.', label='indian'),\n",
       " Row(Title='Paleo Slow Cooker Pork Loin', Description='Boneless pork loin slowly cooks in a curried fruit sauce until tender and delicious.', label='indian')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipe_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all packages needed for data cleaning\n",
    "\n",
    "from pyspark.sql.functions import udf, regexp_replace, lower, col\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+\n",
      "|               Title|         Description| label|\n",
      "+--------------------+--------------------+------+\n",
      "|  indian peanut stew|this is an easy, ...|indian|\n",
      "|        roomali roti|there is no leave...|indian|\n",
      "|spicy sweet potat...|it's important to...|indian|\n",
      "|        chicken saag|the classic india...|indian|\n",
      "|paleo slow cooker...|boneless pork loi...|indian|\n",
      "|bombay chicken an...|chicken parts are...|indian|\n",
      "|indian carrots, p...|potatoes, peas an...|indian|\n",
      "|wendy's indian bu...|this recipe resem...|indian|\n",
      "|    indian chickpeas|garbanzo beans, o...|indian|\n",
      "|dal makhani (indi...|these richly spic...|indian|\n",
      "|               raita|chopped tomatoes ...|indian|\n",
      "|yogurt-marinated ...|a yogurt-based ma...|indian|\n",
      "|indian-spiced roa...|spicy roasted chi...|indian|\n",
      "|cauliflower and t...|pressed tofu cube...|indian|\n",
      "|channa masala (ch...|this fantastic in...|indian|\n",
      "|bengali chicken c...|thy this deliciou...|indian|\n",
      "|  indian sweet bread|a crisp and sweet...|indian|\n",
      "| rosy's palak paneer|an indian friend ...|indian|\n",
      "|roti bread from i...|this version of t...|indian|\n",
      "|indian vegetable ...|basmati rice enha...|indian|\n",
      "+--------------------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lowercase\n",
    "\n",
    "recipe = recipe.select(*[lower(col(col_name)).name(col_name) for col_name in recipe.columns])\n",
    "recipe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation and digits\n",
    "\n",
    "recipe_clean = recipe.select(regexp_replace('Title', \"[^a-zA-Z\\\\s]\", \"\").alias('title'), \n",
    "    (regexp_replace('Description', \"[^a-zA-Z\\\\s]\", \"\").alias('des')),'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+\n",
      "|               title|                 des| label|\n",
      "+--------------------+--------------------+------+\n",
      "|  indian peanut stew|this is an easy a...|indian|\n",
      "|        roomali roti|there is no leave...|indian|\n",
      "|spicy sweet potat...|its important to ...|indian|\n",
      "|        chicken saag|the classic india...|indian|\n",
      "|paleo slow cooker...|boneless pork loi...|indian|\n",
      "|bombay chicken an...|chicken parts are...|indian|\n",
      "|indian carrots pe...|potatoes peas and...|indian|\n",
      "|wendys indian but...|this recipe resem...|indian|\n",
      "|    indian chickpeas|garbanzo beans on...|indian|\n",
      "|dal makhani india...|these richly spic...|indian|\n",
      "|               raita|chopped tomatoes ...|indian|\n",
      "|yogurtmarinated s...|a yogurtbased mar...|indian|\n",
      "|indianspiced roas...|spicy roasted chi...|indian|\n",
      "|cauliflower and t...|pressed tofu cube...|indian|\n",
      "|channa masala chi...|this fantastic in...|indian|\n",
      "|bengali chicken c...|thy this deliciou...|indian|\n",
      "|  indian sweet bread|a crisp and sweet...|indian|\n",
      "|  rosys palak paneer|an indian friend ...|indian|\n",
      "|roti bread from i...|this version of t...|indian|\n",
      "|indian vegetable ...|basmati rice enha...|indian|\n",
      "+--------------------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recipe_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+\n",
      "|               title|           des_clean| label|\n",
      "+--------------------+--------------------+------+\n",
      "|  indian peanut stew|[easy, authentic,...|indian|\n",
      "|        roomali roti|[leavening, simpl...|indian|\n",
      "|spicy sweet potat...|[important, use, ...|indian|\n",
      "|        chicken saag|[classic, indian,...|indian|\n",
      "|paleo slow cooker...|[boneless, pork, ...|indian|\n",
      "|bombay chicken an...|[chicken, parts, ...|indian|\n",
      "|indian carrots pe...|[potatoes, peas, ...|indian|\n",
      "|wendys indian but...|[recipe, resemble...|indian|\n",
      "|    indian chickpeas|[garbanzo, beans,...|indian|\n",
      "|dal makhani india...|[richly, spiced, ...|indian|\n",
      "|               raita|[chopped, tomatoe...|indian|\n",
      "|yogurtmarinated s...|[yogurtbased, mar...|indian|\n",
      "|indianspiced roas...|[spicy, roasted, ...|indian|\n",
      "|cauliflower and t...|[pressed, tofu, c...|indian|\n",
      "|channa masala chi...|[fantastic, india...|indian|\n",
      "|bengali chicken c...|[thy, delicious, ...|indian|\n",
      "|  indian sweet bread|[crisp, sweet, fl...|indian|\n",
      "|  rosys palak paneer|[indian, friend, ...|indian|\n",
      "|roti bread from i...|[version, delicat...|indian|\n",
      "|indian vegetable ...|[basmati, rice, e...|indian|\n",
      "+--------------------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove Stopwords\n",
    "\n",
    "# Tokenize text\n",
    "tokenizer = Tokenizer(inputCol=\"des\", outputCol=\"des_token\")\n",
    "recipe = tokenizer.transform(recipe_clean).select('title','des','des_token','label')\n",
    "# tokenized.select(\"Description\", \"Des_words\")\\\n",
    "    #.withColumn(\"tokens\", countTokens(col(\"Des_words\"))).show(truncate=False)\n",
    "\n",
    "# Remove stopwords\n",
    "remover = StopWordsRemover(inputCol='des_token', outputCol='des_clean')\n",
    "recipe_no_stopw = remover.transform(recipe).select('title','des_clean', 'label')\n",
    "recipe_no_lists = recipe_no_stopw\n",
    "recipe_no_stopw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe = recipe_no_stopw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out different recipes\n",
    "# Create temp table\n",
    "recipe.createOrReplaceTempView('recipes')\n",
    "\n",
    "recipe_ind = sqlContext.sql(\"SELECT * FROM recipes WHERE label == 'indian'\")\n",
    "recipe_ita = sqlContext.sql(\"SELECT * FROM recipes WHERE label == 'italian'\")\n",
    "recipe_mex = sqlContext.sql(\"SELECT * FROM recipes WHERE label == 'mexican'\")\n",
    "# print((recipe_ind.count(), len(recipe_ind.columns)))\n",
    "# print((recipe_ita.count(), len(recipe_ita.columns)))\n",
    "# print((recipe_mex.count(), len(recipe_mex.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We asssume that we do not know labels for the majority of data points, \n",
    "# hence further explore only test split\n",
    "recipe_ind_tr, recipe_ind_ts, recipe_ind_dv = recipe_ind.randomSplit([0.8,0.2,0.1],seed = 11)\n",
    "recipe_ita_tr, recipe_ita_ts, recipe_ita_dv = recipe_ita.randomSplit([0.7,0.2,0.1],seed = 11)\n",
    "recipe_mex_tr, recipe_mex_ts, recipe_mex_dv = recipe_mex.randomSplit([0.7,0.2,0.1],seed = 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We left aside dev/val split as 10% of each of the datasets and 20% as test split to calculate accuracies of LFs. Bigger test set of \"gold\" is valuable to get more matches of LFs and see overall performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create frequency list\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "top_n = 15\n",
    "\n",
    "ind_counts = recipe_ind_dv.select(f.explode('des_clean').alias('col')).groupBy('col').count()\n",
    "ind_des_freq = ind_counts.orderBy(ind_counts[\"count\"].desc()).limit(top_n)\n",
    "\n",
    "ita_counts = recipe_ita_dv.select(f.explode('des_clean').alias('col')).groupBy('col').count()\n",
    "ita_des_freq = ita_counts.orderBy(ita_counts[\"count\"].desc()).limit(top_n)\n",
    "\n",
    "mex_counts = recipe_mex_dv.select(f.explode('des_clean').alias('col')).groupBy('col').count()\n",
    "mex_des_freq = mex_counts.orderBy(mex_counts[\"count\"].desc()).limit(top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+---------+-----+---------+-----+\n",
      "|        col|count|      col|count|      col|count|\n",
      "+-----------+-----+---------+-----+---------+-----+\n",
      "|     indian|   12|   cheese|    8|  chicken|   12|\n",
      "|      curry|   12|  chicken|    8|  mexican|   11|\n",
      "|    chicken|    8|    sauce|    7|     beef|    9|\n",
      "|     yogurt|    5|     easy|    7|     make|    9|\n",
      "|      sweet|    5|  italian|    6|tortillas|    8|\n",
      "|       rice|    5|   recipe|    5|     rice|    8|\n",
      "|       dish|    4|delicious|    5| tomatoes|    7|\n",
      "|cauliflower|    4|   tomato|    4|   filled|    7|\n",
      "| vegetarian|    4|    using|    3|   recipe|    7|\n",
      "|     spiced|    4|   creamy|    3|    spicy|    7|\n",
      "|       make|    4| parmesan|    3|    beans|    7|\n",
      "|      cumin|    4|    basil|    3|    flour|    6|\n",
      "|      quick|    4|   flavor|    3|     dish|    6|\n",
      "|    mixture|    4|  breasts|    3|    great|    6|\n",
      "|      sauce|    3|   simple|    3|    sauce|    6|\n",
      "+-----------+-----+---------+-----+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View then in one dataframe\n",
    "from pyspark.sql.functions import monotonically_increasing_id \n",
    "\n",
    "df1 = ind_des_freq.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "df2 = ita_des_freq.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "df3 = mex_des_freq.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "\n",
    "des_freq = df1.join(df2,(\"row_id\")).join(df3,(\"row_id\")).drop(\"row_id\")\n",
    "des_freq.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "# train split full / remove lists\n",
    "df_tr = recipe_ind_tr.union(recipe_ita_tr)\n",
    "df_tr = df_tr.union(recipe_mex_tr).withColumn(\"des_clean\", concat_ws(\" \", \"des_clean\"))\n",
    "\n",
    "# test split \n",
    "df_ts = recipe_ind_ts.union(recipe_ita_ts)\n",
    "df_ts = df_ts.union(recipe_mex_ts).withColumn(\"des_clean\", concat_ws(\" \", \"des_clean\"))\n",
    "# dev/val split\n",
    "df_dv = recipe_ind_dv.union(recipe_ita_dv)\n",
    "df_dv = df_dv.union(recipe_mex_dv).withColumn(\"des_clean\", concat_ws(\" \", \"des_clean\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(title='ada adai', des_clean='try crepelike items indianstyle breakfast made lentils rice')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop train split for labelling\n",
    "df_tr.drop('label').first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'indian':0, 'italian':1, 'mexican':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+\n",
      "|               title|           des_clean|num_label|\n",
      "+--------------------+--------------------+---------+\n",
      "|anapakaya paala k...|recipe calabash s...|        0|\n",
      "|basic indian curr...|wonderful indian ...|        0|\n",
      "|bombay chicken an...|chicken parts bru...|        0|\n",
      "|channa masala chi...|fantastic indian ...|        0|\n",
      "|       chicken korma|prepare flavorful...|        0|\n",
      "|cucumbercilantro ...|quick tasty india...|        0|\n",
      "|curried mushroom ...|steaming curriedm...|        0|\n",
      "|curried pork chop...|lean pork chops t...|        0|\n",
      "|curried stew with...|lamb marinated yo...|        0|\n",
      "|dairyfree caulifl...|battered fried ca...|        0|\n",
      "| easy veggie samosas|quick vegetarian ...|        0|\n",
      "|grilled lamb chop...|grilled lamb chop...|        0|\n",
      "|indian chicken ko...|indian chicken ko...|        0|\n",
      "|indian masala chi...|marinate chicken ...|        0|\n",
      "|   indian pork chops|great weeknight d...|        0|\n",
      "|indian vegetable ...|indian dry curry ...|        0|\n",
      "|instant potr indi...|quick easy indian...|        0|\n",
      "|    jeera cumin rice|cumin seeds fun c...|        0|\n",
      "|jorges indianspic...|tomato lentil sou...|        0|\n",
      "|kale and spinach ...|fresh spinach kal...|        0|\n",
      "+--------------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, create_map, lit\n",
    "from itertools import chain\n",
    "\n",
    "mapping_func = create_map([lit(x) for x in chain(*mapping.items())])\n",
    "\n",
    "df_dv = df_dv.withColumn(\"num_label\", mapping_func.getItem(col(\"label\")))\n",
    "# df_dv = df_dv.select('title','des_clean', 'num_label')\n",
    "df_dv = df_dv.select('title','des_clean', 'num_label')\n",
    "df_dv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137, 3)\n"
     ]
    }
   ],
   "source": [
    "print((df_dv.count(), len(df_dv.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# create column with actual values \n",
    "Y_dv = df_dv.select('num_label').rdd.flatMap(lambda x: x).collect()\n",
    "Y_dv = np.array(Y_dv)\n",
    "# import numpy as np\n",
    "# Y_dv = np.array(df_dv.select('num_label').collect())\n",
    "\n",
    "np.shape(Y_dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For clarity, we define constants to represent the class labels and abstaining.\n",
    "ABSTAIN = -1\n",
    "INDIAN = 0\n",
    "ITALIAN = 1\n",
    "MEXICAN = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting snorkel\n",
      "  Downloading snorkel-0.9.3-py3-none-any.whl (139 kB)\n",
      "\u001b[K     |████████████████████████████████| 139 kB 13.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch<1.2.0,>=1.1.0\n",
      "  Downloading torch-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (676.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 676.9 MB 2.5 kB/s s eta 0:00:01     |██████████                      | 211.1 MB 71.7 MB/s eta 0:00:07\n",
      "\u001b[?25hRequirement already satisfied: scipy<2.0.0,>=1.2.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from snorkel) (1.4.1)\n",
      "Collecting networkx<2.4,>=2.2\n",
      "  Downloading networkx-2.3.zip (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 86.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting munkres==1.1.2\n",
      "  Downloading munkres-1.1.2-py2.py3-none-any.whl (6.8 kB)\n",
      "Collecting pandas<0.26.0,>=0.25.0\n",
      "  Downloading pandas-0.25.3-cp36-cp36m-manylinux1_x86_64.whl (10.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.4 MB 16.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm<5.0.0,>=4.33.0\n",
      "  Downloading tqdm-4.43.0-py2.py3-none-any.whl (59 kB)\n",
      "\u001b[K     |████████████████████████████████| 59 kB 257 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.16.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from snorkel) (1.18.1)\n",
      "Collecting scikit-learn<0.22.0,>=0.20.2\n",
      "  Downloading scikit_learn-0.21.3-cp36-cp36m-manylinux1_x86_64.whl (6.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.7 MB 71.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboardX<2.0,>=1.6\n",
      "  Downloading tensorboardX-1.9-py2.py3-none-any.whl (190 kB)\n",
      "\u001b[K     |████████████████████████████████| 190 kB 101.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: decorator>=4.3.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from networkx<2.4,>=2.2->snorkel) (4.4.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from pandas<0.26.0,>=0.25.0->snorkel) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from pandas<0.26.0,>=0.25.0->snorkel) (2.8.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from scikit-learn<0.22.0,>=0.20.2->snorkel) (0.14.1)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from tensorboardX<2.0,>=1.6->snorkel) (3.11.3)\n",
      "Requirement already satisfied: six in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from tensorboardX<2.0,>=1.6->snorkel) (1.14.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorboardX<2.0,>=1.6->snorkel) (45.2.0.post20200210)\n",
      "Building wheels for collected packages: networkx\n",
      "  Building wheel for networkx (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for networkx: filename=networkx-2.3-py2.py3-none-any.whl size=1556408 sha256=5225668e74b4c7a7a41a39a29dceec125269c1bc00ad1f95fbb647ba52b19f4f\n",
      "  Stored in directory: /home/faculty/.cache/pip/wheels/81/dc/bb/fbde77ddcbf8d5a04787faf6cc9f1edf4c70a67961d7c75abf\n",
      "Successfully built networkx\n",
      "\u001b[31mERROR: faculty-models 0.1.1 has requirement mlflow-faculty>=0.4.3, but you'll have mlflow-faculty 0.4.2 which is incompatible.\u001b[0m\n",
      "Installing collected packages: torch, networkx, munkres, pandas, tqdm, scikit-learn, tensorboardX, snorkel\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 2.4\n",
      "    Uninstalling networkx-2.4:\n",
      "      Successfully uninstalled networkx-2.4\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.0.1\n",
      "    Uninstalling pandas-1.0.1:\n",
      "      Successfully uninstalled pandas-1.0.1\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.22.2.post1\n",
      "    Uninstalling scikit-learn-0.22.2.post1:\n",
      "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
      "Successfully installed munkres-1.1.2 networkx-2.3 pandas-0.25.3 scikit-learn-0.22.1 snorkel-0.9.3 tensorboardX-1.9 torch-1.1.0 tqdm-4.43.0\n"
     ]
    }
   ],
   "source": [
    "! pip install snorkel\n",
    "from snorkel.labeling.apply.spark import SparkLFApplier\n",
    "from snorkel.labeling import LFAnalysis\n",
    "from snorkel.labeling import labeling_function\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indian LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_keywords = ['curry','indian','masala','paneer','chutney','curried',\n",
    "                'simmered','cumin','yogurt','coconut']\n",
    "\n",
    "@labeling_function()\n",
    "def indian_keywords(x):\n",
    "        if any(word in x.title for word in ind_keywords):\n",
    "            return INDIAN\n",
    "        else:\n",
    "            return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Combo curry + meat\n",
    "@labeling_function()\n",
    "def currymeat(x):\n",
    "    return INDIAN if re.search(r\"(?=.*curry)(?=.*(chicken|lamb|beef))\", x.des_clean, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cooking process + food name\n",
    "@labeling_function()\n",
    "def cook_food(x):\n",
    "    return INDIAN if re.search(r\"(?=.*(quick|easy))(?=.*(rice|sauce|potatoes))\", x.des_clean, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Combo Sweet + Spicy\n",
    "\n",
    "@labeling_function()\n",
    "def sweet_spicy(x):\n",
    "    return INDIAN if re.search(r\"(?=.*sweet)(?=.*(spicy))\", x.des_clean, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Combo Slow + Cook\n",
    "\n",
    "@labeling_function()\n",
    "def slow_cook(x):\n",
    "    return INDIAN if re.search(r\"(?=.*slow)(?=.*(cook))\", x.des_clean, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Italian LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "ita_keywords = ['pasta','mozzarella', 'lasagna','pesto','dente', 'pizza']\n",
    "\n",
    "@labeling_function()\n",
    "def italian_keywords(x):\n",
    "        if any(word in x.title for word in ita_keywords):\n",
    "            return ITALIAN\n",
    "        else:\n",
    "            return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def pasta_with(x):\n",
    "    return ITALIAN if re.search(r\"(?=.*pasta)(?=.*(chicken|lamb|beef|pesto|creamy|shrimps|cheese))\", x.des_clean, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def sundried_tomatoes(x):\n",
    "    return ITALIAN if re.search(r\"(?=.*tomatoes)(?=.*(sun-dried|sundried))\", x.des_clean, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "ita_regions = ['tuscan','sicilian', 'romano', 'romaine', 'mediterranean','meditterranean' ]\n",
    "\n",
    "@labeling_function()\n",
    "def ita_regions(x):\n",
    "        if any(word in x.title for word in ita_keywords):\n",
    "            return ITALIAN\n",
    "        else:\n",
    "            return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aadison', 'aadit', 'aadith', 'aadithya', 'aaditiya']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #Importing dataset of first names \n",
    "\n",
    "# known_names = open(\"/project/Project/DataEngineeringGroupAO/Data_for_LF/first_names.all.txt\", \"r\")\n",
    "# known_names = list(known_names)\n",
    "# known_names = [x.replace('\\n', '') for x in known_names]\n",
    "# known_names[50:55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# known_names = ', '.join(known_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @labeling_function()\n",
    "# def chef_name(x):\n",
    "#     return ITALIAN if re.search(r\"(?=.*chef)(?=.*(John))\", x.des_clean, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mexican LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "mex_keywords = ['chicken','beef','cheese','corn','beans','salsa',\n",
    "                'spicy','tortillas','rice','rice']\n",
    "\n",
    "@labeling_function()\n",
    "def mexican_keywords(x):\n",
    "        if any(word in x.title for word in mex_keywords):\n",
    "            return MEXICAN\n",
    "        else:\n",
    "            return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Combo beef + cheese\n",
    "@labeling_function()\n",
    "def beefcheese(x):\n",
    "    return MEXICAN if re.search(r\"(?=.*beef)(?=.*(chicken|lamb))\", x.des_clean, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cooking process + food name\n",
    "@labeling_function()\n",
    "def cook_food_mexican(x):\n",
    "    return MEXICAN if re.search(r\"(?=.*(quick|easy))(?=.*(rice|sauce|potatoes))\", x.des_clean, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# canned + food\n",
    "@labeling_function()\n",
    "def canned_food(x):\n",
    "    return MEXICAN if re.search(r\"(?=.*(canned))(?=.*(chillies|soups|soup|sauce))\", x.des_clean, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cooking process + food name\n",
    "@labeling_function()\n",
    "def chipotle(x):\n",
    "    return MEXICAN if re.search(r\"(?=.*(chipotle))(?=.*(chicken|shrimp|chillies|peppers|sauce))\", x.des_clean, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions Aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr_rdd = df_tr.rdd\n",
    "df_dv_rdd = df_dv.rdd\n",
    "\n",
    "lfs = [indian_keywords, currymeat, cook_food, sweet_spicy, slow_cook, \n",
    "       italian_keywords, pasta_with, sundried_tomatoes, chef_name, \n",
    "       ita_regions, mexican_keywords, beefcheese, cook_food_mexican, canned_food, chipotle]\n",
    "\n",
    "applier = SparkLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df_tr_rdd)\n",
    "L_dev = applier.apply(df_dv_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137, 15)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(L_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>indian_keywords</th>\n",
       "      <td>0</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.169991</td>\n",
       "      <td>0.069326</td>\n",
       "      <td>0.062678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>currymeat</th>\n",
       "      <td>1</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.028490</td>\n",
       "      <td>0.023742</td>\n",
       "      <td>0.018044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cook_food</th>\n",
       "      <td>2</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sweet_spicy</th>\n",
       "      <td>3</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.003799</td>\n",
       "      <td>0.001899</td>\n",
       "      <td>0.001899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slow_cook</th>\n",
       "      <td>4</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.020893</td>\n",
       "      <td>0.013295</td>\n",
       "      <td>0.011396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>italian_keywords</th>\n",
       "      <td>5</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.084520</td>\n",
       "      <td>0.084520</td>\n",
       "      <td>0.012346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pasta_with</th>\n",
       "      <td>6</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sundried_tomatoes</th>\n",
       "      <td>7</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.004748</td>\n",
       "      <td>0.002849</td>\n",
       "      <td>0.000950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chef_name</th>\n",
       "      <td>8</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.006648</td>\n",
       "      <td>0.001899</td>\n",
       "      <td>0.001899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ita_regions</th>\n",
       "      <td>9</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.084520</td>\n",
       "      <td>0.084520</td>\n",
       "      <td>0.012346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mexican_keywords</th>\n",
       "      <td>10</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.330484</td>\n",
       "      <td>0.098765</td>\n",
       "      <td>0.094017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beefcheese</th>\n",
       "      <td>11</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.003799</td>\n",
       "      <td>0.000950</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cook_food_mexican</th>\n",
       "      <td>12</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>canned_food</th>\n",
       "      <td>13</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.005698</td>\n",
       "      <td>0.003799</td>\n",
       "      <td>0.000950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chipotle</th>\n",
       "      <td>14</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.002849</td>\n",
       "      <td>0.000950</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    j Polarity  Coverage  Overlaps  Conflicts\n",
       "indian_keywords     0      [0]  0.169991  0.069326   0.062678\n",
       "currymeat           1      [0]  0.028490  0.023742   0.018044\n",
       "cook_food           2      [0]  0.025641  0.025641   0.025641\n",
       "sweet_spicy         3      [0]  0.003799  0.001899   0.001899\n",
       "slow_cook           4      [0]  0.020893  0.013295   0.011396\n",
       "italian_keywords    5      [1]  0.084520  0.084520   0.012346\n",
       "pasta_with          6       []  0.000000  0.000000   0.000000\n",
       "sundried_tomatoes   7      [1]  0.004748  0.002849   0.000950\n",
       "chef_name           8      [1]  0.006648  0.001899   0.001899\n",
       "ita_regions         9      [1]  0.084520  0.084520   0.012346\n",
       "mexican_keywords   10      [2]  0.330484  0.098765   0.094017\n",
       "beefcheese         11      [2]  0.003799  0.000950   0.000000\n",
       "cook_food_mexican  12      [2]  0.025641  0.025641   0.025641\n",
       "canned_food        13      [2]  0.005698  0.003799   0.000950\n",
       "chipotle           14      [2]  0.002849  0.000950   0.000000"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.labeling import LFAnalysis\n",
    "\n",
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Incorrect</th>\n",
       "      <th>Emp. Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>indian_keywords</th>\n",
       "      <td>0</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.138686</td>\n",
       "      <td>0.058394</td>\n",
       "      <td>0.043796</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>currymeat</th>\n",
       "      <td>1</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.043796</td>\n",
       "      <td>0.043796</td>\n",
       "      <td>0.029197</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cook_food</th>\n",
       "      <td>2</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.021898</td>\n",
       "      <td>0.021898</td>\n",
       "      <td>0.021898</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sweet_spicy</th>\n",
       "      <td>3</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.007299</td>\n",
       "      <td>0.007299</td>\n",
       "      <td>0.007299</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slow_cook</th>\n",
       "      <td>4</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.007299</td>\n",
       "      <td>0.007299</td>\n",
       "      <td>0.007299</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>italian_keywords</th>\n",
       "      <td>5</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.080292</td>\n",
       "      <td>0.080292</td>\n",
       "      <td>0.021898</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pasta_with</th>\n",
       "      <td>6</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sundried_tomatoes</th>\n",
       "      <td>7</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.007299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chef_name</th>\n",
       "      <td>8</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ita_regions</th>\n",
       "      <td>9</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.080292</td>\n",
       "      <td>0.080292</td>\n",
       "      <td>0.021898</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mexican_keywords</th>\n",
       "      <td>10</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.379562</td>\n",
       "      <td>0.109489</td>\n",
       "      <td>0.102190</td>\n",
       "      <td>31</td>\n",
       "      <td>21</td>\n",
       "      <td>0.596154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beefcheese</th>\n",
       "      <td>11</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cook_food_mexican</th>\n",
       "      <td>12</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.021898</td>\n",
       "      <td>0.021898</td>\n",
       "      <td>0.021898</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>canned_food</th>\n",
       "      <td>13</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chipotle</th>\n",
       "      <td>14</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.014599</td>\n",
       "      <td>0.007299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    j Polarity  Coverage  Overlaps  Conflicts  Correct  \\\n",
       "indian_keywords     0      [0]  0.138686  0.058394   0.043796       19   \n",
       "currymeat           1      [0]  0.043796  0.043796   0.029197        6   \n",
       "cook_food           2      [0]  0.021898  0.021898   0.021898        0   \n",
       "sweet_spicy         3      [0]  0.007299  0.007299   0.007299        0   \n",
       "slow_cook           4      [0]  0.007299  0.007299   0.007299        0   \n",
       "italian_keywords    5      [1]  0.080292  0.080292   0.021898        9   \n",
       "pasta_with          6       []  0.000000  0.000000   0.000000        0   \n",
       "sundried_tomatoes   7      [1]  0.007299  0.000000   0.000000        1   \n",
       "chef_name           8       []  0.000000  0.000000   0.000000        0   \n",
       "ita_regions         9      [1]  0.080292  0.080292   0.021898        9   \n",
       "mexican_keywords   10      [2]  0.379562  0.109489   0.102190       31   \n",
       "beefcheese         11       []  0.000000  0.000000   0.000000        0   \n",
       "cook_food_mexican  12      [2]  0.021898  0.021898   0.021898        0   \n",
       "canned_food        13       []  0.000000  0.000000   0.000000        0   \n",
       "chipotle           14      [2]  0.014599  0.007299   0.000000        2   \n",
       "\n",
       "                   Incorrect  Emp. Acc.  \n",
       "indian_keywords            0   1.000000  \n",
       "currymeat                  0   1.000000  \n",
       "cook_food                  3   0.000000  \n",
       "sweet_spicy                1   0.000000  \n",
       "slow_cook                  1   0.000000  \n",
       "italian_keywords           2   0.818182  \n",
       "pasta_with                 0   0.000000  \n",
       "sundried_tomatoes          0   1.000000  \n",
       "chef_name                  0   0.000000  \n",
       "ita_regions                2   0.818182  \n",
       "mexican_keywords          21   0.596154  \n",
       "beefcheese                 0   0.000000  \n",
       "cook_food_mexican          3   0.000000  \n",
       "canned_food                0   0.000000  \n",
       "chipotle                   0   1.000000  "
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LFAnalysis(L_dev, lfs=lfs).lf_summary(Y_dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
